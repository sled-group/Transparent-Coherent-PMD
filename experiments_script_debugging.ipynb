{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d89e332-8f41-4711-8649-f9e5ac3aa0ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Initial configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609598b0-4653-4b42-85ba-2a38a01e4300",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8388735e-8531-4f6d-a3fb-9426fd4871df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb  8 08:28:31 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A40                     On  | 00000000:23:00.0 Off |                    0 |\n",
      "|  0%   28C    P8              26W / 300W |      7MiB / 46068MiB |      0%   E. Process |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7105f6c",
   "metadata": {},
   "source": [
    "# SuccessVQA Baseline Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3456798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from data.captaincook4d.constants import ANNOTATIONS_DIR, VIDEO_DIR, ERROR_CATEGORIES\n",
    "import os\n",
    "from pprint import pprint\n",
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "from travel.constants import DATA_CACHE_DIR, MODEL_CACHE_DIR, RESULTS_DIR\n",
    "from travel.model import MISTAKE_DETECTION_STRATEGIES, heuristic_cutoff_time, HEURISTIC_TARGET_FRAMES_PROPORTION\n",
    "from travel.model.vqa import VQAOutputs, VQAResponse, SUCCESSVQA_PROMPT_TEMPLATES, get_vqa_response_token_ids\n",
    "from travel.data import MistakeDetectionTasks, get_cutoff_time_by_proportion\n",
    "from travel.data.captaincook4d import CaptainCook4DDataset\n",
    "\n",
    "os.environ['HF_HOME'] = MODEL_CACHE_DIR\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"task\", type=str, default=\"captaincook4d\", choices=[task.value for task in MistakeDetectionTasks])\n",
    "parser.add_argument(\"eval_partition\", type=str, choices=[\"val\", \"test\"])\n",
    "parser.add_argument(\"vlm_name\", type=str, default=\"llava-hf/llava-1.5-7b-hf\", choices=list(SUCCESSVQA_PROMPT_TEMPLATES.keys()), help=\"Name or path to Hugging Face model for VLM.\")\n",
    "parser.add_argument(\"mistake_detection_strategy\", type=str, default=\"heuristic\", choices=list(MISTAKE_DETECTION_STRATEGIES.keys()))\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Load mistake detection dataset\n",
    "# TODO: implement cache for pre-loaded dataset?\n",
    "eval_dataset = CaptainCook4DDataset(data_split=args.eval_partition,\n",
    "                                    debug_n_examples_per_class=20)\n",
    "\n",
    "# Some mistake detection strategies are only applied to a specific proportion of frames; if so, we can skip running inference on these frames\n",
    "if args.mistake_detection_strategy == \"heuristic\":\n",
    "    target_frames_proportion = HEURISTIC_TARGET_FRAMES_PROPORTION\n",
    "else:\n",
    "    target_frames_proportion = None\n",
    "\n",
    "# Load VLM\n",
    "vlm_processor = AutoProcessor.from_pretrained(args.vlm_nmae)\n",
    "vlm = AutoModelForVision2Seq.from_pretrained(args.vlm_name, cache_dir=DATA_CACHE_DIR, load_in_8bit=True) # NOTE: when loading in 8bit, batch inference may output nans\n",
    "pprint(vlm.config)\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(device)\n",
    "# TODO: ensure zero temperature\n",
    "\n",
    "prompt_template = SUCCESSVQA_PROMPT_TEMPLATES[args.vlm_name]\n",
    "response_token_ids = get_vqa_response_token_ids(vlm_processor)\n",
    "\n",
    "# TODO: perform inference in batches?\n",
    "# TODO: cache VQA outputs for models?\n",
    "vqa_outputs = []\n",
    "for example in tqdm(eval_dataset, \"running inference on clips\"):\n",
    "    this_vqa_outputs = []\n",
    "    \n",
    "    step_id = example.procedure_id\n",
    "    step = example.procedure_description\n",
    "    \n",
    "    prompt = prompt_template.format(step=step)\n",
    "    expected_answer = VQAResponse[\"Yes\"]\n",
    "    \n",
    "    if target_frames_proportion is not None:\n",
    "        cutoff_time = get_cutoff_time_by_proportion(example, target_frames_proportion)\n",
    "    else:\n",
    "        cutoff_time = None\n",
    "\n",
    "    for frame, frame_time in zip(example.frames, example.frame_times):\n",
    "        if cutoff_time is not None and frame_time < cutoff_time:\n",
    "            # Don't run inference on this frame\n",
    "            this_vqa_outputs.append([VQAOutputs(\n",
    "                step_id,\n",
    "                frame,\n",
    "                prompt,\n",
    "                expected_answer,\n",
    "                response_token_ids,\n",
    "                torch.zeros((vlm_processor.tokenizer.vocab_size)).float() # Placeholder zero logits since we didn't prompt the VLM\n",
    "            )])\n",
    "            continue\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            inputs = vlm_processor(text=prompt, images=frame, return_tensors=\"pt\").to(vlm.device)\n",
    "            logits = vlm(**inputs).logits[0] # (seq length, vocab size)\n",
    "            logits = logits[-1].detach().cpu() # (vocab size)\n",
    "\n",
    "            this_vqa_outputs.append(\n",
    "                [VQAOutputs(\n",
    "                    step_id,\n",
    "                    frame,\n",
    "                    prompt,\n",
    "                    expected_answer,\n",
    "                    response_token_ids,\n",
    "                    logits,        \n",
    "                )]\n",
    "            )        \n",
    "        \n",
    "    vqa_outputs.append(this_vqa_outputs)\n",
    "\n",
    "# TODO: add DET curve to evaluator based on confidence - improve heuristic approach better?\n",
    "evaluator = MISTAKE_DETECTION_STRATEGIES[args.mistake_detection_strategy](eval_dataset.examples, vqa_outputs)\n",
    "metrics = evaluator.get_mistake_detection_metrics()\n",
    "print(\"Mistake Detection Metrics:\")\n",
    "pprint(metrics)\n",
    "\n",
    "results_folder = f\"SuccessVQA_{args.vlm_name.split(\"/\")[-1]}\"\n",
    "metrics_filename = f\"metrics_{args.mistake_detection_strategy}_{args.eval_partition}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel",
   "language": "python",
   "name": "travel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
