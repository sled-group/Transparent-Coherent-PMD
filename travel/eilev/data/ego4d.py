# This file was adapted from EILEV: https://github.com/yukw777/EILEV/blob/main/eilev/data/ego4d.py

import json
import os
from collections.abc import Callable
from typing import Any

import torch
from pytorchvideo.data import LabeledVideoDataset

from travel.eilev.data.utils import C_REGEX, NarratedActionClipSampler

# Some verbs would not be expected in the task-oriented mistake detection setting, so we can filter them out
EGO4D_IGNORE_VERBS = ['scroll', # Scrolling on phone, tablet, etc.
                     'touch', # Touching an object
                     'walk', # Walking around, sometimes to a destination
                     'give', # Usually handing an object to someone else
                     'read', # Reading a book
                     'drive_(ride,_drive)', # (Mostly) driving vehicles, sometimes driving nails or other parts into something else
                     'pet', # Petting animals
                     'climb', # Climbing stairs, walls, rocks, etc.
                     'play', # Playing games and instruments
                     'point', # Aiming hands and objects at something else
                     'consume_(taste,_sip,_eat,_drink)', # Eating and drinking
                     'search', # Looking for objects (typically followed by picking something up)
                     'enter', # Entering rooms or buildings
                     'watch', # Watching videos and movies
                     'park', # Parking a vehicle
                     'talk_(talk,_interact,_converse)', # Talking with others
                     'sit', # (Most often) a person sitting on something
                     'feed', # Feeding animals
                     'cross', # People crossing over things
                     'kick'] # Kicking objects with foot - usually not related to any task

def filter_action(action: dict[str, Any], previous_action: dict[str, Any]=None) -> bool:
    """Return True if the given action should be used, False otherwise."""
    return (
        not action["is_rejected"]
        and action["is_valid_action"]
        and action["critical_frames"] is not None
        and action["structured_verb"] not in EGO4D_IGNORE_VERBS # Omit clips with non-task actions
        and "#O" not in action["narration_text"] # Omit clips involving interacting with other people
        and (previous_action is None or not ((action['structured_verb'], action['structured_noun']) == (previous_action['structured_verb'], previous_action['structured_noun']))) # Filter out clips where the same action is being performed over and over
        and C_REGEX.match(action["narration_text"]) is not None
    )

def get_structured_noun(action: dict) -> str | None:
    if action["frames"] is None:
        return None
    for frame in action["frames"]:
        if frame["frame_type"] != "pnr_frame":
            # some actions don't have contact frames so use pnr_frame
            continue
        for box in frame["boxes"]:
            if (
                box["object_type"] == "object_of_change"
                and box["structured_noun"] is not None
            ):
                return box["structured_noun"]
    return None

# TODO: adapt below into a preprocessing method for actions - don't just add structures, but also combine repetitive actions
def add_structured_nouns(actions: list[dict]) -> list[dict]:
    for action_idx, action in enumerate(actions):
        action['structured_noun'] = get_structured_noun(action)
        
    for action_idx, action in enumerate(actions):
        structured_action = (action['structured_verb'], action['structured_noun'])
        action['previous_occurrences'] = sum([1 if structured_action == (previous_action['structured_verb'], previous_action['structured_noun']) else 0 for previous_action in actions[:action_idx - 1]])
        action['future_occurrences'] = sum([1 if structured_action == (future_action['structured_verb'], future_action['structured_noun']) else 0 for future_action in actions[action_idx + 1:]])
    return actions


class Ego4dFHOMainDataset(LabeledVideoDataset):
    """Class to store data from Ego4D. Some domain-specific filtering steps are performed for egocentric mistake detection."""
    def __init__(
        self,
        annotation_path: str,
        split_path: str,
        video_dir_path: str,
        transform: Callable[[dict], Any] | None = None,
        random_clip: bool = False,
    ) -> None:
        """
        :param annotation_path: path to the main annotation file, e.g., `fho_main.json`.
        :param split_path: path to video split file generated by
            `scripts/split_train_val_test.py`.
        :param video_path: path to video dir
        :param transform: optional transform function
        :param random_clip: whether to sample clips randomly
        """
        with open(annotation_path) as f:
            annotations = json.load(f)

        # create a dict video_uid => video
        video_dict = {video["video_uid"]: video for video in annotations["videos"]}

        with open(split_path) as f:
            split_data = json.load(f)

        self.split = split_data["split"]
        self.num_narrated_actions = sum(split_data["videos"].values())

        def _transform(item: dict) -> Any:
            """The first transform function that formats `narrated_actions` and
            `video`."""
            # format narrated_actions
            narrated_actions = item.pop("narrated_actions")
            item.update(narrated_actions[item["clip_index"]])

            # turn video tensor to torch.uint8
            item["video"] = item["video"].to(torch.uint8)
            if transform is not None:
                item = transform(item)
            return item

        super().__init__(
            [
                (
                    os.path.join(video_dir_path, video_uid + ".mp4"),
                    {
                        "narrated_actions": [
                            {
                                "pre_45": action["critical_frames"]["pre_45"],
                                "pre_30": action["critical_frames"]["pre_30"],
                                "pre_15": action["critical_frames"]["pre_15"],
                                "pre_frame": action["critical_frames"]["pre_frame"],
                                "pnr_frame": action["critical_frames"]["pnr_frame"],
                                "post_frame": action["critical_frames"]["post_frame"],
                                "fps": video_dict[video_uid]["video_metadata"]["fps"],
                                "narration_text": action["narration_text"],
                                "structured_verb": action["structured_verb"],
                                "structured_noun": action["structured_noun"],
                                "previous_occurrences": action["previous_occurrences"],
                                "future_occurrences": action["future_occurrences"]
                            }
                            for interval in video_dict[video_uid]["annotated_intervals"]
                            for action_idx, action in enumerate(add_structured_nouns(interval["narrated_actions"]))
                            if filter_action(action, previous_action=interval["narrated_actions"][action_idx - 1] if action_idx > 0 else None)
                        ],
                        "video_uid": video_uid,
                        "video_metadata": video_dict[video_uid]["video_metadata"],
                    },
                )
                for video_uid in split_data["videos"]
            ],
            NarratedActionClipSampler(random_clip),
            transform=_transform,
            decode_audio=False,
        )

    def __len__(self) -> int:
        return self.num_narrated_actions