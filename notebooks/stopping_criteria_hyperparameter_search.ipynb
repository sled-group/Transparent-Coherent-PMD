{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151f08f8-478e-4ea9-be76-665ef5790dba",
   "metadata": {},
   "source": [
    "This notebook can be used to hyperparameter tune the early stopping criteria (rather than specifying them):\n",
    "* Maximum number of iterations before stopping (still capped at 10, the default maximum)\n",
    "* The early stop delta, i.e., if the success probability changes by less than this number across 3 iterations, stop early\n",
    "* The early stop confidence threshold, i.e., how confident VLM should be to stop early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609598b0-4653-4b42-85ba-2a38a01e4300",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir(\"/nfs/turbo/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl\")\n",
    "from travel import init_travel\n",
    "init_travel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a398faf-4c3c-49c4-a680-27f9777aef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import defaultdict, Counter\n",
    "from copy import deepcopy\n",
    "from itertools import product\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForVision2Seq, AutoModelForCausalLM, AutoProcessor, BitsAndBytesConfig, AutoModelForSequenceClassification, AutoTokenizer, PhrasalConstraint           \n",
    "\n",
    "from travel.constants import RESULTS_DIR, IMAGES_CHUNK_SIZE, HF_TOKEN\n",
    "from travel.data.captaincook4d import CaptainCook4DDataset\n",
    "from travel.data.ego4d import Ego4DMistakeDetectionDataset\n",
    "from travel.data.mistake_detection import MistakeDetectionTasks\n",
    "from travel.data.vqa import VQAResponse, get_vqa_response_token_ids, VQAOutputs, DIALOG_START_TOKENS, IMAGE_TOKENS, USER_START_TOKENS, USER_END_TOKENS, ASSISTANT_START_TOKENS, ASSISTANT_END_TOKENS, IVQA_PREAMBLE, IVQA_SUCCESS_QUESTION\n",
    "from travel.data.vqg import generate_vqg_prompt_icl\n",
    "from travel.model import simple_lm_prompt_beam_search, simple_vlm_prompt_beam_search, compute_completion_log_likelihoods, compute_completion_log_likelihoods_encoder_decoder, compute_completion_log_likelihoods_vlm\n",
    "from travel.model.grounding import VisualFilterTypes, ContrastiveRegionFilter, VisualContrastiveFilter, SpatialVisualFilter, AGLAFilter, ImageMaskTypes\n",
    "from travel.model.metrics import mistake_detection_metrics, question_coherence_metrics_nli, question_coherence_metrics_vlm, generate_det_curve, generate_tiered_metric_curves, entropy, compile_accuracy_and_coherence_metrics\n",
    "from travel.model.mistake_detection import MISTAKE_DETECTION_THRESHOLDS\n",
    "from travel.model.nli import NLI_MODEL_PATH, NLI_BATCH_SIZE\n",
    "from travel.model.vqa import run_vqa_with_visual_filter\n",
    "from travel.model.vqg import cleanup_generated_question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30ecbc78-0f3c-4317-bdd1-58eb56635f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9565c5b2a040f7ac76b133622e6516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VLM_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# Load VLM - some VLMs may be under AutoModelForVision2Seq, some may be under AutoModelForCausalLM\n",
    "try:\n",
    "    vlm = AutoModelForVision2Seq.from_pretrained(VLM_NAME, quantization_config=bnb_config, trust_remote_code=True, token=HF_TOKEN)   \n",
    "except Exception as e:\n",
    "    print(\"Encountered exception when trying to load model with AutoModelForVision2Seq:\")\n",
    "    pprint(e)\n",
    "    \n",
    "    vlm = AutoModelForCausalLM.from_pretrained(VLM_NAME, quantization_config=bnb_config, trust_remote_code=True, token=HF_TOKEN)\n",
    "vlm_processor = AutoProcessor.from_pretrained(VLM_NAME, trust_remote_code=True, token=HF_TOKEN)\n",
    "vlm_processor.tokenizer.padding_side = \"left\"\n",
    "response_token_ids = get_vqa_response_token_ids(vlm_processor.tokenizer)\n",
    "\n",
    "# We'll use VLM's LM directly to generate questions\n",
    "if getattr(vlm, \"language_model\", None):\n",
    "    lm = vlm.language_model\n",
    "else:\n",
    "    lm = vlm\n",
    "tokenizer = vlm_processor.tokenizer\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# NLI model to score consistency and verifiability\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(NLI_MODEL_PATH, quantization_config=bnb_config)\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(NLI_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "918bbdf9-196b-4d99-9718-c952046ad9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "this_results_dir = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222/vqa_mistake_detection/ego4d_single_debug250/llava-1.5-7b-hf/IterativeVQA_q10_ego4d_single_debug250_llava-1.5-7b-hf_beam8-4_likelihood_nohistory_20240815204213\"\n",
    "# this_results_dir = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222_dpo/vqa_mistake_detection/ego4d_single_debug20/llava-1.5-7b-hf/IterativeVQA_q10_ego4d_single_debug20_llava-1.5-7b-hf_beam8-4_likelihood_nohistory_0000000000\"\n",
    "\n",
    "with open(os.path.join(this_results_dir, \"outputs_val.json\"), \"r\") as f:\n",
    "    all_results_dicts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4036280-5c99-4096-8438-2d1013bfa38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running generation (cuda:0):  68%|██████▊   | 170/250 [10:24<04:53,  3.67s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Get all coherence metrics for all turns (original run doesn't have this)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_coherence_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m     all_coherence_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mquestion_coherence_metrics_nli\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnli_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mnli_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                                         \u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprocedure\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresults_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocedure\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_results_dicts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_procedures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mall_chosen_questions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43manswers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_predicted_answers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mprevious_questions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_previous_questions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mprevious_answers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_previous_answers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mmistake_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mresults_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmistake\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresults_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mall_results_dicts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mrephrase_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Adjust all_coherence_metrics for the specific final turns we chose here\u001b[39;00m\n\u001b[1;32m     58\u001b[0m readjusted_all_coherence_metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/nfs/turbo/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/travel/model/metrics.py:475\u001b[0m, in \u001b[0;36mquestion_coherence_metrics_nli\u001b[0;34m(nli_tokenizer, nli_model, lm_tokenizer, lm_model, procedures, questions, answers, previous_questions, previous_answers, mistake_labels, rephrase_batch_size, rephrase_success)\u001b[0m\n\u001b[1;32m    468\u001b[0m     hypothesis_procedure \u001b[38;5;241m=\u001b[39m rephrase_procedure_success(\n\u001b[1;32m    469\u001b[0m         procedures,\n\u001b[1;32m    470\u001b[0m         lm_tokenizer,\n\u001b[1;32m    471\u001b[0m         lm_model,\n\u001b[1;32m    472\u001b[0m         generation_batch_size\u001b[38;5;241m=\u001b[39mrephrase_batch_size,\n\u001b[1;32m    473\u001b[0m     )\n\u001b[1;32m    474\u001b[0m \u001b[38;5;66;03m# Rephrase question with a yes and no answer as statements to compare their entailment probability of success\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m rephrased_yes \u001b[38;5;241m=\u001b[39m \u001b[43mrephrase_question_answer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlm_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrephrase_batch_size\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m rephrased_no \u001b[38;5;241m=\u001b[39m rephrase_question_answer(\n\u001b[1;32m    483\u001b[0m     questions, \n\u001b[1;32m    484\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(questions),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    487\u001b[0m     generation_batch_size\u001b[38;5;241m=\u001b[39mrephrase_batch_size\n\u001b[1;32m    488\u001b[0m )    \n\u001b[1;32m    489\u001b[0m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrephrased_questions_yes\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m rephrased_yes\n",
      "File \u001b[0;32m/nfs/turbo/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/travel/model/metrics.py:406\u001b[0m, in \u001b[0;36mrephrase_question_answer\u001b[0;34m(questions, answers, tokenizer, lm, generation_batch_size)\u001b[0m\n\u001b[1;32m    393\u001b[0m examples \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: Is there a bowl on the table?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer: Yes\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStatement: There is a bowl on the table.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: Are the eggs cracked?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer: No\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStatement: The eggs are not cracked.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: Is the cabinet closed?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer: No\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStatement: The cabinet is not closed.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    404\u001b[0m ]\n\u001b[1;32m    405\u001b[0m prompts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(examples) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStatement: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m question, answer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(questions, answers)]\n\u001b[0;32m--> 406\u001b[0m rephrased_texts \u001b[38;5;241m=\u001b[39m \u001b[43msimple_lm_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpad_token_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m rephrased_texts \u001b[38;5;241m=\u001b[39m [text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m rephrased_texts]\n\u001b[1;32m    408\u001b[0m rephrased_texts \u001b[38;5;241m=\u001b[39m [text\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m rephrased_texts]\n",
      "File \u001b[0;32m/nfs/turbo/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/travel/model/__init__.py:104\u001b[0m, in \u001b[0;36msimple_lm_prompt\u001b[0;34m(lm, tokenizer, prompts, max_new_tokens, batch_size, generation_kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text\u001b[38;5;241m=\u001b[39mbatch_prompts, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(lm\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 104\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39msequences[:, inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:]\n\u001b[1;32m    107\u001b[0m outputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2047\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2040\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2041\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2042\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2043\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2044\u001b[0m     )\n\u001b[1;32m   2046\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2047\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2048\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2058\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2059\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2060\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2061\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2066\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2067\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2996\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2993\u001b[0m unfinished_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(batch_size, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2994\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_initial_cache_position(input_ids, model_kwargs)\n\u001b[0;32m-> 2996\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\n\u001b[1;32m   2998\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   2999\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[1;32m   3000\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   3002\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2245\u001b[0m, in \u001b[0;36mGenerationMixin._has_unfinished_sequences\u001b[0;34m(self, this_peer_finished, synced_gpus, device, cur_len, max_length)\u001b[0m\n\u001b[1;32m   2243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m this_peer_finished_flag\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   2244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 2245\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2247\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cand_max_iterations = [2, 4, 6, 8, 10]\n",
    "cand_early_stop_delta = [0.05, 0.1, 0.2, 0.4]\n",
    "cand_confident_delta = [0.025, 0.05, 0.1, 0.2]\n",
    "cand_criteria = product(cand_max_iterations, cand_early_stop_delta, cand_confident_delta)\n",
    "\n",
    "best_accuracy = None\n",
    "\n",
    "all_coherence_metrics = None\n",
    "\n",
    "for mi, esd, cd in cand_criteria:\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    all_procedures = []\n",
    "    for example_id, output in all_results_dicts.items():\n",
    "        final_success_prob = None\n",
    "        success_probs = output['success_probs']\n",
    "        for success_prob_idx, success_prob in enumerate(success_probs[:mi]): \n",
    "            # Early stopping mechanism: \n",
    "            # if success score doesn't change enough over 3 turns, stop incorporating questions\n",
    "            # (we still run inference across all questions for efficiency and simplicity, but later can make a proper demo script)\n",
    "            final_success_prob = success_prob\n",
    "            if success_prob_idx >= 2 and success_prob_idx < len(success_probs) - 1:\n",
    "                if np.abs(success_probs[success_prob_idx-1] - success_probs[success_prob_idx-2]) < esd and np.abs(success_probs[success_prob_idx] - success_probs[success_prob_idx-1]) < esd:\n",
    "                    break\n",
    "            # OR if success score is within confident delta, stop\n",
    "            if success_prob < cd or 1.0 - success_prob < cd:\n",
    "                break           \n",
    "                \n",
    "        output['final_turn'] = success_prob_idx\n",
    "        all_results_dicts[example_id] = output\n",
    "        all_probs.append(final_success_prob)\n",
    "        all_labels.append(output['mistake_type'])\n",
    "        all_procedures.append(output['procedure'])\n",
    "                \n",
    "    # Calculate coherence metrics of updated rollouts\n",
    "    all_chosen_questions = [question for results_dict in all_results_dicts.values() for question in results_dict['questions'][:10]]\n",
    "    all_previous_questions = [[q for qi, q in enumerate(results_dict['questions'][:question_idx]) if results_dict['answers'][qi] != \"Unsure\"] for results_dict in all_results_dicts.values() for question_idx in range(10)]\n",
    "\n",
    "    label_answer_mapping = {0: \"No\", 1: \"Yes\"}\n",
    "    all_predicted_answers = [label_answer_mapping[np.argmax(answer_probs)] for results_dict in all_results_dicts.values() for answer_probs in results_dict['answer_probs'][:10]]\n",
    "    all_previous_answers = [[a for a in results_dict['answers'][:question_idx] if a != \"Unsure\"] for results_dict in all_results_dicts.values() for question_idx in range(10)]\n",
    "\n",
    "    # Get all coherence metrics for all turns (original run doesn't have this)\n",
    "    if all_coherence_metrics is None:\n",
    "        all_coherence_metrics = question_coherence_metrics_nli(nli_tokenizer,\n",
    "                                                               nli_model,\n",
    "                                                               tokenizer,\n",
    "                                                               lm,                                         \n",
    "                                                               [procedure for results_dict, procedure in zip(all_results_dicts.values(), all_procedures) for _ in range(10)],\n",
    "                                                               all_chosen_questions,\n",
    "                                                               answers=all_predicted_answers,\n",
    "                                                               previous_questions=all_previous_questions,\n",
    "                                                               previous_answers=all_previous_answers,\n",
    "                                                               mistake_labels=[results_dict['mistake'] for results_dict in all_results_dicts.values() for _ in range(10)],\n",
    "                                                               rephrase_batch_size=20)\n",
    "        \n",
    "    # Adjust all_coherence_metrics for the specific final turns we chose here\n",
    "    readjusted_all_coherence_metrics = {}\n",
    "    for k in all_coherence_metrics:\n",
    "        parallel_idx = 0\n",
    "        this_metrics = []\n",
    "        for results_dict in all_results_dicts.values():\n",
    "            for question_idx in range(results_dict['final_turn'] + 1):\n",
    "                if type(k) != str:\n",
    "                    this_metrics.append(max(round(float(all_coherence_metrics[k][parallel_idx]), 6), 0.0)) # If negative, just round up to 0.0 for aggregated metrics\n",
    "                else:\n",
    "                    this_metrics.append(all_coherence_metrics[k][parallel_idx])\n",
    "                parallel_idx += 1\n",
    "        readjusted_all_coherence_metrics[k] = this_metrics\n",
    "    \n",
    "    # Compile accuracy and coherence metrics\n",
    "    accuracy_metrics_by_threshold, coherence_metrics = compile_accuracy_and_coherence_metrics(all_labels, all_probs, readjusted_all_coherence_metrics, all_results_dicts, MISTAKE_DETECTION_THRESHOLDS, 0.1)\n",
    "    coherence_metrics_by_threshold = coherence_metrics['metrics_by_threshold']\n",
    "    \n",
    "    this_accuracy = accuracy_metrics_by_threshold['best_metrics']['accuracy']\n",
    "    if best_accuracy is None or this_accuracy > best_accuracy:\n",
    "        best_accuracy = this_accuracy\n",
    "        best_metrics = (accuracy_metrics_by_threshold, readjusted_all_coherence_metrics, coherence_metrics, coherence_metrics_by_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f076ab0e-fc0d-4d2e-a547-2f69c2a86f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metrics_by_threshold, readjusted_all_coherence_metrics, coherence_metrics, coherence_metrics_by_theshold = best_metrics\n",
    "\n",
    "# Save accuracy and coherence metrics\n",
    "json.dump(accuracy_metrics_by_threshold, \n",
    "        open(os.path.join(this_results_dir, f\"tuned_metrics_accuracy_val.json\"), \"w\"),\n",
    "        indent=4)\n",
    "\n",
    "json.dump(coherence_metrics, \n",
    "        open(os.path.join(this_results_dir, f\"tuned_metrics_coherence_nli_val.json\"), \"w\"),\n",
    "        indent=4)\n",
    "\n",
    "json.dump(readjusted_all_coherence_metrics, \n",
    "        open(os.path.join(this_results_dir, f\"tuned_metrics_coherence_raw_nli_val_tuned.json\"), \"w\"),\n",
    "        indent=4)            \n",
    "\n",
    "# Generate DET curves for accuracy\n",
    "generate_det_curve(accuracy_metrics_by_threshold, os.path.join(this_results_dir, f\"tuned_det_accuracy_val.pdf\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f70e66-fecc-468b-94ea-0a323d2ac1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel",
   "language": "python",
   "name": "travel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
