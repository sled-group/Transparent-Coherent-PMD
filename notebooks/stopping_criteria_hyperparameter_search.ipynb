{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151f08f8-478e-4ea9-be76-665ef5790dba",
   "metadata": {},
   "source": [
    "This notebook can be used to hyperparameter tune the early stopping criteria (rather than specifying them):\n",
    "* Maximum number of iterations before stopping (still capped at 10, the default maximum)\n",
    "* The early stop delta, i.e., if the success probability changes by less than this number across 3 iterations, stop early\n",
    "* The early stop confidence threshold, i.e., how confident VLM should be to stop early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609598b0-4653-4b42-85ba-2a38a01e4300",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir(\"/nfs/turbo/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl\")\n",
    "from travel import init_travel\n",
    "init_travel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a398faf-4c3c-49c4-a680-27f9777aef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import defaultdict, Counter\n",
    "from copy import deepcopy\n",
    "from itertools import product\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForVision2Seq, AutoModelForCausalLM, AutoProcessor, BitsAndBytesConfig, AutoModelForSequenceClassification, AutoTokenizer, PhrasalConstraint           \n",
    "\n",
    "from travel.constants import RESULTS_DIR, IMAGES_CHUNK_SIZE, HF_TOKEN\n",
    "from travel.data.captaincook4d import CaptainCook4DDataset\n",
    "from travel.data.ego4d import Ego4DMistakeDetectionDataset\n",
    "from travel.data.mistake_detection import MistakeDetectionTasks\n",
    "from travel.data.vqa import VQAResponse, get_vqa_response_token_ids, VQAOutputs, DIALOG_START_TOKENS, IMAGE_TOKENS, USER_START_TOKENS, USER_END_TOKENS, ASSISTANT_START_TOKENS, ASSISTANT_END_TOKENS, IVQA_PREAMBLE, IVQA_SUCCESS_QUESTION\n",
    "from travel.data.vqg import generate_vqg_prompt_icl\n",
    "from travel.model import simple_lm_prompt_beam_search, simple_vlm_prompt_beam_search, compute_completion_log_likelihoods, compute_completion_log_likelihoods_encoder_decoder, compute_completion_log_likelihoods_vlm\n",
    "from travel.model.grounding import VisualFilterTypes, ContrastiveRegionFilter, VisualContrastiveFilter, SpatialVisualFilter, AGLAFilter, ImageMaskTypes\n",
    "from travel.model.metrics import mistake_detection_metrics, question_coherence_metrics_nli, question_coherence_metrics_vlm, generate_det_curve, generate_tiered_metric_curves, entropy, compile_accuracy_and_coherence_metrics\n",
    "from travel.model.mistake_detection import MISTAKE_DETECTION_THRESHOLDS\n",
    "from travel.model.nli import NLI_MODEL_PATH, NLI_BATCH_SIZE\n",
    "from travel.model.vqa import run_vqa_with_visual_filter\n",
    "from travel.model.vqg import cleanup_generated_question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30ecbc78-0f3c-4317-bdd1-58eb56635f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5fc761a1da429987fc749a9e796066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VLM_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# Load VLM - some VLMs may be under AutoModelForVision2Seq, some may be under AutoModelForCausalLM\n",
    "try:\n",
    "    vlm = AutoModelForVision2Seq.from_pretrained(VLM_NAME, quantization_config=bnb_config, trust_remote_code=True, token=HF_TOKEN)   \n",
    "except Exception as e:\n",
    "    print(\"Encountered exception when trying to load model with AutoModelForVision2Seq:\")\n",
    "    pprint(e)\n",
    "    \n",
    "    vlm = AutoModelForCausalLM.from_pretrained(VLM_NAME, quantization_config=bnb_config, trust_remote_code=True, token=HF_TOKEN)\n",
    "vlm_processor = AutoProcessor.from_pretrained(VLM_NAME, trust_remote_code=True, token=HF_TOKEN)\n",
    "vlm_processor.tokenizer.padding_side = \"left\"\n",
    "response_token_ids = get_vqa_response_token_ids(vlm_processor.tokenizer)\n",
    "\n",
    "# We'll use VLM's LM directly to generate questions\n",
    "if getattr(vlm, \"language_model\", None):\n",
    "    lm = vlm.language_model\n",
    "else:\n",
    "    lm = vlm\n",
    "tokenizer = vlm_processor.tokenizer\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# NLI model to score consistency and verifiability\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(NLI_MODEL_PATH, quantization_config=bnb_config)\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(NLI_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "918bbdf9-196b-4d99-9718-c952046ad9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "this_results_dir = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222/vqa_mistake_detection/ego4d_single_debug250/llava-1.5-7b-hf/IterativeVQA_q10_ego4d_single_debug250_llava-1.5-7b-hf_beam8-4_likelihood_nohistory_20240815204213\"\n",
    "# this_results_dir = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222_dpo/vqa_mistake_detection/ego4d_single_debug20/llava-1.5-7b-hf/IterativeVQA_q10_ego4d_single_debug20_llava-1.5-7b-hf_beam8-4_likelihood_nohistory_0000000000\"\n",
    "\n",
    "with open(os.path.join(this_results_dir, \"outputs_val.json\"), \"r\") as f:\n",
    "    all_results_dicts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4036280-5c99-4096-8438-2d1013bfa38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running generation (cuda:0): 100%|██████████| 250/250 [15:17<00:00,  3.67s/it]\n",
      "running generation (cuda:0): 100%|██████████| 250/250 [15:17<00:00,  3.67s/it]\n",
      "running NLI (cuda:0): 100%|██████████| 40/40 [00:04<00:00,  9.77it/s]\n",
      "running NLI (cuda:0): 100%|██████████| 40/40 [00:04<00:00,  9.50it/s]\n",
      "running generation (cuda:0): 100%|██████████| 975/975 [59:23<00:00,  3.65s/it]\n",
      "running NLI (cuda:0):  18%|█▊        | 7/40 [00:02<00:11,  2.81it/s]"
     ]
    }
   ],
   "source": [
    "cand_max_iterations = [2, 4, 6, 8, 10]\n",
    "cand_early_stop_delta = [0.05, 0.1, 0.2, 0.4]\n",
    "cand_confident_delta = [0.025, 0.05, 0.1, 0.2]\n",
    "cand_criteria = product(cand_max_iterations, cand_early_stop_delta, cand_confident_delta)\n",
    "\n",
    "best_accuracy = None\n",
    "\n",
    "all_coherence_metrics = None\n",
    "\n",
    "for mi, esd, cd in cand_criteria:\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    all_procedures = []\n",
    "    for example_id, output in all_results_dicts.items():\n",
    "        final_success_prob = None\n",
    "        success_probs = output['success_probs']\n",
    "        for success_prob_idx, success_prob in enumerate(success_probs[:mi]): \n",
    "            # Early stopping mechanism: \n",
    "            # if success score doesn't change enough over 3 turns, stop incorporating questions\n",
    "            # (we still run inference across all questions for efficiency and simplicity, but later can make a proper demo script)\n",
    "            final_success_prob = success_prob\n",
    "            if success_prob_idx >= 2 and success_prob_idx < len(success_probs) - 1:\n",
    "                if np.abs(success_probs[success_prob_idx-1] - success_probs[success_prob_idx-2]) < esd and np.abs(success_probs[success_prob_idx] - success_probs[success_prob_idx-1]) < esd:\n",
    "                    break\n",
    "            # OR if success score is within confident delta, stop\n",
    "            if success_prob < cd or 1.0 - success_prob < cd:\n",
    "                break           \n",
    "                \n",
    "        output['final_turn'] = success_prob_idx\n",
    "        all_results_dicts[example_id] = output\n",
    "        all_probs.append(final_success_prob)\n",
    "        all_labels.append(output['mistake_type'])\n",
    "        all_procedures.append(output['procedure'])\n",
    "                \n",
    "    # Calculate coherence metrics of updated rollouts\n",
    "    all_chosen_questions = [question for results_dict in all_results_dicts.values() for question in results_dict['questions'][:10]]\n",
    "    all_previous_questions = [[q for qi, q in enumerate(results_dict['questions'][:question_idx]) if results_dict['answers'][qi] != \"Unsure\"] for results_dict in all_results_dicts.values() for question_idx in range(10)]\n",
    "\n",
    "    label_answer_mapping = {0: \"No\", 1: \"Yes\"}\n",
    "    all_predicted_answers = [label_answer_mapping[np.argmax(answer_probs)] for results_dict in all_results_dicts.values() for answer_probs in results_dict['answer_probs'][:10]]\n",
    "    all_previous_answers = [[a for a in results_dict['answers'][:question_idx] if a != \"Unsure\"] for results_dict in all_results_dicts.values() for question_idx in range(10)]\n",
    "\n",
    "    # Get all coherence metrics for all turns (original run doesn't have this)\n",
    "    if all_coherence_metrics is None:\n",
    "        all_coherence_metrics = question_coherence_metrics_nli(nli_tokenizer,\n",
    "                                                               nli_model,\n",
    "                                                               tokenizer,\n",
    "                                                               lm,                                         \n",
    "                                                               [procedure for results_dict, procedure in zip(all_results_dicts.values(), all_procedures) for _ in range(10)],\n",
    "                                                               all_chosen_questions,\n",
    "                                                               answers=all_predicted_answers,\n",
    "                                                               previous_questions=all_previous_questions,\n",
    "                                                               previous_answers=all_previous_answers,\n",
    "                                                               mistake_labels=[results_dict['mistake'] for results_dict in all_results_dicts.values() for _ in range(10)],\n",
    "                                                               rephrase_batch_size=20)\n",
    "        \n",
    "    # Adjust all_coherence_metrics for the specific final turns we chose here\n",
    "    readjusted_all_coherence_metrics = {}\n",
    "    for k in all_coherence_metrics:\n",
    "        parallel_idx = 0\n",
    "        this_metrics = []\n",
    "        for results_dict in all_results_dicts.values():\n",
    "            for question_idx in range(results_dict['final_turn'] + 1):\n",
    "                if type(k) != str:\n",
    "                    this_metrics.append(max(round(float(all_coherence_metrics[k][parallel_idx]), 6), 0.0)) # If negative, just round up to 0.0 for aggregated metrics\n",
    "                else:\n",
    "                    this_metrics.append(all_coherence_metrics[k][parallel_idx])\n",
    "                parallel_idx += 1\n",
    "        readjusted_all_coherence_metrics[k] = this_metrics\n",
    "    \n",
    "    # Compile accuracy and coherence metrics\n",
    "    accuracy_metrics_by_threshold, coherence_metrics = compile_accuracy_and_coherence_metrics(all_labels, all_probs, readjusted_all_coherence_metrics, all_results_dicts, MISTAKE_DETECTION_THRESHOLDS, 0.1)\n",
    "    coherence_metrics_by_threshold = coherence_metrics['metrics_by_threshold']\n",
    "    \n",
    "    this_accuracy = accuracy_metrics_by_threshold['best_metrics']['accuracy']\n",
    "    if best_accuracy is None or this_accuracy > best_accuracy:\n",
    "        best_accuracy = this_accuracy\n",
    "        best_metrics = (accuracy_metrics_by_threshold, readjusted_all_coherence_metrics, coherence_metrics, coherence_metrics_by_threshold)\n",
    "        best_criteria = (mi, esd, cd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f076ab0e-fc0d-4d2e-a547-2f69c2a86f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metrics_by_threshold, readjusted_all_coherence_metrics, coherence_metrics, coherence_metrics_by_theshold = best_metrics\n",
    "\n",
    "# Save accuracy and coherence metrics\n",
    "json.dump(accuracy_metrics_by_threshold, \n",
    "        open(os.path.join(this_results_dir, f\"tuned_metrics_accuracy_val.json\"), \"w\"),\n",
    "        indent=4)\n",
    "\n",
    "json.dump(coherence_metrics, \n",
    "        open(os.path.join(this_results_dir, f\"tuned_metrics_coherence_nli_val.json\"), \"w\"),\n",
    "        indent=4)\n",
    "\n",
    "json.dump(readjusted_all_coherence_metrics, \n",
    "        open(os.path.join(this_results_dir, f\"tuned_metrics_coherence_raw_nli_val_tuned.json\"), \"w\"),\n",
    "        indent=4)            \n",
    "\n",
    "mi, esd, cd = best_criteria\n",
    "json.dump({\"max_iterations\": mi, \"early_stop_delta\": esd, \"confident_delta\": cd},\n",
    "          open(os.path.join(this_results_dir, \"tuned_stopping_criteria.json\"), \"w\"),\n",
    "          indent=4,\n",
    ")\n",
    "\n",
    "# Generate DET curves for accuracy\n",
    "generate_det_curve(accuracy_metrics_by_threshold, os.path.join(this_results_dir, f\"tuned_det_accuracy_val.pdf\"))\n",
    "\n",
    "# Generate curves for all metrics by threshold\n",
    "generate_tiered_metric_curves(MISTAKE_DETECTION_THRESHOLDS, \n",
    "                              [accuracy_metrics_by_threshold[t]['accuracy'] for t in MISTAKE_DETECTION_THRESHOLDS],\n",
    "                              [coherence_metrics_by_threshold[t]['consistency'] for t in MISTAKE_DETECTION_THRESHOLDS], \n",
    "                              [coherence_metrics_by_threshold[t]['verifiability'] for t in MISTAKE_DETECTION_THRESHOLDS],\n",
    "                              [os.path.join(this_results_dir, f\"tuned_graph_tiered_metrics_nli_val.pdf\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f70e66-fecc-468b-94ea-0a323d2ac1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel",
   "language": "python",
   "name": "travel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
