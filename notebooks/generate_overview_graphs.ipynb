{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37007fd5-7794-4cbf-8a2b-cd5e3d0a3d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir(\"/nfs/turbo/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl\")\n",
    "from travel import init_travel\n",
    "init_travel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b471f55-29df-463f-9c43-d90150eceb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Question Selection\n",
    "\n",
    "# InstructBLIP\n",
    "# RESULTS_PATH = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222_final/vqa_mistake_detection/ego4d_single_debug1000/instructblip-vicuna-7b/IterativeVQA_q4_ego4d_single_debug1000_instructblip-vicuna-7b_beam8-4_likelihood_nohistory_20241118142604\"\n",
    "# RESULTS_PATH = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222_final/vqa_mistake_detection/ego4d_single_debug1000/instructblip-vicuna-7b/IterativeVQA_q4_ego4d_single_debug1000_instructblip-vicuna-7b_beam8-4_likelihood_icl20_nohistory_20241118135132\"\n",
    "# RESULTS_PATH = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222_final/vqa_mistake_detection/ego4d_single_debug1000/instructblip-vicuna-7b/IterativeVQA_q10_ego4d_single_debug1000_instructblip-vicuna-7b_beam8-4_coherence_nohistory_20241114111416\"\n",
    "# RESULTS_PATH = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222_final/vqa_mistake_detection/ego4d_single_debug1000/instructblip-vicuna-7b/IterativeVQA_q6_ego4d_single_debug1000_instructblip-vicuna-7b_beam8-4_coherence_icl20_nohistory_20241114101726\"\n",
    "\n",
    "# LLaVA\n",
    "# RESULTS_PATH = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222_final/vqa_mistake_detection/ego4d_single_debug1000/llava-1.5-7b-hf/IterativeVQA_q4_ego4d_single_debug1000_llava-1.5-7b-hf_beam8-4_likelihood_nohistory_20241118085158\"\n",
    "# RESULTS_PATH = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222_final/vqa_mistake_detection/ego4d_single_debug1000/llava-1.5-7b-hf/IterativeVQA_q6_ego4d_single_debug1000_llava-1.5-7b-hf_beam8-4_likelihood_icl20_nohistory_20241118021122\"\n",
    "RESULTS_PATH = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222_final/vqa_mistake_detection/ego4d_single_debug1000/llava-1.5-7b-hf/IterativeVQA_q6_ego4d_single_debug1000_llava-1.5-7b-hf_beam8-4_coherence_nohistory_20241118085156\"\n",
    "# RESULTS_PATH = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222_final/vqa_mistake_detection/ego4d_single_debug1000/llava-1.5-7b-hf/IterativeVQA_q10_ego4d_single_debug1000_llava-1.5-7b-hf_beam8-4_coherence_icl20_nohistory_20241117141055\"\n",
    "\n",
    "# Llama 3\n",
    "# RESULTS_PATH = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222_final/vqa_mistake_detection/ego4d_single_debug1000/Llama-3.2-11B-Vision-Instruct/IterativeVQA_q10_ego4d_single_debug1000_Llama-3.2-11B-Vision-Instruct_beam8-4_likelihood_nohistory_20241118214334\"\n",
    "# RESULTS_PATH = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222_final/vqa_mistake_detection/ego4d_single_debug1000/Llama-3.2-11B-Vision-Instruct/IterativeVQA_q2_ego4d_single_debug1000_Llama-3.2-11B-Vision-Instruct_beam8-4_likelihood_icl20_nohistory_20241118213202\"\n",
    "# RESULTS_PATH = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222_final/vqa_mistake_detection/ego4d_single_debug1000/Llama-3.2-11B-Vision-Instruct/IterativeVQA_q8_ego4d_single_debug1000_Llama-3.2-11B-Vision-Instruct_beam8-4_coherence_nohistory_20241116144220\"\n",
    "# RESULTS_PATH = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222_final/vqa_mistake_detection/ego4d_single_debug1000/Llama-3.2-11B-Vision-Instruct/IterativeVQA_q6_ego4d_single_debug1000_Llama-3.2-11B-Vision-Instruct_beam8-4_coherence_icl20_nohistory_20241116144307\"\n",
    "\n",
    "# Question Generation\n",
    "\n",
    "# LLaVA\n",
    "# RESULTS_PATH = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222_final/vqa_mistake_detection/ego4d_single_debug1000/llava-1.5-7b-hf/IterativeVQA_q4_ego4d_single_debug1000_llava-1.5-7b-hf_beam8-4_likelihood_nohistory_dpo_20241127072827\"\n",
    "# RESULTS_PATH = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222_final/vqa_mistake_detection/ego4d_single_debug1000/llava-1.5-7b-hf/IterativeVQA_q4_ego4d_single_debug1000_llava-1.5-7b-hf_beam8-4_likelihood_icl20_nohistory_dpo_20241127072825\"\n",
    "# RESULTS_PATH = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222_final/vqa_mistake_detection/ego4d_single_debug1000/llava-1.5-7b-hf/IterativeVQA_q4_ego4d_single_debug1000_llava-1.5-7b-hf_beam8-4_coherence_nohistory_dpo_20241127072824\"\n",
    "# RESULTS_PATH = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222_final/vqa_mistake_detection/ego4d_single_debug1000/llava-1.5-7b-hf/IterativeVQA_q4_ego4d_single_debug1000_llava-1.5-7b-hf_beam8-4_coherence_icl20_nohistory_dpo_20241127084527\"\n",
    "\n",
    "# LLaVA + ICL\n",
    "# RESULTS_PATH = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222_final/vqa_mistake_detection/ego4d_single_debug1000/llava-1.5-7b-hf/IterativeVQA_q8_ego4d_single_debug1000_llava-1.5-7b-hf_beam8-4_likelihood_nohistory_dpo_20241126222249_icl\"\n",
    "# RESULTS_PATH = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222_final/vqa_mistake_detection/ego4d_single_debug1000/llava-1.5-7b-hf/IterativeVQA_q6_ego4d_single_debug1000_llava-1.5-7b-hf_beam8-4_likelihood_icl20_nohistory_dpo_20241126164933_icl\"\n",
    "# RESULTS_PATH = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222_final/vqa_mistake_detection/ego4d_single_debug1000/llava-1.5-7b-hf/IterativeVQA_q6_ego4d_single_debug1000_llava-1.5-7b-hf_beam8-4_coherence_nohistory_dpo_20241126224009_icl\"\n",
    "# RESULTS_PATH = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222_final/vqa_mistake_detection/ego4d_single_debug1000/llava-1.5-7b-hf/IterativeVQA_q4_ego4d_single_debug1000_llava-1.5-7b-hf_beam8-4_coherence_icl20_nohistory_dpo_20241126163926_icl\"\n",
    "\n",
    "EVAL_PARTITION = \"test\"\n",
    "\n",
    "with open(os.path.join(RESULTS_PATH, f\"outputs_{EVAL_PARTITION}.json\"), \"r\") as f:\n",
    "    all_results_dicts = json.load(f)\n",
    "    \n",
    "with open(os.path.join(RESULTS_PATH, f\"metrics_coherence_nli_{EVAL_PARTITION}.json\"), \"r\") as f:\n",
    "    coherence_metrics = json.load(f)\n",
    "    \n",
    "with open(os.path.join(RESULTS_PATH, f\"metrics_coherence_raw_nli_{EVAL_PARTITION}.json\"), \"r\") as f:\n",
    "    coherence_metrics_raw = json.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c7d8fc9-dbb9-48f7-b95a-7950d23cc978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sstorks/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "from travel.data.ego4d import Ego4DMistakeDetectionDataset\n",
    "\n",
    "dataset = Ego4DMistakeDetectionDataset(data_split=EVAL_PARTITION, \n",
    "                                       mismatch_augmentation=True,\n",
    "                                       multi_frame=False,\n",
    "                                       debug_n_examples_per_class=250 if EVAL_PARTITION == \"val\" else 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ebc0f0-fd53-4564-8882-fd18f767771e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prepare analysis data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ede322-30fe-4690-a612-542ee4099bd3",
   "metadata": {},
   "source": [
    "Get positive/negative form of informativeness for better visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "420becdd-8e48-4791-8e86-b9ad2e084ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.791992,\n",
      " -0.914062,\n",
      " 0.742188,\n",
      " 0.000977,\n",
      " 0.897949,\n",
      " 0.026367,\n",
      " 0.634766,\n",
      " 0.960449,\n",
      " 0.678711,\n",
      " 0.979492]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# TODO: once we have access to raw coherence metrics in test results, use 'informativeness_marginal_ref' in them to get positive and negative results - there may be some cases of failure of reasoning in the VLM this could expose\n",
    "k = 'informativeness_marginal_ref'\n",
    "unsure_range = 0.1\n",
    "\n",
    "example_inf = []\n",
    "\n",
    "parallel_idx = 0\n",
    "for example_idx, results_dict in enumerate(list(all_results_dicts.values())):\n",
    "    this_metrics = []\n",
    "    for question_idx in range(results_dict['final_turn'] + 1):\n",
    "        # this_inf = round(float(coherence_metrics['metrics_by_turn'][k][example_idx][question_idx]), 6)        \n",
    "        this_inf = round(float(coherence_metrics_raw[k][parallel_idx]), 6)\n",
    "        \n",
    "        this_metrics.append(this_inf)\n",
    "        \n",
    "        parallel_idx += 1\n",
    "            \n",
    "    # In metric for full example, don't count informativeness for \"unsure\" answers - model failed to get new information\n",
    "    this_metrics = [this_metrics[question_idx] if np.abs(results_dict['answer_probs'][question_idx][0] - 0.5) >= unsure_range or \"informativeness\" not in k else 0.0 for question_idx in range(len(this_metrics))]\n",
    "    example_metric = round(float(np.max(this_metrics)), 6)\n",
    "    # if example_metric < 0:\n",
    "    #     # If all the outputs are negative, use the most negative one instead\n",
    "    #     example_metric = round(float(np.min(this_metrics)), 6)\n",
    "    example_inf.append(example_metric)\n",
    "    \n",
    "pprint(example_inf[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c70c5ca-3135-42b8-b834-7b22a9759357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:10<00:00, 184.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean decision error: 0.37395540800000004\n",
      "Mean relevance: 0.6525766535\n",
      "Mean informativeness: 0.333015377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "from travel.model.mistake_detection import MISTAKE_DETECTION_THRESHOLDS\n",
    "\n",
    "accuracy = []\n",
    "informativeness = []\n",
    "relevance = []\n",
    "\n",
    "example_ids = []\n",
    "verbs = []\n",
    "nouns = []\n",
    "mistake_types = []\n",
    "\n",
    "labels = [] # These have to be filled in later if we want them\n",
    "graph_name = \"base\"\n",
    "\n",
    "assert len(all_results_dicts) == len(example_inf) == len(coherence_metrics['metrics_by_example']['relevance_marginal_by_example'])\n",
    "for (example_id, output), ex_informativeness, ex_relevance in tqdm(zip(all_results_dicts.items(), example_inf, coherence_metrics['metrics_by_example']['relevance_marginal_by_example']), total=len(all_results_dicts)):\n",
    "    target_success_prob = 0.0 if output['mistake'] else 1.0\n",
    "    actual_success_prob = output['success_probs'][output['final_turn']]\n",
    "        \n",
    "    accuracy.append(np.abs(target_success_prob - actual_success_prob))\n",
    "    relevance.append(ex_relevance)\n",
    "    informativeness.append(ex_informativeness)\n",
    "    \n",
    "    example_ids.append(example_id)\n",
    "    example = dataset.get_example_by_id(example_id, load_frames=False)\n",
    "    verbs.append(example.verb_noun_pair[0].split(\"_\")[0])\n",
    "    nouns.append(example.verb_noun_pair[1].split(\"_\")[0])\n",
    "    mistake_types.append(output['mistake_type'])\n",
    "    \n",
    "# pprint(list(zip(accuracy, relevance, informativeness, example_ids, verbs, nouns))[:10])\n",
    "\n",
    "mean_accuracy = np.mean(accuracy)\n",
    "mean_relevance = np.mean(relevance)\n",
    "mean_informativeness = np.mean(informativeness)\n",
    "\n",
    "print(\"Mean decision error:\", mean_accuracy)\n",
    "print(\"Mean relevance:\", mean_relevance)\n",
    "print(\"Mean informativeness:\", mean_informativeness)\n",
    "\n",
    "json.dump({\"mean_error\": mean_accuracy, \"mean_relevance\": mean_relevance, \"mean_informativeness\": mean_informativeness},\n",
    "          open(os.path.join(RESULTS_PATH, f\"metrics_3d_graph_{EVAL_PARTITION}.json\"), \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31210c18-47b3-4bab-82f8-b00975395d2f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Generate graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baf9e60-33a4-45e9-86c3-65713728b5d4",
   "metadata": {},
   "source": [
    "Graph of data dlabels = []\n",
    "istribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b59bb478-f761-46d9-ba60-94b412ab6c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_959075/4109331550.py:64: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_959075/4109331550.py:67: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# Sample data\n",
    "x = accuracy\n",
    "y = relevance\n",
    "z = informativeness\n",
    "\n",
    "# Normalize the values for coloring\n",
    "# x_norm = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "# y_norm = (y - np.min(y)) / (np.max(y) - np.min(y))\n",
    "# z_norm = (z - np.min(z)) / (np.max(z) - np.min(z))\n",
    "\n",
    "x_norm = x\n",
    "y_norm = y\n",
    "z_norm = (np.array(z) + 1.0) / 2.0\n",
    "\n",
    "# Combine the normalized values to get the colors\n",
    "colors = np.array([x_norm, y_norm, z_norm]).T\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))  # Increase the figure size\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Create the scatter plot\n",
    "ax.scatter(x, y, z, c=colors, s=150, edgecolor='k', linewidth=0.25, alpha=0.5)\n",
    "\n",
    "# Plot mean point\n",
    "# ax.scatter(np.mean(x), np.mean(y), np.mean(z), c='k', marker='*', alpha=1.0, linewidth=0.25, s=500) # [np.mean(x_norm), np.mean(y_norm), np.mean(z_norm)]\n",
    "\n",
    "# Data labels\n",
    "if len(labels) > 0:\n",
    "    for i in range(len(x)):\n",
    "        ax.text(x[i], y[i], z[i] + 0.03, labels[i], size=8, zorder=1, color='k')\n",
    "\n",
    "# Set custom z-axis tick labels\n",
    "xy_ticks = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "ax.set_xticks(xy_ticks)\n",
    "ax.set_yticks(xy_ticks)\n",
    "ax.set_xticklabels([f'{tick:.1f}' for tick in xy_ticks], fontsize=12)\n",
    "ax.set_yticklabels([f'{tick:.1f}' for tick in xy_ticks], fontsize=12)\n",
    "\n",
    "z_ticks = [-1.0, -0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "ax.set_zticks(z_ticks)\n",
    "ax.set_zticklabels([f'{tick:.1f}' for tick in z_ticks], fontsize=12)\n",
    "        \n",
    "# Set axis labels\n",
    "ax.set_xlabel(f'Decision Error', labelpad=6, fontsize=20, fontweight='bold')\n",
    "ax.set_ylabel(f'Relevance', labelpad=6, fontsize=20, fontweight='bold')\n",
    "ax.set_zlabel(f'Informativeness', labelpad=6, fontsize=20, fontweight='bold')\n",
    "\n",
    "ax.xaxis.label.set_color('#AA0000')\n",
    "ax.yaxis.label.set_color('#00AA00')\n",
    "ax.zaxis.label.set_color('#0000AA')\n",
    "\n",
    "# Set axis limits\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_zlim(-1, 1)\n",
    "ax.set_box_aspect(aspect=None, zoom=0.94)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(RESULTS_PATH, f\"3d_graph_{graph_name}.pdf\"), bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28d8c2d-2698-4553-8b81-863e15e77420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db93fe12-6bc7-4519-8f7c-730afb80b319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f17abb-91e5-4348-afb9-7d5f7fa1ec41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "962c3670-9d50-4450-8503-175a2169b330",
   "metadata": {},
   "source": [
    "Gather data by verb, noun, and verb-noun pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a3e1c5-85ec-4f2c-a8f1-8ed447587a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "acc_by_verb = defaultdict(list)\n",
    "acc_by_noun = defaultdict(list)\n",
    "acc_by_vnpair = defaultdict(list)\n",
    "\n",
    "rel_by_verb = defaultdict(list)\n",
    "rel_by_noun = defaultdict(list)\n",
    "rel_by_vnpair = defaultdict(list)\n",
    "\n",
    "inf_by_verb = defaultdict(list)\n",
    "inf_by_noun = defaultdict(list)\n",
    "inf_by_vnpair = defaultdict(list)\n",
    "\n",
    "verb_counts = Counter()\n",
    "noun_counts = Counter()\n",
    "vnpair_counts = Counter()\n",
    "\n",
    "for acc, rel, inf, verb, noun in zip(accuracy, relevance, informativeness, verbs, nouns):\n",
    "    acc_by_verb[verb].append(acc)\n",
    "    acc_by_noun[noun].append(acc)\n",
    "    acc_by_vnpair[(verb, noun)].append(acc)\n",
    "    \n",
    "    rel_by_verb[verb].append(rel)\n",
    "    rel_by_noun[noun].append(rel)\n",
    "    rel_by_vnpair[(verb, noun)].append(rel)\n",
    "    \n",
    "    inf_by_verb[verb].append(inf)\n",
    "    inf_by_noun[noun].append(inf)\n",
    "    inf_by_vnpair[(verb, noun)].append(inf)\n",
    "    \n",
    "    verb_counts[verb] += 1\n",
    "    noun_counts[noun] += 1\n",
    "    vnpair_counts[(verb, noun)] += 1            \n",
    "\n",
    "# Get average metrics for each verb, noun, VN pair\n",
    "for d in [acc_by_verb, acc_by_noun, acc_by_vnpair, rel_by_verb, rel_by_noun, rel_by_vnpair, inf_by_verb, inf_by_noun, inf_by_vnpair]:\n",
    "    for k in d:\n",
    "        d[k] = np.mean(d[k])\n",
    "\n",
    "pprint(list(acc_by_verb.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5284aa-276f-46c4-a758-58c4d5960088",
   "metadata": {},
   "source": [
    "Replace scatter plot data lists with gathered verb-noun data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2aeaa9-0c43-4501-a77f-17e974cb6541",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list([k for k, _ in verb_counts.most_common()])\n",
    "accuracy = [acc_by_verb[k] for k in labels]\n",
    "relevance = [rel_by_verb[k] for k in labels]\n",
    "informativeness = [inf_by_verb[k] for k in labels]\n",
    "\n",
    "xtitle = \"Verb\"\n",
    "graph_name = \"verbs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e021e0d2-f1d1-4ed7-8c6d-c63f07f15af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list([k for k, _ in noun_counts.most_common()])\n",
    "accuracy = [acc_by_noun[k] for k in labels]\n",
    "relevance = [rel_by_noun[k] for k in labels]\n",
    "informativeness = [inf_by_noun[k] for k in labels]\n",
    "\n",
    "xtitle = \"Noun\"\n",
    "graph_name = \"nouns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d61faf8-eb6c-4edd-8fa0-1b0e8f7a82fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list([k for k, _ in vnpair_counts.most_common()])\n",
    "accuracy = [acc_by_vnpair[k] for k in labels]\n",
    "relevance = [rel_by_vnpair[k] for k in labels]\n",
    "informativeness = [inf_by_vnpair[k] for k in labels]\n",
    "\n",
    "labels = [f\"{v} {n}\" for v, n in labels]\n",
    "\n",
    "xtitle = \"Verb-Noun Pair\"\n",
    "graph_name = \"verbs_nouns\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d35b6ff-f918-45e4-a924-5bf97d30117f",
   "metadata": {},
   "source": [
    "Break down by mistake type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb35b27f-efec-4ad7-b07a-48e53553b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_mistake_type = None\n",
    "# target_mistake_type = \"Action Incomplete\"\n",
    "# target_mistake_type = \"MisalignSRL_V\"\n",
    "# target_mistake_type = \"MisalignSRL_ARG1\"\n",
    "target_mistake_type = \"MisalignSRL_V_ARG1\"\n",
    "\n",
    "accuracy = [acc for acc, mt in zip(accuracy, mistake_types) if mt == target_mistake_type]\n",
    "relevance = [rel for rel, mt in zip(relevance, mistake_types) if mt == target_mistake_type]\n",
    "informativeness = [inf for inf, mt in zip(informativeness, mistake_types) if mt == target_mistake_type]\n",
    "    \n",
    "print(len(accuracy), \"points recovered\")\n",
    "    \n",
    "xtitle = None # Not implemented for bar graphs\n",
    "graph_name = f\"mistake_type_{target_mistake_type}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b32f5d-9167-4df2-9ff8-daa2ce59d82c",
   "metadata": {},
   "source": [
    "2D plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12c94e4-3912-4523-911a-640ce5765045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Sample data\n",
    "x = accuracy\n",
    "y = np.array(relevance) * np.array(informativeness)\n",
    "\n",
    "# Normalize the values for coloring\n",
    "x_norm = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "y_norm = (y - np.min(y)) / (np.max(y) - np.min(y))\n",
    "\n",
    "# Combine the normalized values to get the colors\n",
    "colors = np.array([x_norm, y_norm, y_norm]).T # For more informative/relevant instances, color should shift to more cyan\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))  # Increase the figure size\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Create the scatter plot\n",
    "for i in range(len(x)):\n",
    "    ax.scatter(x[i], y[i], c=[colors[i]], s=50, edgecolor='k', linewidth=1)\n",
    "\n",
    "if len(labels) > 0:\n",
    "    for i in range(len(x)):\n",
    "        ax.text(x[i], y[i] + 0.01, labels[i], size=8, zorder=1, color='k')\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xlabel('Decision Error', labelpad=5, fontsize=16, fontweight='bold')\n",
    "ax.set_ylabel('Verifiability', labelpad=5, fontsize=16, fontweight='bold')\n",
    "\n",
    "ax.xaxis.label.set_color('#AA0000')\n",
    "ax.yaxis.label.set_color('#00AAAA')\n",
    "\n",
    "# Set axis limits\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(-1, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54e3440-7dc9-43f2-96f1-d8656cfff4fa",
   "metadata": {},
   "source": [
    "Bar graph of verb noun data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e57ef27-1daa-4b73-86ee-becea1f8f360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example lists\n",
    "# TODO: we should probably use \"multi tiered\" versions of these metrics here\n",
    "A = accuracy[:50]\n",
    "B = relevance[:50]\n",
    "C = np.array(informativeness[:50]) * np.array(relevance[:50])\n",
    "\n",
    "# Number of tuples\n",
    "n = len(A)\n",
    "\n",
    "# Define the positions of the bars on the x-axis\n",
    "indices = np.arange(n)\n",
    "\n",
    "# Width of a single bar\n",
    "bar_width = 0.5\n",
    "\n",
    "# Plot bars\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.bar(indices, A, width=bar_width, color='red', alpha=0.8, label='Accuracy')\n",
    "plt.bar(indices, B, width=bar_width, color='blue', alpha=0.8, label='Relevance')\n",
    "plt.bar(indices, C, width=bar_width, color='yellow', alpha=0.8, label='Informativeness')\n",
    "\n",
    "# Set labels and title\n",
    "plt.ylabel('Mean Metric Value')\n",
    "\n",
    "# Remove the x-axis tick labels\n",
    "plt.xticks([])\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Add individual labels under each set of bars\n",
    "for i, label in enumerate(labels[:50]):\n",
    "    plt.text(indices[i], -0.01, label, ha='center', va='top', rotation=90)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3554fbd-5239-4a5f-ac17-127a5678939d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Visualize specific outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1abdce-d3bb-432b-bafe-201c53c24554",
   "metadata": {},
   "source": [
    "Next: grab example outputs from different regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "482693d9-d511-4beb-8cb8-59865eeae810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good: 333\n",
      "bad: 27\n",
      "unverified: 21\n",
      "informative_irrelevant: 7\n",
      "relevant_uninformative: 52\n",
      "hallucination: 255\n",
      "knowledge_deficiency: 6\n"
     ]
    }
   ],
   "source": [
    "good = []\n",
    "bad = []\n",
    "unverified = []\n",
    "informative_irrelevant = []\n",
    "relevant_uninformative = []\n",
    "hallucination = []\n",
    "knowledge_deficiency = []\n",
    "\n",
    "for output, err, rel, inf in zip(all_results_dicts.values(), accuracy, relevance, informativeness):\n",
    "    \n",
    "    acc = 1 - err\n",
    "    \n",
    "    if acc > 0.8 and rel > 0.8 and inf > 0.8: # cyan points\n",
    "        good.append(output)\n",
    "        \n",
    "    elif acc < 0.2 and rel < 0.2 and inf < 0.2: # red points\n",
    "        bad.append(output)\n",
    "    \n",
    "    elif acc > 0.8 and rel < 0.2 and inf < 0.2: # black points\n",
    "        unverified.append(output)\n",
    "        \n",
    "    elif acc > 0.8 and rel < 0.2 and inf > 0.8: # blue points\n",
    "        informative_irrelevant.append(output)\n",
    "        \n",
    "    elif acc > 0.5 and rel > 0.7 and inf < 0.1 and inf > -0.1: # green points\n",
    "        relevant_uninformative.append(output)\n",
    "    \n",
    "    elif acc < 0.3 and rel > 0.7 and inf < 0.0: # yellow points\n",
    "        hallucination.append(output)\n",
    "    \n",
    "    elif acc < 0.3 and rel > 0.7 and inf > 0.7: # white points\n",
    "        knowledge_deficiency.append(output)\n",
    "        \n",
    "print(\"good:\", len(good))\n",
    "print(\"bad:\", len(bad))\n",
    "print(\"unverified:\", len(unverified))\n",
    "print(\"informative_irrelevant:\", len(informative_irrelevant))\n",
    "print(\"relevant_uninformative:\", len(relevant_uninformative))\n",
    "print(\"hallucination:\", len(hallucination))\n",
    "print(\"knowledge_deficiency:\", len(knowledge_deficiency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac1cf112-a42d-4bab-9552-c8cc541afc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# outputs_to_visualize = good\n",
    "# example_type = \"good\"\n",
    "\n",
    "# outputs_to_visualize = bad\n",
    "# example_type = \"bad\"\n",
    "\n",
    "# outputs_to_visualize = unverified\n",
    "# example_type = \"unverified\"\n",
    "\n",
    "# outputs_to_visualize = informative_irrelevant\n",
    "# example_type = \"informative_irrelevant\"\n",
    "\n",
    "# outputs_to_visualize = relevant_uninformative\n",
    "# example_type = \"relevant_uninformative\"\n",
    "\n",
    "outputs_to_visualize = hallucination\n",
    "example_type = \"hallucination\"\n",
    "\n",
    "# outputs_to_visualize = knowledge_deficiency\n",
    "# example_type = \"knowledge_deficiency\"\n",
    "\n",
    "print(len(outputs_to_visualize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ba24a-3739-4d9f-a1a6-61f287596a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "frames = []\n",
    "for oi, output in enumerate(outputs_to_visualize):\n",
    "    \n",
    "    # if \"the tap\" not in output['procedure']:\n",
    "        # continue\n",
    "    \n",
    "    frame_dir = os.path.join(output['frame_dir'], \"frames\")\n",
    "    # print(frame_dir)\n",
    "    possible_frame_dirs = [frame_dir] + [frame_dir.replace(\"_debug1000\",\"\")] + [frame_dir.replace(\"_debug1000\", f\"_partition{i+1}of4\") for i in range(4)]\n",
    "    for pfd in possible_frame_dirs:\n",
    "        try:\n",
    "            frame_path = os.path.join(pfd, os.listdir(pfd)[0]) # only a single frame in frame dir\n",
    "            frame = Image.open(frame_path)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    if frame is not None:\n",
    "        frames.append(frame)\n",
    "        plt.figure()\n",
    "        plt.imshow(frame)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    print(oi)\n",
    "    print(f\"Procedure: {output['procedure']} ({'mistake' if output['mistake'] else 'success'}: {output['mistake_type']})\")\n",
    "    print(\"\")\n",
    "    for i in range(output['final_turn'] + 1):     \n",
    "        \n",
    "        print(f\"Q{i+1}: {output['questions'][i]}\")\n",
    "        print(f\"A{i+1}: {output['answers'][i]} ({output['answer_probs'][i]})\")\n",
    "        print(f\"Success probability {i+1}: {output['success_probs'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fef348d2-7efd-4ae1-8135-deb8d365c87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "selected_idx = 9\n",
    "\n",
    "# Save data for sample outputs\n",
    "example_dir = os.path.join(RESULTS_PATH, \"sample_outputs\", example_type)\n",
    "if not os.path.exists(example_dir):\n",
    "    os.makedirs(example_dir)\n",
    "\n",
    "frames[selected_idx].save(os.path.join(example_dir, f\"frame_{selected_idx}.png\"))\n",
    "json.dump(outputs_to_visualize[selected_idx], open(os.path.join(example_dir, f\"metadata_{selected_idx}.json\"), \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1512146a-de5e-4fa0-94c7-5d7a9f1a5a94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d15115e-17c0-4239-a6b5-bb647578c8be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel",
   "language": "python",
   "name": "travel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
