{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "609598b0-4653-4b42-85ba-2a38a01e4300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir(\"/nfs/turbo/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl\")\n",
    "from travel import init_travel\n",
    "init_travel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dfe59f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Load preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26f0084f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_probs': [[0.010489, 0.989511],\n",
      "                  [0.7773, 0.2227],\n",
      "                  [0.007233, 0.992767],\n",
      "                  [0.651355, 0.348645],\n",
      "                  [0.307358, 0.692642],\n",
      "                  [0.33112, 0.66888],\n",
      "                  [0.573678, 0.426322],\n",
      "                  [0.73412, 0.26588],\n",
      "                  [0.538983, 0.461017],\n",
      "                  [0.72487, 0.27513]],\n",
      " 'answers': ['Yes',\n",
      "             'No',\n",
      "             'Yes',\n",
      "             'No',\n",
      "             'Yes',\n",
      "             'Yes',\n",
      "             'Unsure',\n",
      "             'No',\n",
      "             'Unsure',\n",
      "             'No'],\n",
      " 'candidate_questions': [['Is the person wearing a lab coat?',\n",
      "                          'Is the person wearing gloves?',\n",
      "                          'Is the person wearing a white lab coat?'],\n",
      "                         ['Is the person wearing safety goggles?',\n",
      "                          'Is the person wearing gloves?'],\n",
      "                         ['Is the person wearing gloves?',\n",
      "                          'Is the person using a pipette to put the liquid in '\n",
      "                          'the paper?'],\n",
      "                         ['Is the person using a pipette to put the liquid in '\n",
      "                          'the paper?',\n",
      "                          'Is the person using a syringe to put the liquid in '\n",
      "                          'the paper?',\n",
      "                          'Is the person wearing a hat?',\n",
      "                          'Is the person using a syringe to put the liquid '\n",
      "                          'into the paper?'],\n",
      "                         ['Is the person using a pipette to put the liquid in '\n",
      "                          'the paper?',\n",
      "                          'Is the person using a dropper to put the liquid in '\n",
      "                          'the paper?'],\n",
      "                         ['Is the person using a pipette to put the liquid in '\n",
      "                          'the paper?',\n",
      "                          'Is the paper on a table?'],\n",
      "                         ['Is the person using a test tube to put the liquid '\n",
      "                          'in the paper?',\n",
      "                          'Is the person using a microscope to put the liquid '\n",
      "                          'in the paper?'],\n",
      "                         ['Is the person using a test tube to put the liquid '\n",
      "                          'in the paper?',\n",
      "                          'Is the person using a beaker to put the liquid in '\n",
      "                          'the paper?',\n",
      "                          'Is the person using a spoon to put the liquid in '\n",
      "                          'the paper?'],\n",
      "                         ['Is the person using a funnel to put the liquid in '\n",
      "                          'the paper?',\n",
      "                          'Is the person using a measuring cup to put the '\n",
      "                          'liquid in the paper?',\n",
      "                          'Is the person using a spatula to put the liquid in '\n",
      "                          'the paper?'],\n",
      "                         ['Is the person using a beaker to put the liquid in '\n",
      "                          'the paper?',\n",
      "                          'Is the person using a funnel to put the liquid in '\n",
      "                          'the paper?',\n",
      "                          'Is the person using a test tube to put the liquid '\n",
      "                          'in the paper?',\n",
      "                          'Is the person using a ruler to put the liquid in '\n",
      "                          'the paper?']],\n",
      " 'candidate_questions_scores': [[-2.6088404655456543,\n",
      "                                 -2.8505301475524902,\n",
      "                                 -2.3957202434539795],\n",
      "                                [-1.6824370622634888, -2.127070188522339],\n",
      "                                [-2.0017526149749756, -3.3623712062835693],\n",
      "                                [-4.940241813659668,\n",
      "                                 -4.340146541595459,\n",
      "                                 -4.483624458312988,\n",
      "                                 -4.494911193847656],\n",
      "                                [-4.695842742919922, -4.503211498260498],\n",
      "                                [-5.8441314697265625, -6.727949142456055],\n",
      "                                [-5.058520793914795, -4.772224426269531],\n",
      "                                [-4.7559661865234375,\n",
      "                                 -5.156332015991211,\n",
      "                                 -4.646953582763672],\n",
      "                                [-6.693734645843506,\n",
      "                                 -6.228703498840332,\n",
      "                                 -6.335910797119141],\n",
      "                                [-6.90987491607666,\n",
      "                                 -6.865169048309326,\n",
      "                                 -6.455591201782227,\n",
      "                                 -6.331698894500732]],\n",
      " 'candidate_questions_sources': [['vlm', 'vlm', 'vlm'],\n",
      "                                 ['vlm', 'vlm'],\n",
      "                                 ['vlm', 'vlm'],\n",
      "                                 ['vlm', 'vlm', 'vlm', 'vlm'],\n",
      "                                 ['vlm', 'vlm'],\n",
      "                                 ['vlm', 'vlm'],\n",
      "                                 ['vlm', 'vlm'],\n",
      "                                 ['vlm', 'vlm', 'vlm'],\n",
      "                                 ['vlm', 'vlm', 'vlm'],\n",
      "                                 ['vlm', 'vlm', 'vlm', 'vlm']],\n",
      " 'example_id': '90093217-6a61-48ce-b737-e581499cf491/144/pos',\n",
      " 'final_success_prob': 0.256832,\n",
      " 'final_turn': 2,\n",
      " 'frame_dir': '/nfs/turbo/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_generated_data_0808/ego4d_val_seed222_mismatch_debug250/90093217-6a61-48ce-b737-e581499cf491/144/pos',\n",
      " 'informativeness': [0.178223, 0.270508, 0.03418],\n",
      " 'informativeness_marginal': [0.178223, 0.064453, 0.147461],\n",
      " 'informativeness_marginal_x_relevance_marginal': [0.013748, 0.01004, 0.019302],\n",
      " 'informativeness_marginal_x_relevance_marginal_ref': [0.0, 0.0, 0.0],\n",
      " 'mistake': False,\n",
      " 'mistake_type': None,\n",
      " 'procedure': 'Put liquid in the paper',\n",
      " 'questions': ['Is the person wearing a white lab coat?',\n",
      "               'Is the person wearing safety goggles?',\n",
      "               'Is the person wearing gloves?',\n",
      "               'Is the person using a syringe to put the liquid in the paper?',\n",
      "               'Is the person using a dropper to put the liquid in the paper?',\n",
      "               'Is the person using a pipette to put the liquid in the paper?',\n",
      "               'Is the person using a microscope to put the liquid in the '\n",
      "               'paper?',\n",
      "               'Is the person using a spoon to put the liquid in the paper?',\n",
      "               'Is the person using a measuring cup to put the liquid in the '\n",
      "               'paper?',\n",
      "               'Is the person using a ruler to put the liquid in the paper?'],\n",
      " 'relevance_marginal': [0.077148, 0.155762, 0.130859],\n",
      " 'scores': [-2.3957202434539795,\n",
      "            -1.6824370622634888,\n",
      "            -2.0017526149749756,\n",
      "            -4.340146541595459,\n",
      "            -4.503211498260498,\n",
      "            -5.8441314697265625,\n",
      "            -4.772224426269531,\n",
      "            -4.646953582763672,\n",
      "            -6.228703498840332,\n",
      "            -6.331698894500732],\n",
      " 'success_probs': [0.185947,\n",
      "                   0.182426,\n",
      "                   0.256832,\n",
      "                   0.236516,\n",
      "                   0.586996,\n",
      "                   0.635222,\n",
      "                   0.569853,\n",
      "                   0.605799,\n",
      "                   0.592667,\n",
      "                   0.581303],\n",
      " 'success_probs_negated': [0.138462,\n",
      "                           0.388618,\n",
      "                           0.115961,\n",
      "                           0.731059,\n",
      "                           0.187133,\n",
      "                           0.70253,\n",
      "                           0.699254,\n",
      "                           0.754915,\n",
      "                           0.731059,\n",
      "                           0.737158]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "RESULTS_PATH_LIKELIHOOD = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222/vqa_mistake_detection/ego4d_single_debug250/llava-1.5-7b-hf/IterativeVQA_q10_ego4d_single_debug250_llava-1.5-7b-hf_beam8-4_likelihood_nohistory_20240815204213\"\n",
    "preds_l = json.load(open(os.path.join(RESULTS_PATH_LIKELIHOOD, \"outputs_val.json\"), \"r\"))\n",
    "metrics_l = json.load(open(os.path.join(RESULTS_PATH_LIKELIHOOD, \"metrics_coherence_nli_val.json\"), \"r\"))\n",
    "combined_l = [v | {\"example_id\": k, \n",
    "                   \"relevance_marginal\": metrics_l['metrics_by_turn']['relevance_marginal_by_turn'][i], \n",
    "                   \"informativeness\": metrics_l['metrics_by_turn']['informativeness_by_turn'][i], \n",
    "                   \"informativeness_marginal\": metrics_l['metrics_by_turn']['informativeness_marginal_by_turn'][i],\n",
    "                   \"informativeness_marginal_x_relevance_marginal\": metrics_l['metrics_by_turn']['informativeness_marginal_x_relevance_marginal_by_turn'][i],\n",
    "                   \"informativeness_marginal_x_relevance_marginal_ref\": metrics_l['metrics_by_turn']['informativeness_marginal_x_relevance_marginal_ref_by_turn'][i]\n",
    "                  } for i, (k, v) in enumerate(preds_l.items())]\n",
    "\n",
    "combined_c = []\n",
    "RESULTS_PATH_COHERENCE = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222/vqa_mistake_detection/ego4d_single_debug250/llava-1.5-7b-hf/IterativeVQA_q10_ego4d_single_debug250_llava-1.5-7b-hf_beam8-4_coherence_icl20_nohistory_20240815204213\"\n",
    "preds_c = json.load(open(os.path.join(RESULTS_PATH_COHERENCE, \"outputs_val.json\"), \"r\"))\n",
    "metrics_c = json.load(open(os.path.join(RESULTS_PATH_COHERENCE, \"metrics_coherence_nli_val.json\"), \"r\"))\n",
    "combined_c = [v | {\"example_id\": k, \n",
    "                   \"informativeness\": metrics_c['metrics_by_turn']['informativeness_by_turn'][i], \n",
    "                   \"relevance_marginal\": metrics_c['metrics_by_turn']['relevance_marginal_by_turn'][i], \n",
    "                   \"informativeness_marginal\": metrics_c['metrics_by_turn']['informativeness_marginal_by_turn'][i],\n",
    "                   \"informativeness_marginal_x_relevance_marginal\": metrics_c['metrics_by_turn']['informativeness_marginal_x_relevance_marginal_by_turn'][i],\n",
    "                   \"informativeness_marginal_x_relevance_marginal_ref\": metrics_c['metrics_by_turn']['informativeness_marginal_x_relevance_marginal_ref_by_turn'][i],\n",
    "                  } for i, (k, v) in enumerate(preds_c.items())]\n",
    "\n",
    "all_data = combined_l + combined_c\n",
    "\n",
    "pprint(all_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eec2d81-c7c5-483d-bbb1-e9535d28a6be",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Select turns and candidates to score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fbf567-d550-49af-835d-06a95cc1dac2",
   "metadata": {},
   "source": [
    "Relevance study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bcaca06-bf06-45d4-8251-724de8603281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "from travel import set_random_seed\n",
    "from travel.constants import RANDOM_SEED, RESULTS_DIR\n",
    "\n",
    "N_SAMPLES = 100\n",
    "SAMPLES_PER_ANNOTATOR = 10\n",
    "\n",
    "timestamp = datetime.datetime.now()\n",
    "output_dir = os.path.join(\"annotation_files\", f\"relevance_{timestamp.strftime('%Y%m%d%H%M%S')}\")\n",
    "os.makedirs(output_dir)\n",
    "\n",
    "random_seed = 555\n",
    "set_random_seed(random_seed) # Set seed to 555 for relevance samples\n",
    "\n",
    "example_idxs = random.sample(list(range(len(all_data))), N_SAMPLES)\n",
    "\n",
    "data_dicts = []\n",
    "for example_idx in example_idxs:   \n",
    "    output = all_data[example_idx]\n",
    "        \n",
    "    example_id = output['example_id']\n",
    "\n",
    "    n_turns = output['final_turn'] + 1\n",
    "    selected_turn = random.choice(list(range(n_turns)))\n",
    "\n",
    "    selected_question = output['questions'][selected_turn]\n",
    "    selected_answer = np.argmax(output['answer_probs'][selected_turn])\n",
    "    assert selected_answer == 0 or selected_answer == 1\n",
    "    selected_answer = \"Yes\" if selected_answer == 1 else \"No\"\n",
    "\n",
    "    previous_questions = [q for q, a in zip(output['questions'][:selected_turn], output['answers'][:selected_turn]) if a != \"Unsure\"]\n",
    "    previous_answers = [a for a in output['answers'][:selected_turn] if a != \"Unsure\"]\n",
    "\n",
    "    actual_relevance_marginal = output['relevance_marginal'][selected_turn]\n",
    "    actual_informativeness = output['informativeness'][selected_turn]\n",
    "    actual_informativeness_marginal = output['informativeness_marginal'][selected_turn]\n",
    "    max_informativeness_marginal_so_far = max([turn_metric for turn_idx, turn_metric in enumerate(output['informativeness_marginal'][:selected_turn+1]) if output['answers'][turn_idx] != \"Unsure\" or turn_idx == selected_turn])\n",
    "    max_informativeness_marginal_so_far_x_mean_relevance_marginal = round(max_informativeness_marginal_so_far * np.mean(output['relevance_marginal'][:selected_turn + 1]), 6)\n",
    "    actual_informativeness_marginal_x_relevance_marginal = output['informativeness_marginal_x_relevance_marginal'][selected_turn]\n",
    "    actual_informativeness_marginal_x_relevance_marginal_ref = output['informativeness_marginal_x_relevance_marginal_ref'][selected_turn]\n",
    "                                  \n",
    "    data_dicts.append(\n",
    "        {\n",
    "            \"example_id\": example_id,\n",
    "            \"procedure\": output['procedure'],\n",
    "            \"question\": selected_question,\n",
    "            \"answer\": selected_answer,\n",
    "            \"previous_questions_answers\": [(q, a) for q, a in zip(previous_questions, previous_answers)],\n",
    "            \"nli_metric_scores\": {\n",
    "                \"relevance_marginal\": actual_relevance_marginal,\n",
    "                \"informativeness\": actual_informativeness,\n",
    "                \"informativeness_x_relevance_marginal\": actual_informativeness * actual_relevance_marginal, # TODO: later can just get this directly from metrics\n",
    "                \"informativeness_marginal\": actual_informativeness_marginal,\n",
    "                \"max_informativeness_marginal_so_far\": max_informativeness_marginal_so_far,\n",
    "                \"max_informativeness_marginal_so_far_x_mean_relevance_marginal\": max_informativeness_marginal_so_far_x_mean_relevance_marginal,\n",
    "                \"informativeness_marginal_x_relevance_marginal\": actual_informativeness_marginal_x_relevance_marginal,\n",
    "                \"informativeness_marginal_x_relevance_marginal_ref\": actual_informativeness_marginal_x_relevance_marginal_ref,\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "json.dump(data_dicts, open(os.path.join(output_dir, f\"relevance_sample{random_seed}.json\"), \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea9c6066-4fdc-4ab5-abd7-8b9e97d73f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide sampled data into a txt file for each annotator\n",
    "for annotator_idx in range(N_SAMPLES // SAMPLES_PER_ANNOTATOR):\n",
    "    lines = []\n",
    "\n",
    "    samples = data_dicts[annotator_idx * SAMPLES_PER_ANNOTATOR:(annotator_idx + 1) * SAMPLES_PER_ANNOTATOR]\n",
    "    assert len(samples) == SAMPLES_PER_ANNOTATOR\n",
    "    \n",
    "    lines.append(\"Imagine you just had eye surgery, and are unable to see. You're performing a task you're familiar with, but need help to determine whether you successfully completed it.\")\n",
    "    lines.append(\"You video call a friend (who is unfamiliar with the task) and show them what you're working on. You then ask them some yes/no questions to figure out whether you successfully completed the task.\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"For each annotation task, you will be given the following information:\")\n",
    "    lines.append(\" - A sentence describing the procedure you're trying to perform.\")\n",
    "    lines.append(\" - An optional list of previous questions you already asked, and their answers.\")\n",
    "    lines.append(\" - A potential question you could ask your friend next.\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"You must rate how relevant the potential next question is. By relevant, we mean: given the previous questions and answers, how helpful could an answer to this question be in determining whether the procedure has been completed?\")\n",
    "    lines.append(\"You can also choose to mark 'Instructions Unclear', which means that the instructional text itself is not clear, so you're not sure how to determine whether it's successful. This should only be used in rare cases.\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"Some tips:\")\n",
    "    lines.append(\" - Only judge the relevance of the potential next question, not the previous questions (which may or may not be relevant).\")\n",
    "    lines.append(\" - A question may seem relevant to the task at hand, but you should consider it irrelevant if it doesn't provide essential information to judge the success of the task.\")\n",
    "    lines.append(\" - If a seemingly relevant question is redundant with previous questions, you may consider it less relevant.\")\n",
    "    lines.append(\" - Assume that the answer to the question won't contradict the information you have from previous questions and answers. If previous questions and answers already contradict each other, consider whether this question could sway you one way or another.\")\n",
    "    lines.append(' - The instructional text and questions may refer to \"someone\" or a \"person\"; always assume this is referring to yourself (the person performing the task).')\n",
    "    lines.append(' - The questions may refer to a \"photo\" or \"image\"; always assume this is referring to the video feed your friend would see through the video call.')\n",
    "    lines.append(\"\\n\")\n",
    "                 \n",
    "    # How relevant is this question, i.e., how useful would an answer to this question be in determining whether the procedure has been completed?\n",
    "    for sample_idx, sample in enumerate(samples):\n",
    "        lines.append(f\"Annotation {sample_idx + 1}\")\n",
    "        lines.append(\"===============================\")\n",
    "        lines.append(f\"Instruction: {sample['procedure']}\")\n",
    "        \n",
    "        lines.append(\"Previous questions and answers:\")\n",
    "        if len(sample['previous_questions_answers']) == 0:\n",
    "            lines.append(\"None\")\n",
    "        else:\n",
    "            for q_idx, (q, a) in enumerate(sample['previous_questions_answers']):\n",
    "                 lines.append(f\"Q{q_idx+1}. {q}     (Answer: {a})\")\n",
    "        lines.append(\"\")\n",
    "        lines.append(f\"Potential next question: {sample['question']}\")\n",
    "        lines.append(\"Your rating (place an 'x' next to your choice):\")\n",
    "        lines.append(\"1 (very irrelevant): \")\n",
    "        lines.append(\"2 (slightly irrelevant): \")\n",
    "        lines.append(\"3 (neutral): \")\n",
    "        lines.append(\"4 (slightly relevant): \")\n",
    "        lines.append(\"5 (very relevant): \")\n",
    "        lines.append(\"Instructions Unclear: \")\n",
    "        lines.append(\"\\n\")\n",
    "                 \n",
    "    with open(os.path.join(output_dir, f\"relevance_sample{random_seed}_annotator{annotator_idx+1}.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75182c9c-1166-4d71-b13f-28d129b0b1c1",
   "metadata": {},
   "source": [
    "Informativeness study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db5a3ae4-538d-4f24-9a37-345f0b4df68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from travel import set_random_seed\n",
    "from travel.constants import RANDOM_SEED, RESULTS_DIR\n",
    "\n",
    "N_SAMPLES = 100\n",
    "SAMPLES_PER_ANNOTATOR = 10\n",
    "\n",
    "timestamp = datetime.datetime.now()\n",
    "output_dir = os.path.join(\"annotation_files\", f\"informativeness_{timestamp.strftime('%Y%m%d%H%M%S')}\")\n",
    "os.makedirs(output_dir)\n",
    "\n",
    "random_seed = 888\n",
    "set_random_seed(random_seed) # Set seed to 888 for informativeness samples\n",
    "\n",
    "example_idxs = random.sample(list(range(len(all_data))), N_SAMPLES)\n",
    "\n",
    "data_dicts = []\n",
    "for example_idx in example_idxs:\n",
    "    output = all_data[example_idx]\n",
    "    example_id = output['example_id']\n",
    "\n",
    "    n_turns = output['final_turn'] + 1\n",
    "    selected_turn = random.choice(list(range(n_turns)))\n",
    "\n",
    "    selected_question = output['questions'][selected_turn]\n",
    "    selected_answer = np.argmax(output['answer_probs'][selected_turn])\n",
    "    assert selected_answer == 0 or selected_answer == 1\n",
    "    selected_answer = \"Yes\" if selected_answer == 1 else \"No\"\n",
    "\n",
    "    previous_questions = [q for q, a in zip(output['questions'][:selected_turn], output['answers'][:selected_turn]) if a != \"Unsure\"]\n",
    "    previous_answers = [a for a in output['answers'][:selected_turn] if a != \"Unsure\"]\n",
    "\n",
    "    actual_relevance_marginal = output['relevance_marginal'][selected_turn]\n",
    "    actual_informativeness = output['informativeness'][selected_turn]\n",
    "    actual_informativeness_marginal = output['informativeness_marginal'][selected_turn]\n",
    "    max_informativeness_marginal_so_far = max([turn_metric for turn_idx, turn_metric in enumerate(output['informativeness_marginal'][:selected_turn+1]) if output['answers'][turn_idx] != \"Unsure\" or turn_idx == selected_turn])\n",
    "    max_informativeness_marginal_so_far_x_mean_relevance_marginal = round(max_informativeness_marginal_so_far * np.mean(output['relevance_marginal'][:selected_turn + 1]), 6)\n",
    "    actual_informativeness_marginal_x_relevance_marginal = output['informativeness_marginal_x_relevance_marginal'][selected_turn]\n",
    "    actual_informativeness_marginal_x_relevance_marginal_ref = output['informativeness_marginal_x_relevance_marginal_ref'][selected_turn]\n",
    "    \n",
    "    data_dicts.append(\n",
    "        {\n",
    "            \"example_id\": example_id,\n",
    "            \"procedure\": output['procedure'],\n",
    "            \"question\": selected_question,\n",
    "            \"answer\": selected_answer,\n",
    "            \"previous_questions_answers\": [(q, a) for q, a in zip(previous_questions, previous_answers)],\n",
    "            \"nli_metric_scores\": {\n",
    "                \"relevance_marginal\": actual_relevance_marginal,\n",
    "                \"informativeness\": actual_informativeness,\n",
    "                \"informativeness_x_relevance_marginal\": actual_informativeness * actual_relevance_marginal, # TODO: later can just get this directly from metrics\n",
    "                \"informativeness_marginal\": actual_informativeness_marginal,\n",
    "                \"max_informativeness_marginal_so_far\": max_informativeness_marginal_so_far,\n",
    "                \"max_informativeness_marginal_so_far_x_mean_relevance_marginal\": max_informativeness_marginal_so_far_x_mean_relevance_marginal,\n",
    "                \"informativeness_marginal_x_relevance_marginal\": actual_informativeness_marginal_x_relevance_marginal,\n",
    "                \"informativeness_marginal_x_relevance_marginal_ref\": actual_informativeness_marginal_x_relevance_marginal_ref,\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "json.dump(data_dicts, open(os.path.join(output_dir, f\"informativeness_sample{random_seed}.json\"), \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b09b0859-6f8e-43be-92b8-95f8e62cca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide sampled data into a txt file for each annotator\n",
    "for annotator_idx in range(N_SAMPLES // SAMPLES_PER_ANNOTATOR):\n",
    "    lines = []\n",
    "\n",
    "    samples = data_dicts[annotator_idx * SAMPLES_PER_ANNOTATOR:(annotator_idx + 1) * SAMPLES_PER_ANNOTATOR]\n",
    "    assert len(samples) == SAMPLES_PER_ANNOTATOR\n",
    "    \n",
    "    lines.append(\"Imagine you just had eye surgery, and are unable to see. You're performing a task you're familiar with, but need help to determine whether you successfully completed it.\")\n",
    "    lines.append(\"You video call a friend (who is unfamiliar with the task) and show them what you're working on. You then ask them some yes/no questions to figure out whether you successfully completed the task.\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"For each annotation task, you will be given the following information:\")\n",
    "    lines.append(\" - A sentence describing the procedure you're trying to perform.\")\n",
    "    lines.append(\" - An optional list of previous questions you already asked, and their answers.\")\n",
    "    lines.append(\" - The last question you just asked your friend, and its answer.\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"You must rate how informative the last question and its answer are. By informative, we mean: compared to what you knew from the previous questions and answers, how much more sure would the last question and answer be about whether you succeeded?\")\n",
    "    lines.append(\"You can also choose to mark 'Instructions Unclear', which means that the instructional text itself is not clear, so you're not sure how to determine whether it's successful. This should only be used in rare cases.\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"Some tips:\")\n",
    "    lines.append(\" - Only judge the informativeness of the last question and answer, not the previous questions and answers (which may or may not be informative).\")\n",
    "    lines.append(\" - A question may seem relevant to the task at hand, but you should consider it uninformative if it doesn't provide essential information to judge the success of the task.\")\n",
    "    lines.append(\" - If a seemingly informative question is redundant with previous questions, you may consider it less informative.\")\n",
    "    lines.append(\" - If the last answer contradicts critical information you had from previous questions and answers, you may consider it more informative.\")\n",
    "    lines.append(' - The instructional text and questions may refer to \"someone\" or a \"person\"; always assume this is referring to yourself (the person performing the task).')\n",
    "    lines.append(' - The questions may refer to a \"photo\" or \"image\"; always assume this is referring to the video feed your friend would see through the video call.')\n",
    "    lines.append(\"\\n\")\n",
    "                 \n",
    "    # How relevant is this question, i.e., how useful would an answer to this question be in determining whether the procedure has been completed?\n",
    "    for sample_idx, sample in enumerate(samples):\n",
    "        lines.append(f\"Annotation {sample_idx + 1}\")\n",
    "        lines.append(\"===============================\")\n",
    "        lines.append(f\"Instruction: {sample['procedure']}\")\n",
    "        \n",
    "        lines.append(\"Previous questions and answers:\")\n",
    "        if len(sample['previous_questions_answers']) == 0:\n",
    "            lines.append(\"None\")\n",
    "        else:\n",
    "            for q_idx, (q, a) in enumerate(sample['previous_questions_answers']):\n",
    "                 lines.append(f\"Q{q_idx+1}. {q}     (Answer: {a})\")\n",
    "        lines.append(\"\")\n",
    "        lines.append(f\"Last question: {sample['question']}\")\n",
    "        lines.append(f\"Last answer: {sample['answer']}\")\n",
    "        lines.append(\"Your rating (place an 'x' next to your choice):\")\n",
    "        lines.append(\"1 (very informative): \")\n",
    "        lines.append(\"2 (slightly informative): \")\n",
    "        lines.append(\"3 (neutral): \")\n",
    "        lines.append(\"4 (slightly informative): \")\n",
    "        lines.append(\"5 (very informative): \")\n",
    "        lines.append(\"Instructions Unclear: \")\n",
    "        lines.append(\"\\n\")\n",
    "                 \n",
    "    with open(os.path.join(output_dir, f\"informativeness_sample{random_seed}_annotator{annotator_idx+1}.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff9e42e-04d8-4a42-a001-0d3d8ebf14c5",
   "metadata": {},
   "source": [
    "# Checking correlations of annotations with metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce8bc340-8690-4116-b1ab-94547a780dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# INPUT_FILE = \"annotation_files/relevance_20240816145229/relevance_sample555.json\"\n",
    "# ANNOTATIONS_DIR = \"../informativeness_relevance_annotation/relevance\"\n",
    "# metric_key = 'relevance_marginal'\n",
    "\n",
    "INPUT_FILE = \"annotation_files/informativeness_20240817125001/informativeness_sample888.json\"\n",
    "ANNOTATIONS_DIR = \"../informativeness_relevance_annotation/informativeness\"\n",
    "metric_key = \"informativeness_marginal_x_relevance_marginal\"\n",
    "# metric_key = \"informativeness_marginal\"\n",
    "\n",
    "data_dicts = json.load(open(INPUT_FILE, \"r\"))\n",
    "\n",
    "all_annotations = None\n",
    "for fname in os.listdir(ANNOTATIONS_DIR):\n",
    "    d = pd.read_csv(os.path.join(ANNOTATIONS_DIR, fname))\n",
    "    \n",
    "    if all_annotations is None:\n",
    "        all_annotations = d\n",
    "    else:\n",
    "        all_annotations = pd.concat((all_annotations, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6f796c4-e895-4a3d-97fc-4ced4078681d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: did find annotations for annotator 5, annotation 0. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 5, annotation 1. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 5, annotation 2. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 5, annotation 3. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 5, annotation 4. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 5, annotation 5. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 5, annotation 6. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 5, annotation 7. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 5, annotation 8. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 5, annotation 9. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 6, annotation 0. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 6, annotation 1. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 6, annotation 2. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 6, annotation 3. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 6, annotation 4. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 6, annotation 5. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 6, annotation 6. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 6, annotation 7. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 6, annotation 8. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 6, annotation 9. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 7, annotation 0. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 7, annotation 1. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 7, annotation 2. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 7, annotation 3. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 7, annotation 4. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 7, annotation 5. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 7, annotation 6. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 7, annotation 7. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 7, annotation 8. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 7, annotation 9. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 8, annotation 0. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 8, annotation 1. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 8, annotation 2. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 8, annotation 3. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 8, annotation 4. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 8, annotation 5. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 8, annotation 6. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 8, annotation 7. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 8, annotation 8. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 8, annotation 9. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 9, annotation 0. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 9, annotation 1. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 9, annotation 2. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 9, annotation 3. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 9, annotation 4. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 9, annotation 5. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 9, annotation 6. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 9, annotation 7. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 9, annotation 8. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Warning: did find annotations for annotator 9, annotation 9. This may be expected if we did not have annotators do all the annotations in the source file.\n",
      "Recovered 50 annotations.\n"
     ]
    }
   ],
   "source": [
    "N_SAMPLES = 100\n",
    "SAMPLES_PER_ANNOTATOR = 10\n",
    "\n",
    "calculated_values = []\n",
    "annotated_values = []\n",
    "for annotator_idx in range(N_SAMPLES // SAMPLES_PER_ANNOTATOR):\n",
    "    samples = data_dicts[annotator_idx * SAMPLES_PER_ANNOTATOR:(annotator_idx + 1) * SAMPLES_PER_ANNOTATOR]\n",
    "    assert len(samples) == SAMPLES_PER_ANNOTATOR\n",
    "    \n",
    "    for annotation_idx in range(len(samples)):\n",
    "    \n",
    "        retrieved_annotations = all_annotations[all_annotations['annotator_index'] == annotator_idx]\n",
    "        retrieved_annotations = retrieved_annotations[retrieved_annotations['annotation_index'] == annotation_idx]\n",
    "        if len(retrieved_annotations) == 0:\n",
    "            print(f\"Warning: did find annotations for annotator {annotator_idx}, annotation {annotation_idx}. This may be expected if we did not have annotators do all the annotations in the source file.\")\n",
    "            continue\n",
    "            \n",
    "        assert len(retrieved_annotations) == 1, \"Do we have some duplicate annotations?\"\n",
    "        \n",
    "        rating = retrieved_annotations['rating'].iloc[0]\n",
    "        if \"Instructions Unclear\" in rating:\n",
    "            print(\"Warning: threw away an instance due to instructions being labeled unclear\")\n",
    "            continue\n",
    "            \n",
    "        rating = int(rating.split()[0])\n",
    "        actual = samples[annotation_idx]['nli_metric_scores'][metric_key]\n",
    "        \n",
    "        calculated_values.append(actual)\n",
    "        annotated_values.append(rating)\n",
    "        \n",
    "print(f\"Recovered {len(annotated_values)} annotations.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa83002e-9171-4ac1-a44b-8174b285fa22",
   "metadata": {},
   "source": [
    "Informativeness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a01e3348-ab49-4dce-9d6e-97510ab4fd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SignificanceResult(statistic=0.499254177891243, pvalue=0.00022356275399887732)\n",
      "PearsonRResult(statistic=0.5119454478480675, pvalue=0.00014459157099064063)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "res = spearmanr(calculated_values, annotated_values)\n",
    "pprint(res)\n",
    "\n",
    "res = pearsonr(calculated_values, annotated_values)\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad4e3c-1e01-43b5-aac2-6ee156c6c095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel",
   "language": "python",
   "name": "travel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
