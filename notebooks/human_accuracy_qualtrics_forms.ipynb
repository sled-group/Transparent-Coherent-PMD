{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c84d0fe7",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cc2187b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sstorks/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir(\"/nfs/turbo/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl\")\n",
    "from travel import init_travel\n",
    "init_travel()\n",
    "\n",
    "import argparse\n",
    "from collections import defaultdict, Counter\n",
    "from copy import deepcopy\n",
    "from itertools import product\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "import shutil\n",
    "import spacy\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForVision2Seq, AutoModelForCausalLM, AutoProcessor, BitsAndBytesConfig, AutoModelForSequenceClassification, AutoTokenizer, PhrasalConstraint           \n",
    "\n",
    "from travel.constants import RESULTS_DIR, IMAGES_CHUNK_SIZE, HF_TOKEN, CONFIG_PATH\n",
    "from travel.data.captaincook4d import CaptainCook4DDataset\n",
    "from travel.data.ego4d import Ego4DMistakeDetectionDataset\n",
    "from travel.data.mistake_detection import MistakeDetectionTasks\n",
    "from travel.data.vqa import VQAResponse, get_vqa_response_token_ids, VQAOutputs, DIALOG_START_TOKENS, IMAGE_TOKENS, USER_START_TOKENS, USER_END_TOKENS, ASSISTANT_START_TOKENS, ASSISTANT_END_TOKENS, IVQA_PREAMBLE, IVQA_SUCCESS_QUESTION\n",
    "from travel.data.vqg import generate_vqg_prompt_icl\n",
    "from travel.model import simple_lm_prompt_beam_search, simple_vlm_prompt_beam_search, compute_completion_log_likelihoods, compute_completion_log_likelihoods_encoder_decoder, compute_completion_log_likelihoods_vlm\n",
    "from travel.model.metrics import question_coherence_metrics_nli, question_coherence_metrics_vlm, generate_det_curve, compile_accuracy_and_coherence_metrics, generate_3d_overview_graph\n",
    "from travel.model.mistake_detection import MISTAKE_DETECTION_THRESHOLDS\n",
    "from travel.model.nli import NLI_MODEL_PATH, NLI_BATCH_SIZE\n",
    "from travel.model.vqa import run_vqa_with_visual_filter\n",
    "from travel.model.vqg import cleanup_generated_question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83128df",
   "metadata": {},
   "source": [
    "Qualtrics API helpers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ad0304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "import uuid\n",
    "from typing import List, Optional\n",
    "\n",
    "class QualtricsAnnotationSurvey:\n",
    "    def __init__(self, api_token: str, data_center: str):\n",
    "        \"\"\"\n",
    "        Initialize Qualtrics API client\n",
    "        \n",
    "        Args:\n",
    "            api_token: Your Qualtrics API token\n",
    "            data_center: Your data center (e.g., 'ca1', 'fra1', 'sydney1')\n",
    "        \"\"\"\n",
    "        self.api_token = api_token\n",
    "        self.base_url = f\"https://{data_center}.qualtrics.com/API/v3\"\n",
    "        self.headers = {\n",
    "            \"X-API-TOKEN\": api_token,\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "    \n",
    "    def upload_image_to_library(self, pil_image: Image.Image, filename: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Upload PIL Image to Qualtrics Graphics Library\n",
    "        \n",
    "        Args:\n",
    "            pil_image: PIL Image object\n",
    "            filename: Name for the uploaded file\n",
    "            \n",
    "        Returns:\n",
    "            String URL of uploaded image or None if failed\n",
    "        \"\"\"\n",
    "        # Convert PIL image to bytes\n",
    "        img_bytes = io.BytesIO()\n",
    "        pil_image.save(img_bytes, format='PNG')\n",
    "        img_bytes.seek(0)\n",
    "        \n",
    "        # Upload to graphics library\n",
    "        files = {\n",
    "            'file': (filename, img_bytes, 'image/png')\n",
    "        }\n",
    "        \n",
    "        headers_upload = {\n",
    "            \"X-API-TOKEN\": self.api_token\n",
    "        }\n",
    "        \n",
    "        url = f\"{self.base_url}/libraries/GR_MAIN/graphics\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, headers=headers_upload, files=files)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            return result['result']['url']  # Return the URL of uploaded image\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to upload image {filename}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_survey(self, survey_name: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Create a new survey\n",
    "        \n",
    "        Args:\n",
    "            survey_name: Name for the survey\n",
    "            \n",
    "        Returns:\n",
    "            Survey ID or None if failed\n",
    "        \"\"\"\n",
    "        data = {\n",
    "            \"SurveyName\": survey_name,\n",
    "            \"Language\": \"EN\",\n",
    "            \"ProjectCategory\": \"CORE\"\n",
    "        }\n",
    "        \n",
    "        url = f\"{self.base_url}/survey-definitions\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, headers=self.headers, json=data)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            return result['result']['SurveyID']\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to create survey: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_mistake_detection_question(self, survey_id: str, image_url: str, \n",
    "                                        task_description: str, question_id: str) -> bool:\n",
    "        \"\"\"\n",
    "        Create a mistake detection question with image and binary choice\n",
    "        \n",
    "        Args:\n",
    "            survey_id: ID of the survey\n",
    "            image_url: URL of the uploaded image\n",
    "            task_description: Text describing the task\n",
    "            question_id: Unique identifier for this question\n",
    "            \n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        # HTML content with image and task description\n",
    "        question_html = f\"\"\"\n",
    "        <div style=\"margin-bottom: 20px;\">\n",
    "            You will be shown pairs of procedural texts and egocentric (POV) photos from people's perspective after trying to perform the procedure. Based on the photo, you will be asked to determine whether the person has made a mistake in completing the given procedure, or successfully completed it. In making your decision, consider the following guidelines:\n",
    "            <ul>\n",
    "                <li>If you see the object that should be affected by the procedure and it appears to have undergone the action(s) in the procedure (e.g., moving to a specific location, slicing, opening), the image should be labeled a success. If the object's state contradicts the success state for the procedure, the image should be labeled a mistake.</li>                \n",
    "                <li>If you don't see the object or only see part of it, it could either be a success or mistake. Make your best guess based on what you do see in the image.</li>\n",
    "                <li>If you're ever not sure (e.g., the image doesn't have enough information or it's blurry/low quality), also make your best guess based on any context clues you see.</li>\n",
    "            </ul>\n",
    "\n",
    "            \n",
    "            <h3>Target procedure:</h3>\n",
    "            <p>{task_description}</p>\n",
    "        </div>\n",
    "        <div style=\"text-align: center; margin: 20px 0;\">\n",
    "            <img src=\"{image_url}\" style=\"max-width: 100%; height: auto;\" alt=\"Task completion image\"/>\n",
    "        </div>\n",
    "        <p><strong>Question: Did the person make a mistake in completing the described task?</strong></p>\n",
    "        \"\"\"\n",
    "        \n",
    "        question_data = {\n",
    "            \"QuestionText\": question_html,\n",
    "            \"DataExportTag\": f\"mistake_detection_{question_id}\",\n",
    "            \"QuestionType\": \"MC\",\n",
    "            \"Selector\": \"SAVR\",  # Single Answer Vertical\n",
    "            \"Configuration\": {\n",
    "                \"QuestionDescriptionOption\": \"UseText\"\n",
    "            },\n",
    "            \"QuestionDescription\": f\"Mistake Detection - {question_id}\",\n",
    "            \"Choices\": {\n",
    "                \"1\": {\n",
    "                    \"Display\": \"Yes - There is a mistake\"\n",
    "                },\n",
    "                \"2\": {\n",
    "                    \"Display\": \"No - Task completed successfully\"\n",
    "                }\n",
    "            },\n",
    "            \"ChoiceOrder\": [\"1\", \"2\"],\n",
    "            \"Validation\": {\n",
    "                \"Settings\": {\n",
    "                    \"ForceResponse\": \"ON\",\n",
    "                    \"Type\": \"None\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        url = f\"{self.base_url}/survey-definitions/{survey_id}/questions\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, headers=self.headers, json=question_data)\n",
    "            response.raise_for_status()\n",
    "            return True\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to create question {question_id}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def create_confidence_question(self, survey_id: str, question_id: str) -> bool:\n",
    "        \"\"\"\n",
    "        Create a confidence rating question (1-5 scale)\n",
    "        \n",
    "        Args:\n",
    "            survey_id: ID of the survey\n",
    "            question_id: Unique identifier for this question\n",
    "            \n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        question_data = {\n",
    "            \"QuestionText\": \"How confident are you in your answer?\",\n",
    "            \"DataExportTag\": f\"confidence_{question_id}\",\n",
    "            \"QuestionType\": \"MC\",\n",
    "            \"Selector\": \"SAVR\",\n",
    "            \"Configuration\": {\n",
    "                \"QuestionDescriptionOption\": \"UseText\"\n",
    "            },\n",
    "            \"QuestionDescription\": f\"Confidence Rating - {question_id}\",\n",
    "            \"Choices\": {\n",
    "                \"1\": {\"Display\": \"1 - Not confident at all\"},\n",
    "                \"2\": {\"Display\": \"2 - Slightly confident\"},\n",
    "                \"3\": {\"Display\": \"3 - Moderately confident\"},\n",
    "                \"4\": {\"Display\": \"4 - Very confident\"},\n",
    "                \"5\": {\"Display\": \"5 - Extremely confident\"}\n",
    "            },\n",
    "            \"ChoiceOrder\": [\"1\", \"2\", \"3\", \"4\", \"5\"],\n",
    "            \"Validation\": {\n",
    "                \"Settings\": {\n",
    "                    \"ForceResponse\": \"ON\",\n",
    "                    \"Type\": \"None\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        url = f\"{self.base_url}/survey-definitions/{survey_id}/questions\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, headers=self.headers, json=question_data)\n",
    "            response.raise_for_status()\n",
    "            return True\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to create confidence question {question_id}: {e}\")\n",
    "            return False\n",
    "\n",
    "def generate_annotation_survey(api_token: str, data_center: str, \n",
    "                             images: List[Image.Image], \n",
    "                             task_descriptions: List[str],\n",
    "                             survey_name: str = \"Mistake Detection Annotation\") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Generate a complete annotation survey with images and task descriptions\n",
    "    \n",
    "    Args:\n",
    "        api_token: Qualtrics API token\n",
    "        data_center: Qualtrics data center\n",
    "        images: List of PIL Image objects\n",
    "        task_descriptions: List of task description strings (same length as images)\n",
    "        survey_name: Name for the created survey\n",
    "        \n",
    "    Returns:\n",
    "        Survey ID if successful, None otherwise\n",
    "    \"\"\"\n",
    "    if len(images) != len(task_descriptions):\n",
    "        raise ValueError(\"Number of images must match number of task descriptions\")\n",
    "    \n",
    "    # Initialize API client\n",
    "    client = QualtricsAnnotationSurvey(api_token, data_center)\n",
    "    \n",
    "    # Create survey\n",
    "    survey_id = client.create_survey(survey_name)\n",
    "    if not survey_id:\n",
    "        print(\"Failed to create survey\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Created survey with ID: {survey_id}\")\n",
    "    \n",
    "    # Process each image and description pair\n",
    "    for i, (image, description) in enumerate(zip(images, task_descriptions)):\n",
    "        question_id = f\"q{i+1:03d}\"\n",
    "        \n",
    "        # Upload image\n",
    "        image_filename = f\"annotation_image_{question_id}.png\"\n",
    "        image_url = client.upload_image_to_library(image, image_filename)\n",
    "        \n",
    "        if not image_url:\n",
    "            print(f\"Failed to upload image for question {question_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Create mistake detection question\n",
    "        if client.create_mistake_detection_question(survey_id, image_url, description, question_id):\n",
    "            print(f\"Created mistake detection question {question_id}\")\n",
    "        else:\n",
    "            print(f\"Failed to create mistake detection question {question_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Create confidence question\n",
    "        if client.create_confidence_question(survey_id, question_id):\n",
    "            print(f\"Created confidence question for {question_id}\")\n",
    "        else:\n",
    "            print(f\"Failed to create confidence question for {question_id}\")\n",
    "    \n",
    "    print(f\"Survey generation complete. Survey ID: {survey_id}\")\n",
    "    return survey_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597c6ee5",
   "metadata": {},
   "source": [
    "# Load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9574a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading evaluation dataset (try 0)...\n"
     ]
    }
   ],
   "source": [
    "# Load approopriate evaluation dataset\n",
    "dataset = None\n",
    "for retry in range(5):\n",
    "    print(f\"Loading evaluation dataset (try {retry})...\")\n",
    "    try:\n",
    "        dataset = Ego4DMistakeDetectionDataset(data_split=\"val\", \n",
    "                                                mismatch_augmentation=True,\n",
    "                                                multi_frame=False,\n",
    "                                                debug_n_examples_per_class=250)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(\"Encountered error during data loading:\")\n",
    "        pprint(e)\n",
    "        time.sleep(60)\n",
    "if dataset is None:\n",
    "    raise ValueError(\"Could not load dataset after retrying!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a846de75",
   "metadata": {},
   "source": [
    "# Sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6111eea2",
   "metadata": {},
   "source": [
    "First reorganize data by mistake type (and success) to estimate distribution of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fa44571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Action Incomplete': 0.102,\n",
      " 'MisalignSRL_ARG1': 0.174,\n",
      " 'MisalignSRL_V': 0.062,\n",
      " 'MisalignSRL_V_ARG1': 0.162,\n",
      " 'None': 0.5}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "examples_by_mistake_type = defaultdict(list)\n",
    "\n",
    "for example in dataset:\n",
    "    examples_by_mistake_type[str(example.mistake_type)].append(example)\n",
    "\n",
    "mistake_type_dist = {}\n",
    "for mistake_type in examples_by_mistake_type:\n",
    "    mistake_type_dist[mistake_type] = len(examples_by_mistake_type[mistake_type]) / len(dataset)\n",
    "\n",
    "pprint(mistake_type_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5febf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "TOTAL_EXAMPLES_TO_ANNOTATE = 100\n",
    "N_EXAMPLES_PER_ANNOTATOR = 25\n",
    "examples_to_annotate = {\n",
    "    \"Action Incomplete\": 10,\n",
    "    \"MisalignSRL_ARG1\": 18,\n",
    "    \"MisalignSRL_V\": 6,\n",
    "    \"MisalignSRL_V_ARG1\": 16,\n",
    "    \"None\": 50,\n",
    "}\n",
    "assert TOTAL_EXAMPLES_TO_ANNOTATE == sum(list(examples_to_annotate.values())), \"Configured distribution of annotated example types doesn't add up to total number of expected examples to annotate.\"\n",
    "\n",
    "selected_examples = []\n",
    "for k in examples_to_annotate:\n",
    "    selected_examples += random.sample(examples_by_mistake_type[k], examples_to_annotate[k])\n",
    "\n",
    "assert len(selected_examples) == TOTAL_EXAMPLES_TO_ANNOTATE, \"Didn't sample expected number of examples to annotate.\"\n",
    "\n",
    "random.shuffle(selected_examples)\n",
    "assert TOTAL_EXAMPLES_TO_ANNOTATE % N_EXAMPLES_PER_ANNOTATOR == 0, \"Number of examples per annotator must evenly divide total number of annotated examples.\"\n",
    "sample_chunks = []\n",
    "# Split into even chunks\n",
    "n_chunks = TOTAL_EXAMPLES_TO_ANNOTATE // N_EXAMPLES_PER_ANNOTATOR\n",
    "for chunk in range(n_chunks):\n",
    "    examples = selected_examples[N_EXAMPLES_PER_ANNOTATOR*chunk:N_EXAMPLES_PER_ANNOTATOR*(chunk+1)]\n",
    "    assert all(len(e.frames) == 0 for e in examples)\n",
    "\n",
    "    descriptions = [e.procedure_description for e in examples]\n",
    "    images = [e.frames[0] for e in examples]\n",
    "    \n",
    "    assert len(descriptions) == len(images) == N_EXAMPLES_PER_ANNOTATOR, f\"Didn't get correct number of descriptions or images for chunk {chunk}.\"\n",
    "\n",
    "    sample_chunks.append((images, descriptions))\n",
    "\n",
    "assert len(sample_chunks) == n_chunks, \"Didn't get the correct number of chunks.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f473bef",
   "metadata": {},
   "source": [
    "# Generate Qualtrics forms for annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fca82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from travel.constants import QUALTRICS_API_TOKEN, QUALTRICS_DATA_CENTER\n",
    "\n",
    "for (images, descriptions) in sample_chunks:\n",
    "    # Generate survey\n",
    "    survey_id = generate_annotation_survey(\n",
    "        api_token=QUALTRICS_API_TOKEN,\n",
    "        data_center=QUALTRICS_DATA_CENTER,\n",
    "        images=images,\n",
    "        task_descriptions=descriptions,\n",
    "        survey_name=\"Procedural Mistake Detection Human Annotation\"\n",
    "    ) # NOTE: make sure questions are shuffled\n",
    "\n",
    "    if survey_id:\n",
    "        print(f\"\\n✅ Survey created successfully!\")\n",
    "        print(f\"Survey ID: {survey_id}\")\n",
    "        print(f\"You can access it at: https://{QUALTRICS_DATA_CENTER}.qualtrics.com/jfe/form/{survey_id}\")\n",
    "    else:\n",
    "        print(\"❌ Survey creation failed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel",
   "language": "python",
   "name": "travel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
