{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c84d0fe7",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cc2187b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sstorks/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir(\"/nfs/turbo/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl\")\n",
    "from travel import init_travel\n",
    "init_travel()\n",
    "\n",
    "import argparse\n",
    "from collections import defaultdict, Counter\n",
    "from copy import deepcopy\n",
    "from itertools import product\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "import shutil\n",
    "import spacy\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForVision2Seq, AutoModelForCausalLM, AutoProcessor, BitsAndBytesConfig, AutoModelForSequenceClassification, AutoTokenizer, PhrasalConstraint           \n",
    "\n",
    "from travel.constants import RESULTS_DIR, IMAGES_CHUNK_SIZE, HF_TOKEN, CONFIG_PATH\n",
    "from travel.data.captaincook4d import CaptainCook4DDataset\n",
    "from travel.data.ego4d import Ego4DMistakeDetectionDataset\n",
    "from travel.data.mistake_detection import MistakeDetectionTasks\n",
    "from travel.data.vqa import VQAResponse, get_vqa_response_token_ids, VQAOutputs, DIALOG_START_TOKENS, IMAGE_TOKENS, USER_START_TOKENS, USER_END_TOKENS, ASSISTANT_START_TOKENS, ASSISTANT_END_TOKENS, IVQA_PREAMBLE, IVQA_SUCCESS_QUESTION\n",
    "from travel.data.vqg import generate_vqg_prompt_icl\n",
    "from travel.model import simple_lm_prompt_beam_search, simple_vlm_prompt_beam_search, compute_completion_log_likelihoods, compute_completion_log_likelihoods_encoder_decoder, compute_completion_log_likelihoods_vlm\n",
    "from travel.model.metrics import question_coherence_metrics_nli, question_coherence_metrics_vlm, generate_det_curve, compile_accuracy_and_coherence_metrics, generate_3d_overview_graph\n",
    "from travel.model.mistake_detection import MISTAKE_DETECTION_THRESHOLDS\n",
    "from travel.model.nli import NLI_MODEL_PATH, NLI_BATCH_SIZE\n",
    "from travel.model.vqa import run_vqa_with_visual_filter\n",
    "from travel.model.vqg import cleanup_generated_question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83128df",
   "metadata": {},
   "source": [
    "Qualtrics API helpers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba143fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "import uuid\n",
    "from typing import List, Optional\n",
    "from pprint import pprint\n",
    "\n",
    "class QualtricsAnnotationSurvey:\n",
    "    def __init__(self, api_token: str, data_center: str, library_name: str):\n",
    "        \"\"\"\n",
    "        Initialize Qualtrics API client\n",
    "        \n",
    "        Args:\n",
    "            api_token: Your Qualtrics API token\n",
    "            data_center: Your data center (e.g., 'ca1', 'fra1', 'sydney1')\n",
    "        \"\"\"\n",
    "        self.api_token = api_token\n",
    "        self.data_center = data_center\n",
    "        self.library_name = library_name\n",
    "        self.base_url = f\"https://{data_center}.qualtrics.com/API/v3\"\n",
    "        self.headers = {\n",
    "            \"X-API-TOKEN\": api_token,\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "    \n",
    "    def get_library_id(self) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Get the main graphics library ID for the account\n",
    "        \n",
    "        Returns:\n",
    "            Library ID or None if failed\n",
    "        \"\"\"\n",
    "        url = f\"{self.base_url}/libraries\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            libraries = result.get('result', {}).get('elements', [])\n",
    "            \n",
    "            # Find the graphics library\n",
    "            for library in libraries:\n",
    "                pprint(library)\n",
    "                if library.get('libraryName') == self.library_name:\n",
    "                    return library.get('libraryId')\n",
    "            \n",
    "            # If no graphics library found, return None\n",
    "            print(\"No graphics library found\")\n",
    "            return None\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to get libraries: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def upload_image_to_library(self, pil_image: Image.Image, filename: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Upload PIL Image to Qualtrics Graphics Library\n",
    "        \n",
    "        Args:\n",
    "            pil_image: PIL Image object\n",
    "            filename: Name for the uploaded file\n",
    "            \n",
    "        Returns:\n",
    "            String URL of uploaded image or None if failed\n",
    "        \"\"\"\n",
    "        # Get the library ID first\n",
    "        library_id = self.get_library_id()\n",
    "        if not library_id:\n",
    "            print(\"Could not find graphics library\")\n",
    "            return None\n",
    "        \n",
    "        # Convert PIL image to bytes\n",
    "        img_bytes = io.BytesIO()\n",
    "        pil_image.save(img_bytes, format='PNG')\n",
    "        img_bytes.seek(0)\n",
    "        \n",
    "        # Prepare multipart form data\n",
    "        files = {\n",
    "            'file': (filename, img_bytes.getvalue(), 'image/png')\n",
    "        }\n",
    "        \n",
    "        headers_upload = {\n",
    "            \"X-API-TOKEN\": self.api_token\n",
    "        }\n",
    "        \n",
    "        url = f\"{self.base_url}/libraries/{library_id}/graphics\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, headers=headers_upload, files=files)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            # The response should contain the graphic ID and URL\n",
    "            graphic_id = result.get('result', {}).get('id')\n",
    "            if graphic_id:\n",
    "                # Construct the URL for the uploaded image\n",
    "                return f\"https://{self.data_center}.qualtrics.com/WRQualtricsShared/Graphics/{graphic_id}\"\n",
    "            else:\n",
    "                print(\"No graphic ID returned\")\n",
    "                return None\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to upload image {filename}: {e}\")\n",
    "            if hasattr(e, 'response') and e.response is not None:\n",
    "                print(f\"Response content: {e.response.text}\")\n",
    "            return None\n",
    "        \n",
    "    def add_survey_instructions(self, survey_id: str) -> bool:\n",
    "        \"\"\"\n",
    "        Add an instruction block as the first question in the survey\n",
    "        \n",
    "        Args:\n",
    "            survey_id: ID of the survey\n",
    "            \n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "#         instructions_html = \"\"\"<p>You will be shown pairs of procedural texts and egocentric (POV) photos from people's perspective after trying to perform the procedure. Based on the photo, you will be asked to determine whether the person has made a mistake in completing the given procedure, or successfully completed it.</p>\n",
    "# <p><strong>Guidelines:</strong></p>\n",
    "# <ul>\n",
    "# <li>If you see the object that should be affected by the procedure and it appears to have undergone the action(s) in the procedure (e.g., moving to a specific location, slicing, opening), the image should be labeled a success. If the object's state contradicts the success state for the procedure, the image should be labeled a mistake.</li>\n",
    "# <li>If you don't see the object or only see part of it, it could either be a success or mistake. Make your best guess based on what you do see in the image.</li>\n",
    "# <li>If you're ever not sure (e.g., the image doesn't have enough information or it's blurry/low quality), also make your best guess based on any context clues you see.</li>\n",
    "# </ul>\n",
    "# <p>Click <strong>Next</strong> to begin.</p>\"\"\"\n",
    "        \n",
    "        question_data = {\n",
    "            \"QuestionText\": \"Click Next to begin the annotation task.\",\n",
    "            \"QuestionType\": \"Text\", \n",
    "            \"Selector\": \"ML\",\n",
    "            \"DataExportTag\": \"instructions\"\n",
    "        }\n",
    "\n",
    "        url = f\"{self.base_url}/survey-definitions/{survey_id}/questions\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, headers=self.headers, json=question_data)\n",
    "            response.raise_for_status()\n",
    "            return True\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to create instructions: {e}\")\n",
    "            if hasattr(e, 'response') and e.response is not None:\n",
    "                print(f\"Response status: {e.response.status_code}\")\n",
    "                try:\n",
    "                    error_detail = e.response.json()\n",
    "                    print(f\"Error details: {json.dumps(error_detail, indent=2)}\")\n",
    "                except:\n",
    "                    print(f\"Response content: {e.response.text[:500]}...\")\n",
    "            return False\n",
    "        \n",
    "    def create_survey(self, survey_name: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Create a new survey\n",
    "        \n",
    "        Args:\n",
    "            survey_name: Name for the survey\n",
    "            \n",
    "        Returns:\n",
    "            Survey ID or None if failed\n",
    "        \"\"\"\n",
    "        data = {\n",
    "            \"SurveyName\": survey_name,\n",
    "            \"Language\": \"EN\",\n",
    "            \"ProjectCategory\": \"CORE\"\n",
    "        }\n",
    "        \n",
    "        url = f\"{self.base_url}/survey-definitions\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, headers=self.headers, json=data)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            return result['result']['SurveyID']\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to create survey: {e}\")\n",
    "            if hasattr(e, 'response') and e.response is not None:\n",
    "                print(f\"Response content: {e.response.text}\")\n",
    "            return None\n",
    "    \n",
    "    def create_mistake_detection_question(self, survey_id: str, image_url: str, \n",
    "                                        task_description: str, question_id: str) -> bool:\n",
    "        \"\"\"\n",
    "        Create a mistake detection question with embedded image and binary choice\n",
    "        \n",
    "        Args:\n",
    "            survey_id: ID of the survey\n",
    "            image_data_url: Base64 data URL of the image\n",
    "            task_description: Text describing the task\n",
    "            question_id: Unique identifier for this question\n",
    "            \n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        # HTML content with embedded image and task description\n",
    "        # question_html = f\"\"\"<p>You will be shown pairs of procedural texts and egocentric (POV) photos from people's perspective after trying to perform the procedure. Based on the photo, you will be asked to determine whether the person has made a mistake in completing the given procedure, or successfully completed it.</p>\n",
    "\n",
    "        # <p><strong>Guidelines:</strong></p>\n",
    "        # <ul>\n",
    "        # <li>If you see the object that should be affected by the procedure and it appears to have undergone the action(s) in the procedure (e.g., moving to a specific location, slicing, opening), the image should be labeled a success. If the object's state contradicts the success state for the procedure, the image should be labeled a mistake.</li>\n",
    "        # <li>If you don't see the object or only see part of it, it could either be a success or mistake. Make your best guess based on what you do see in the image.</li>\n",
    "        # <li>If you're ever not sure (e.g., the image doesn't have enough information or it's blurry/low quality), also make your best guess based on any context clues you see.</li>\n",
    "        # </ul>\n",
    "\n",
    "        # <p><strong>Target procedure:</strong> {task_description}</p>\n",
    "\n",
    "        # <p><img src=\"{image_data_url}\" style=\"max-width: 100%; height: auto;\" alt=\"Task completion image\"/></p>\n",
    "\n",
    "        # <p><strong>Question: Did the person make a mistake in completing the described task?</strong></p>\"\"\"\n",
    "\n",
    "        # question_html = f\"\"\"<p><strong>Target procedure:</strong> {task_description}</p>\n",
    "        # <p><img src=\"{image_data_url}\" alt=\"Task image\"/></p>\n",
    "        # <p><strong>Did the person make a mistake completing the described task?</strong></p>\"\"\"\n",
    "\n",
    "        if image_url:\n",
    "            question_html = f\"\"\"<p>Target procedure: {task_description}</p>\n",
    "            <p><img src=\"{image_url}\" alt=\"Task image\"/></p>\n",
    "            <p>Did the person make a mistake completing this task?</p>\"\"\"\n",
    "        else:\n",
    "            print(f\"Failed to upload image for question {question_id}\")\n",
    "            question_html = f\"<p>Target procedure: {task_description}</p><p>Did the person make a mistake completing this task?</p>\"\n",
    "\n",
    "        question_data = {\n",
    "            \"QuestionText\": question_html,\n",
    "            \"DataExportTag\": f\"mistake_detection_{question_id}\",\n",
    "            \"QuestionType\": \"MC\",\n",
    "            \"Selector\": \"SAVR\",  # Single Answer Vertical\n",
    "            \"Configuration\": {\n",
    "                \"QuestionDescriptionOption\": \"UseText\"\n",
    "            },\n",
    "            \"QuestionDescription\": f\"Mistake Detection - {question_id}\",\n",
    "            \"Choices\": {\n",
    "                \"1\": {\n",
    "                    \"Display\": \"Yes - There is a mistake\"\n",
    "                },\n",
    "                \"2\": {\n",
    "                    \"Display\": \"No - Task completed successfully\"\n",
    "                }\n",
    "            },\n",
    "            \"ChoiceOrder\": [\"1\", \"2\"],\n",
    "            \"Validation\": {\n",
    "                \"Settings\": {\n",
    "                    \"ForceResponse\": \"ON\",\n",
    "                    \"Type\": \"None\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        url = f\"{self.base_url}/survey-definitions/{survey_id}/questions\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, headers=self.headers, json=question_data)\n",
    "            response.raise_for_status()\n",
    "            return True\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to create question {question_id}: {e}\")\n",
    "            if hasattr(e, 'response') and e.response is not None:\n",
    "                print(f\"Response status: {e.response.status_code}\")\n",
    "                try:\n",
    "                    error_detail = e.response.json()\n",
    "                    print(f\"Error details: {json.dumps(error_detail, indent=2)}\")\n",
    "                except:\n",
    "                    print(f\"Response content: {e.response.text[:500]}...\")\n",
    "            \n",
    "            # Debug: Print question data size and structure\n",
    "            print(f\"Question HTML length: {len(question_html)} characters\")\n",
    "            print(f\"Image data URL length: {len(image_url)} characters\")\n",
    "            return False\n",
    "    \n",
    "    def create_confidence_question(self, survey_id: str, question_id: str) -> bool:\n",
    "        \"\"\"\n",
    "        Create a confidence rating question (1-5 scale)\n",
    "        \n",
    "        Args:\n",
    "            survey_id: ID of the survey\n",
    "            question_id: Unique identifier for this question\n",
    "            \n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        question_data = {\n",
    "            \"QuestionText\": \"How confident are you in your answer?\",\n",
    "            \"DataExportTag\": f\"confidence_{question_id}\",\n",
    "            \"QuestionType\": \"MC\",\n",
    "            \"Selector\": \"SAVR\",\n",
    "            \"Configuration\": {\n",
    "                \"QuestionDescriptionOption\": \"UseText\"\n",
    "            },\n",
    "            \"QuestionDescription\": f\"Confidence Rating - {question_id}\",\n",
    "            \"Choices\": {\n",
    "                \"1\": {\"Display\": \"1 - Not confident at all\"},\n",
    "                \"2\": {\"Display\": \"2 - Slightly confident\"},\n",
    "                \"3\": {\"Display\": \"3 - Moderately confident\"},\n",
    "                \"4\": {\"Display\": \"4 - Very confident\"},\n",
    "                \"5\": {\"Display\": \"5 - Extremely confident\"}\n",
    "            },\n",
    "            \"ChoiceOrder\": [\"1\", \"2\", \"3\", \"4\", \"5\"],\n",
    "            \"Validation\": {\n",
    "                \"Settings\": {\n",
    "                    \"ForceResponse\": \"ON\",\n",
    "                    \"Type\": \"None\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        url = f\"{self.base_url}/survey-definitions/{survey_id}/questions\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, headers=self.headers, json=question_data)\n",
    "            response.raise_for_status()\n",
    "            return True\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to create confidence question {question_id}: {e}\")\n",
    "            if hasattr(e, 'response') and e.response is not None:\n",
    "                print(f\"Response content: {e.response.text}\")\n",
    "            return False\n",
    "    \n",
    "    def publish_survey(self, survey_id: str, description: str = \"\") -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Publish the survey to make it active and accessible\n",
    "        \n",
    "        Args:\n",
    "            survey_id: ID of the survey to publish\n",
    "            description: Optional description for the publication\n",
    "            \n",
    "        Returns:\n",
    "            Published survey URL or None if failed\n",
    "        \"\"\"\n",
    "        # First, we need to get the current survey state\n",
    "        url = f\"{self.base_url}/survey-definitions/{survey_id}\"\n",
    "        \n",
    "        try:\n",
    "            # Get current survey definition\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Update the survey to published state\n",
    "            publish_data = {\n",
    "                \"isActive\": True\n",
    "            }\n",
    "            \n",
    "            # Use PATCH to update survey state\n",
    "            response = requests.patch(url, headers=self.headers, json=publish_data)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Generate the public survey URL\n",
    "            survey_url = f\"https://{self.data_center}.qualtrics.com/jfe/form/{survey_id}\"\n",
    "            \n",
    "            print(f\"Survey published successfully!\")\n",
    "            print(f\"Survey URL: {survey_url}\")\n",
    "            \n",
    "            return survey_url\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to publish survey: {e}\")\n",
    "            if hasattr(e, 'response') and e.response is not None:\n",
    "                print(f\"Response content: {e.response.text}\")\n",
    "            return None\n",
    "\n",
    "def generate_annotation_survey(api_token: str, data_center: str, library_name: str,\n",
    "                             images: List[Image.Image], \n",
    "                             task_descriptions: List[str],\n",
    "                             survey_name: str = \"Mistake Detection Annotation\",\n",
    "                             publish: bool = True) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Generate a complete annotation survey with images and task descriptions\n",
    "    \n",
    "    Args:\n",
    "        api_token: Qualtrics API token\n",
    "        data_center: Qualtrics data center\n",
    "        images: List of PIL Image objects\n",
    "        task_descriptions: List of task description strings (same length as images)\n",
    "        survey_name: Name for the created survey\n",
    "        publish: Whether to automatically publish the survey\n",
    "        \n",
    "    Returns:\n",
    "        Survey URL if successful and published, Survey ID if not published, None if failed\n",
    "    \"\"\"\n",
    "    if len(images) != len(task_descriptions):\n",
    "        raise ValueError(\"Number of images must match number of task descriptions\")\n",
    "    \n",
    "    # Initialize API client\n",
    "    client = QualtricsAnnotationSurvey(api_token, data_center, library_name)\n",
    "    \n",
    "    # Create survey\n",
    "    survey_id = client.create_survey(survey_name)\n",
    "    if not survey_id:\n",
    "        print(\"Failed to create survey\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Created survey with ID: {survey_id}\")\n",
    "    \n",
    "    # Add instructions as the first question\n",
    "    # if client.add_survey_instructions(survey_id):\n",
    "    #     print(\"✓ Added survey instructions\")\n",
    "    # else:\n",
    "    #     print(\"✗ Failed to add survey instructions\")\n",
    "    \n",
    "    # Process each image and description pair\n",
    "    for i, (image, description) in enumerate(zip(images, task_descriptions)):\n",
    "        question_id = f\"q{i+1:03d}\"\n",
    "        \n",
    "        print(f\"Processing question {question_id}...\")\n",
    "        \n",
    "        image_url = client.upload_image_to_library(image, f\"image_{question_id}.png\")\n",
    "\n",
    "        # Create mistake detection question\n",
    "        if client.create_mistake_detection_question(survey_id, image_url, description, question_id):\n",
    "            print(f\"✓ Created mistake detection question {question_id}\")\n",
    "        else:\n",
    "            print(f\"✗ Failed to create mistake detection question {question_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Create confidence question\n",
    "        if client.create_confidence_question(survey_id, question_id):\n",
    "            print(f\"✓ Created confidence question for {question_id}\")\n",
    "        else:\n",
    "            print(f\"✗ Failed to create confidence question for {question_id}\")\n",
    "    \n",
    "    # Publish survey if requested\n",
    "    if publish:\n",
    "        survey_url = client.publish_survey(survey_id, f\"Automated annotation survey: {survey_name}\")\n",
    "        if survey_url:\n",
    "            print(f\"\\n🚀 Survey published and ready for use!\")\n",
    "            return survey_url\n",
    "        else:\n",
    "            print(f\"\\n⚠️ Survey created but publishing failed. Survey ID: {survey_id}\")\n",
    "            return survey_id\n",
    "    else:\n",
    "        print(f\"\\n📋 Survey created successfully. Survey ID: {survey_id}\")\n",
    "        print(f\"Manual URL: https://{data_center}.qualtrics.com/jfe/form/{survey_id}\")\n",
    "        return survey_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597c6ee5",
   "metadata": {},
   "source": [
    "# Load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9574a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading evaluation dataset (try 0)...\n"
     ]
    }
   ],
   "source": [
    "# Load approopriate evaluation dataset\n",
    "dataset = None\n",
    "for retry in range(5):\n",
    "    print(f\"Loading evaluation dataset (try {retry})...\")\n",
    "    try:\n",
    "        dataset = Ego4DMistakeDetectionDataset(data_split=\"val\", \n",
    "                                                mismatch_augmentation=True,\n",
    "                                                multi_frame=False,\n",
    "                                                debug_n_examples_per_class=250)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(\"Encountered error during data loading:\")\n",
    "        pprint(e)\n",
    "        time.sleep(60)\n",
    "if dataset is None:\n",
    "    raise ValueError(\"Could not load dataset after retrying!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a846de75",
   "metadata": {},
   "source": [
    "# Sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6111eea2",
   "metadata": {},
   "source": [
    "First reorganize data by mistake type (and success) to estimate distribution of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fa44571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Action Incomplete': 0.102,\n",
      " 'MisalignSRL_ARG1': 0.174,\n",
      " 'MisalignSRL_V': 0.062,\n",
      " 'MisalignSRL_V_ARG1': 0.162,\n",
      " 'None': 0.5}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "examples_by_mistake_type = defaultdict(list)\n",
    "\n",
    "for example in dataset:\n",
    "    examples_by_mistake_type[str(example.mistake_type)].append(example)\n",
    "\n",
    "mistake_type_dist = {}\n",
    "for mistake_type in examples_by_mistake_type:\n",
    "    mistake_type_dist[mistake_type] = len(examples_by_mistake_type[mistake_type]) / len(dataset)\n",
    "\n",
    "pprint(mistake_type_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de5febf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "TOTAL_EXAMPLES_TO_ANNOTATE = 100\n",
    "N_EXAMPLES_PER_ANNOTATOR = 25\n",
    "examples_to_annotate = {\n",
    "    \"Action Incomplete\": 10,\n",
    "    \"MisalignSRL_ARG1\": 18,\n",
    "    \"MisalignSRL_V\": 6,\n",
    "    \"MisalignSRL_V_ARG1\": 16,\n",
    "    \"None\": 50,\n",
    "}\n",
    "assert TOTAL_EXAMPLES_TO_ANNOTATE == sum(list(examples_to_annotate.values())), \"Configured distribution of annotated example types doesn't add up to total number of expected examples to annotate.\"\n",
    "\n",
    "selected_examples = []\n",
    "for k in examples_to_annotate:\n",
    "    selected_examples += random.sample(examples_by_mistake_type[k], examples_to_annotate[k])\n",
    "\n",
    "assert len(selected_examples) == TOTAL_EXAMPLES_TO_ANNOTATE, \"Didn't sample expected number of examples to annotate.\"\n",
    "\n",
    "random.shuffle(selected_examples)\n",
    "assert TOTAL_EXAMPLES_TO_ANNOTATE % N_EXAMPLES_PER_ANNOTATOR == 0, \"Number of examples per annotator must evenly divide total number of annotated examples.\"\n",
    "sample_chunks = []\n",
    "# Split into even chunks\n",
    "n_chunks = TOTAL_EXAMPLES_TO_ANNOTATE // N_EXAMPLES_PER_ANNOTATOR\n",
    "for chunk in range(n_chunks):\n",
    "    examples = selected_examples[N_EXAMPLES_PER_ANNOTATOR*chunk:N_EXAMPLES_PER_ANNOTATOR*(chunk+1)]\n",
    "    assert all(len(e.frames) == 1 for e in examples)\n",
    "\n",
    "    descriptions = [e.procedure_description for e in examples]\n",
    "    images = [e.frames[0] for e in examples]\n",
    "    \n",
    "    assert len(descriptions) == len(images) == N_EXAMPLES_PER_ANNOTATOR, f\"Didn't get correct number of descriptions or images for chunk {chunk}.\"\n",
    "\n",
    "    sample_chunks.append((images, descriptions))\n",
    "\n",
    "assert len(sample_chunks) == n_chunks, \"Didn't get the correct number of chunks.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f473bef",
   "metadata": {},
   "source": [
    "# Generate Qualtrics forms for annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d336d1c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtravel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QUALTRICS_API_TOKEN, QUALTRICS_DATA_CENTER\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ci, (images, descriptions) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43msample_chunks\u001b[49m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ci \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "from travel.constants import QUALTRICS_API_TOKEN, QUALTRICS_DATA_CENTER, QUALTRICS_LIBRARY_NAME\n",
    "\n",
    "for ci, (images, descriptions) in enumerate(sample_chunks):\n",
    "\n",
    "    if ci == 1:\n",
    "        break\n",
    "\n",
    "    # Generate survey\n",
    "    result = generate_annotation_survey(\n",
    "        api_token=QUALTRICS_API_TOKEN,\n",
    "        data_center=QUALTRICS_DATA_CENTER,\n",
    "        library_name=QUALTRICS_LIBRARY_NAME,\n",
    "        images=images,\n",
    "        task_descriptions=descriptions,\n",
    "        survey_name=\"Procedural Mistake Detection Human Annotation\",\n",
    "        publish=True,\n",
    "    ) # NOTE: make sure questions are shuffled\n",
    "\n",
    "    if result:\n",
    "        if result.startswith('http'):\n",
    "            print(f\"\\n✅ Survey {ci} created successfully!\")\n",
    "            print(f\"🔗 Share this URL: {result}\")\n",
    "        else:\n",
    "            print(f\"\\n📝 Survey ID: {result}\")\n",
    "    else:\n",
    "        print(\"❌ Survey {ci} creation failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac83fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created survey with ID: SV_5tYnm6Ld4dUdw0e\n",
      "Processing question q001...\n",
      "{'libraryId': 'UR_2sCnXQ8e2Seuvxc', 'libraryName': 'Shane Storks'}\n",
      "✓ Created mistake detection question q001\n",
      "✓ Created confidence question for q001\n",
      "\n",
      "📋 Survey created successfully. Survey ID: SV_5tYnm6Ld4dUdw0e\n",
      "Manual URL: https://umich.qualtrics.com/jfe/form/SV_5tYnm6Ld4dUdw0e\n"
     ]
    }
   ],
   "source": [
    "from travel.constants import QUALTRICS_API_TOKEN, QUALTRICS_DATA_CENTER, QUALTRICS_LIBRARY_NAME\n",
    "\n",
    "\n",
    "test_image = Image.new('RGB', (100, 100), color='red')\n",
    "test_descriptions = [\"Test procedure: Do something simple\"]\n",
    "\n",
    "result = generate_annotation_survey(\n",
    "    api_token=QUALTRICS_API_TOKEN,\n",
    "    data_center=QUALTRICS_DATA_CENTER,\n",
    "    library_name=QUALTRICS_LIBRARY_NAME,\n",
    "    images=[test_image],\n",
    "    task_descriptions=test_descriptions,\n",
    "    survey_name=\"Test Survey\",\n",
    "    publish=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
