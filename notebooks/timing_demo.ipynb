{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35dacd94",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9accab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir(\"/nfs/turbo/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl\")\n",
    "from travel import init_travel\n",
    "init_travel()\n",
    "\n",
    "import argparse\n",
    "from collections import defaultdict, Counter\n",
    "from copy import deepcopy\n",
    "from itertools import product\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "import shutil\n",
    "import spacy\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForVision2Seq, AutoModelForCausalLM, AutoProcessor, BitsAndBytesConfig, AutoModelForSequenceClassification, AutoTokenizer, PhrasalConstraint           \n",
    "\n",
    "from travel.constants import RESULTS_DIR, IMAGES_CHUNK_SIZE, HF_TOKEN, CONFIG_PATH\n",
    "from travel.data.captaincook4d import CaptainCook4DDataset\n",
    "from travel.data.ego4d import Ego4DMistakeDetectionDataset\n",
    "from travel.data.mistake_detection import MistakeDetectionTasks\n",
    "from travel.data.vqa import VQAResponse, get_vqa_response_token_ids, VQAOutputs, DIALOG_START_TOKENS, IMAGE_TOKENS, USER_START_TOKENS, USER_END_TOKENS, ASSISTANT_START_TOKENS, ASSISTANT_END_TOKENS, IVQA_PREAMBLE, IVQA_SUCCESS_QUESTION\n",
    "from travel.data.vqg import generate_vqg_prompt_icl\n",
    "from travel.model import simple_lm_prompt_beam_search, simple_vlm_prompt_beam_search, compute_completion_log_likelihoods, compute_completion_log_likelihoods_encoder_decoder, compute_completion_log_likelihoods_vlm\n",
    "from travel.model.metrics import question_coherence_metrics_nli, question_coherence_metrics_vlm, generate_det_curve, compile_accuracy_and_coherence_metrics, generate_3d_overview_graph\n",
    "from travel.model.mistake_detection import MISTAKE_DETECTION_THRESHOLDS\n",
    "from travel.model.nli import NLI_MODEL_PATH, NLI_BATCH_SIZE\n",
    "from travel.model.vqa import run_vqa_with_visual_filter\n",
    "from travel.model.vqg import cleanup_generated_question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca060b9c",
   "metadata": {},
   "source": [
    "Load model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8531445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForVision2Seq, AutoModelForCausalLM, AutoProcessor, BitsAndBytesConfig, AutoModelForSequenceClassification, AutoTokenizer, PhrasalConstraint           \n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "VLM_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "# Load VLM - some VLMs may be under AutoModelForVision2Seq, some may be under AutoModelForCausalLM\n",
    "try:\n",
    "    vlm = AutoModelForVision2Seq.from_pretrained(VLM_NAME, quantization_config=bnb_config, trust_remote_code=True, token=HF_TOKEN)   \n",
    "except Exception as e:\n",
    "    print(\"Encountered exception when trying to load model with AutoModelForVision2Seq:\")\n",
    "    pprint(e)\n",
    "    \n",
    "    vlm = AutoModelForCausalLM.from_pretrained(VLM_NAME, quantization_config=bnb_config, trust_remote_code=True, token=HF_TOKEN)\n",
    "vlm_processor = AutoProcessor.from_pretrained(VLM_NAME, trust_remote_code=True, token=HF_TOKEN)\n",
    "vlm_processor.tokenizer.padding_side = \"left\"\n",
    "response_token_ids = get_vqa_response_token_ids(vlm_processor.tokenizer)\n",
    "\n",
    "# We'll use VLM's LM directly to generate questions\n",
    "if getattr(vlm, \"language_model\", None):\n",
    "    lm = vlm.language_model\n",
    "else:\n",
    "    lm = vlm\n",
    "tokenizer = vlm_processor.tokenizer\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# NLI model to score consistency and verifiability\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(NLI_MODEL_PATH, quantization_config=bnb_config)\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(NLI_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12bbc84",
   "metadata": {},
   "source": [
    "Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load approopriate evaluation dataset\n",
    "dataset = None\n",
    "for retry in range(5):\n",
    "    print(f\"Loading evaluation dataset (try {retry})...\")\n",
    "    try:\n",
    "        dataset = Ego4DMistakeDetectionDataset(data_split=\"val\", \n",
    "                                                mismatch_augmentation=True,\n",
    "                                                multi_frame=False,\n",
    "                                                debug_n_examples_per_class=100)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(\"Encountered error during data loading:\")\n",
    "        pprint(e)\n",
    "        time.sleep(60)\n",
    "if dataset is None:\n",
    "    raise ValueError(\"Could not load dataset after retrying!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ecc21f",
   "metadata": {},
   "source": [
    "Other global args:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddf7a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITERATIONS = 10\n",
    "N_ICL_DEMONSTRATIONS = 20\n",
    "UNSURE_RANGE = 0.1\n",
    "\n",
    "# kwargs to force question generations to have a \"?\" and start with words that would typically begin a yes/no question\n",
    "question_generation_constraints = [    \n",
    "    PhrasalConstraint(\n",
    "        [vlm_processor.tokenizer(\"Is it blue?\", add_special_tokens=False).input_ids[-1]]\n",
    "    ),\n",
    "]\n",
    "yes_no_q_tokens = [\n",
    "    vlm_processor.tokenizer(\"Is it blue?\", add_special_tokens=False).input_ids[0], \n",
    "    vlm_processor.tokenizer(\"Was it blue?\", add_special_tokens=False).input_ids[0],\n",
    "    vlm_processor.tokenizer(\"Are they blue?\", add_special_tokens=False).input_ids[0], \n",
    "    vlm_processor.tokenizer(\"Were they blue?\", add_special_tokens=False).input_ids[0],\n",
    "    vlm_processor.tokenizer(\"Does it look blue?\", add_special_tokens=False).input_ids[0],\n",
    "    vlm_processor.tokenizer(\"Do they look blue?\", add_special_tokens=False).input_ids[0],\n",
    "    vlm_processor.tokenizer(\"Did they look blue?\", add_special_tokens=False).input_ids[0],\n",
    "    vlm_processor.tokenizer(\"Has the oven turned on?\", add_special_tokens=False).input_ids[0],\n",
    "    vlm_processor.tokenizer(\"Have the eggs boiled?\", add_special_tokens=False).input_ids[0],\n",
    "    vlm_processor.tokenizer(\"Had the eggs boiled?\", add_special_tokens=False).input_ids[0],\n",
    "]\n",
    "begin_suppress_tokens = [t for t in list(range(vlm_processor.tokenizer.vocab_size)) if t not in yes_no_q_tokens]\n",
    "bad_words_ids = [[vlm_processor.tokenizer(\"Yes or no?\", add_special_tokens=False).input_ids[1]], \n",
    "                 vlm_processor.tokenizer(\"successful\", add_special_tokens=False).input_ids, \n",
    "                 vlm_processor.tokenizer(\"successfully\", add_special_tokens=False).input_ids, \n",
    "                 vlm_processor.tokenizer(\"completed\", add_special_tokens=False).input_ids,\n",
    "                 vlm_processor.tokenizer(\"procedure\", add_special_tokens=False).input_ids]\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"do_sample\": False,\n",
    "    \"num_beams\": 8,\n",
    "    \"num_return_sequences\": 4,\n",
    "    \"constraints\": question_generation_constraints,\n",
    "    \"begin_suppress_tokens\": begin_suppress_tokens,   \n",
    "    \"bad_words_ids\": bad_words_ids, \n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"length_penalty\": 1.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c959d345",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Likelihood-Based Ranking (Vanilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7810f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Modify below values to tuned values for each experiment\n",
    "EARLY_STOP_DELTA = 0.1\n",
    "CONFIDENT_RANGE = 0.1\n",
    "\n",
    "n_iterations_taken = []\n",
    "time_taken = []\n",
    "for batch_idx, batch_example in tqdm(enumerate(dataset.get_batches(1, \n",
    "                                                                    n_workers=1, \n",
    "                                                                    worker_index=0,\n",
    "                                                                    load_frames=False)), \n",
    "                                                desc=\"running iterative VQA inference\"):\n",
    "\n",
    "    batch_examples = [batch_example]\n",
    "    start = time.time()\n",
    "\n",
    "    # Take first frame (expect there to only be one frame)\n",
    "    batch_procedures = [example.procedure_description for example in batch_examples]\n",
    "    batch_frames = [Image.open(example.frames[0]) for example in batch_examples]\n",
    "\n",
    "    this_batch_size = len(batch_examples)\n",
    "\n",
    "    prompts = [\n",
    "        f'{DIALOG_START_TOKENS[VLM_NAME]}{USER_START_TOKENS[VLM_NAME]}{IMAGE_TOKENS[VLM_NAME]}{IVQA_PREAMBLE.format(procedure=procedure)}' \n",
    "        for procedure in batch_procedures\n",
    "    ]\n",
    "    questions = [[] for _ in range(this_batch_size)]\n",
    "    frames = [[] for _ in range(this_batch_size)]\n",
    "    candidate_questions = [[] for _ in range(this_batch_size)]\n",
    "    candidate_questions_scores = [[] for _ in range(this_batch_size)]\n",
    "    candidate_questions_sources = [[] for _ in range(this_batch_size)]\n",
    "    scores = [[] for _ in range(this_batch_size)]\n",
    "    answer_probs = [[] for _ in range(this_batch_size)] \n",
    "    answers = [[] for _ in range(this_batch_size)]\n",
    "    success_probs = [[] for _ in range(this_batch_size)]\n",
    "    success_probs_negated = [[] for _ in range(this_batch_size)]\n",
    "\n",
    "    # Iteratively generate questions\n",
    "    for question_idx in tqdm(range(MAX_ITERATIONS), desc=\"running iterative QA\"):\n",
    "        \n",
    "        # Generate a question (with beam search so we have several candidates)\n",
    "        prompts_q = [prompt + f\"{ASSISTANT_END_TOKENS[VLM_NAME] if question_idx != 0 else USER_END_TOKENS[VLM_NAME]}{USER_START_TOKENS[VLM_NAME]}Q:\" for prompt in prompts]\n",
    "        new_questions, _ = simple_lm_prompt_beam_search(lm,\n",
    "                                                        tokenizer,\n",
    "                                                        [prompt.replace(IMAGE_TOKENS[VLM_NAME], \"\") for prompt in prompts_q],\n",
    "                                                        max_new_tokens=20,\n",
    "                                                        batch_size=1,\n",
    "                                                        generation_kwargs=generation_kwargs)\n",
    "\n",
    "        new_questions = [[cleanup_generated_question(question) for question in beam_search_questions] for beam_search_questions in new_questions]                                \n",
    "        new_questions_sources = [[\"vlm\"] * len(beam_search_questions) for beam_search_questions in new_questions]\n",
    "\n",
    "        # Remove duplicate candidates\n",
    "        keep_idxs = [[question_idx for question_idx, question in enumerate(beam_search_outputs) if question not in beam_search_outputs[:question_idx]] for beam_search_outputs in new_questions]\n",
    "\n",
    "        # Try to remove any candidates that we've seen before (if we've seen all the candidates before, don't remove any)\n",
    "        keep_idxs_filtered = [[question_idx for question_idx, question in enumerate(beam_search_outputs) if question_idx in keep_idxs[batch_sub_idx] and question not in questions[batch_sub_idx]] for batch_sub_idx, beam_search_outputs in enumerate(new_questions)]\n",
    "        keep_idxs = [keep_idxs_filtered[batch_sub_idx] if len(keep_idxs_filtered[batch_sub_idx]) > 0 else keep_idxs[batch_sub_idx] for batch_sub_idx in range(this_batch_size)]\n",
    "\n",
    "        # Apply kept indices to new questions and their sources\n",
    "        new_questions = [[new_questions[batch_sub_idx][question_idx] for question_idx in this_keep_idxs] for batch_sub_idx, this_keep_idxs in enumerate(keep_idxs)]\n",
    "        new_questions_sources = [[new_questions_sources[batch_sub_idx][question_idx] for question_idx in this_keep_idxs] for batch_sub_idx, this_keep_idxs in enumerate(keep_idxs)]\n",
    "\n",
    "        # Save all candidates from beam search\n",
    "        for batch_sub_idx in range(len(candidate_questions)):\n",
    "            candidate_questions[batch_sub_idx].append(new_questions[batch_sub_idx])\n",
    "            candidate_questions_sources[batch_sub_idx].append(new_questions_sources[batch_sub_idx])\n",
    "\n",
    "        # Select best candidate question from pool\n",
    "        generation_scores = compute_completion_log_likelihoods(lm, tokenizer, [prompt.replace(IMAGE_TOKENS[VLM_NAME], \"\") for prompt in prompts_q], new_questions, batch_size=1)\n",
    "\n",
    "        # Select most likely question (first one in list)\n",
    "        selected_questions = []\n",
    "        new_scores = []\n",
    "        for batch_sub_idx, (beam_search_questions, beam_search_scores) in enumerate(zip(new_questions, generation_scores)):                    \n",
    "            assert len(beam_search_questions) == len(beam_search_scores), \"Expected candidate questions and their scores to have the same shape!\"\n",
    "\n",
    "            # Save all candidate scores\n",
    "            candidate_questions_scores[batch_sub_idx].append(beam_search_scores)\n",
    "\n",
    "            candidate_idxs = list(range(len(beam_search_questions)))\n",
    "\n",
    "            # Then pick candidate with highest score\n",
    "            best_candidate = max(candidate_idxs, key=lambda x: beam_search_scores[x] == max(beam_search_scores))\n",
    "            selected_questions.append(beam_search_questions[best_candidate])\n",
    "            new_scores.append(beam_search_scores[best_candidate])\n",
    "\n",
    "        new_questions = selected_questions\n",
    "\n",
    "        # Save scores for best questions\n",
    "        for batch_sub_idx in range(this_batch_size):\n",
    "            scores[batch_sub_idx].append(new_scores[batch_sub_idx])\n",
    "\n",
    "        # Save generated questions\n",
    "        for batch_sub_idx in range(this_batch_size):\n",
    "            questions[batch_sub_idx].append(new_questions[batch_sub_idx])\n",
    "\n",
    "        # Run VQA with generated questions (and optional spatial filter)\n",
    "        prompts_a = [prompt + f' {question}{USER_END_TOKENS[VLM_NAME]}{ASSISTANT_START_TOKENS[VLM_NAME]}A:' for prompt, question in zip(prompts_q, new_questions)]\n",
    "\n",
    "        # Effective prompt for VQA depends on whether we want to exclude dialog history from prompt\n",
    "        use_prompts_a = [f'{USER_START_TOKENS[VLM_NAME]}{IMAGE_TOKENS[VLM_NAME]}Q: {question}{USER_END_TOKENS[VLM_NAME]}{ASSISTANT_START_TOKENS[VLM_NAME]}A:' for prompt, question in zip(prompts_q, new_questions)]\n",
    "\n",
    "        new_answers_logits = run_vqa_with_visual_filter(vlm_processor=vlm_processor, \n",
    "                                                        vlm=vlm, \n",
    "                                                        batch_examples=batch_examples, \n",
    "                                                        batch_frames=batch_frames, \n",
    "                                                        prompts_a=use_prompts_a, \n",
    "                                                        new_questions=new_questions, \n",
    "                                                        question_idx=question_idx,\n",
    "                                                        batch_size=1,\n",
    "                                                        visual_filter=None,\n",
    "                                                        nlp=NotImplemented,\n",
    "                                                        visual_filter_mode=None,\n",
    "                                                        frame_cache_dir=None,\n",
    "                                                        is_encoder_decoder=\"-t5-\" in VLM_NAME.lower())\n",
    "\n",
    "        # Gather up VQA outputs (which automatically calculates answer probabilities from logits)\n",
    "        new_answers = [\n",
    "            VQAOutputs(\n",
    "                task_name=MistakeDetectionTasks(\"ego4d_single\"),\n",
    "                example_id=example.example_id,\n",
    "                procedure_id=example.procedure_id,\n",
    "                frame=example.frames[0],\n",
    "                prompt=prompt,\n",
    "                expected_answer=None,\n",
    "                response_token_ids=response_token_ids,\n",
    "                logits=logits,\n",
    "                question=question,\n",
    "            ) for logits, example, prompt, question in zip(new_answers_logits, batch_examples, prompts_a, new_questions)\n",
    "        ]\n",
    "        new_answers_str = [output.predicted_answer.name if np.abs(output.answer_probs[VQAResponse.Yes] - 0.5) >= UNSURE_RANGE else \"Unsure\" for output in new_answers]\n",
    "\n",
    "        # Save answers and their probabilities\n",
    "        for batch_sub_idx in range(this_batch_size):\n",
    "            answer_probs[batch_sub_idx].append([round(float(new_answers[batch_sub_idx].answer_probs[VQAResponse(answer_idx)]), 6) for answer_idx in range(2)])\n",
    "            answers[batch_sub_idx].append(new_answers_str[batch_sub_idx])\n",
    "        \n",
    "        \n",
    "        prompts = [prompt + \" \" + output for prompt, output in zip(prompts_a, new_answers_str)]\n",
    "\n",
    "        # Ask VLM probability of success\n",
    "        questions_success = [\n",
    "            IVQA_SUCCESS_QUESTION.format(procedure=procedure)\n",
    "            for procedure in batch_procedures\n",
    "        ]\n",
    "        prompts_success = [\n",
    "            prompt + f'{ASSISTANT_END_TOKENS[VLM_NAME]}{USER_START_TOKENS[VLM_NAME]}Q: {question}{USER_END_TOKENS[VLM_NAME]}{ASSISTANT_START_TOKENS[VLM_NAME]}A: '\n",
    "            for prompt, question in zip(prompts, questions_success)\n",
    "        ]\n",
    "\n",
    "        success_vqa_outputs = run_vqa_with_visual_filter(vlm_processor=vlm_processor, \n",
    "                                                            vlm=vlm, \n",
    "                                                            batch_examples=batch_examples, \n",
    "                                                            batch_frames=batch_frames, \n",
    "                                                            prompts_a=prompts_success, \n",
    "                                                            new_questions=questions_success, \n",
    "                                                            question_idx=f\"{question_idx}_success\",\n",
    "                                                            batch_size=1,\n",
    "                                                            visual_filter=None,\n",
    "                                                            nlp=None,\n",
    "                                                            visual_filter_mode=None,\n",
    "                                                            frame_cache_dir=None,\n",
    "                                                            is_encoder_decoder=\"-t5-\" in VLM_NAME.lower(),\n",
    "                                                            ignore_frames=False)\n",
    "        success_vqa_outputs = [\n",
    "            VQAOutputs(\n",
    "                task_name=MistakeDetectionTasks(\"ego4d_single\"),\n",
    "                example_id=example.example_id,\n",
    "                procedure_id=example.procedure_id,\n",
    "                frame=example.frames[0],\n",
    "                prompt=prompt,\n",
    "                expected_answer=None,\n",
    "                response_token_ids=response_token_ids,\n",
    "                logits=logits,\n",
    "                question=question,\n",
    "            ) for logits, example, prompt, question in zip(success_vqa_outputs, batch_examples, prompts_a, new_questions)\n",
    "        ]               \n",
    "\n",
    "        # Save success probability for this turn\n",
    "        for batch_sub_idx in range(this_batch_size):\n",
    "            success_probs[batch_sub_idx].append(\n",
    "                round(float(success_vqa_outputs[batch_sub_idx].answer_probs[VQAResponse.Yes]), 6)\n",
    "            )\n",
    "\n",
    "        # Clear out VQA outputs now because they occupy a lot of memory\n",
    "        del new_answers\n",
    "        del success_vqa_outputs\n",
    "\n",
    "        # Check if we can stop based on early stopping criteria\n",
    "        # if success score doesn't change enough over 3 turns, stop incorporating questions\n",
    "        # (we still run inference across all questions for efficiency and simplicity, but later can make a proper demo script)\n",
    "        if question_idx >= 2:\n",
    "            if np.abs(success_probs[0][question_idx-1] - success_probs[0][question_idx-2]) < EARLY_STOP_DELTA and np.abs(success_probs[0][question_idx] - success_probs[0][question_idx-1]) < EARLY_STOP_DELTA:\n",
    "                n_iterations_taken.append(question_idx+1)\n",
    "                print(\"Early stop!\")\n",
    "                break\n",
    "        # OR if success score is within confident delta, stop\n",
    "        if success_probs[0][-1] < CONFIDENT_RANGE or 1.0 - success_probs[0][-1] < CONFIDENT_RANGE:\n",
    "            n_iterations_taken.append(question_idx+1)\n",
    "            print(\"Early stop!\")\n",
    "            break\n",
    "        # If it's the last iteration, record\n",
    "        if question_idx == MAX_ITERATIONS-1:\n",
    "            n_iterations_taken.append(MAX_ITERATIONS)\n",
    "\n",
    "    end = time.time()\n",
    "    time_taken.append(end-start)\n",
    "\n",
    "print(\"Avg. # iterations:\", np.mean(n_iterations_taken))\n",
    "print(\"Std. # iterations:\", np.std(n_iterations_taken))\n",
    "print(\"Avg. time (sec.):\", np.mean(time_taken))\n",
    "print(\"Std. time (sec.):\", np.std(time_taken))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45a1cdf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# + Coherence-Based Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74c7404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Modify below values to tuned values for each experiment\n",
    "EARLY_STOP_DELTA = 0.1\n",
    "CONFIDENT_RANGE = 0.1\n",
    "\n",
    "n_iterations_taken = []\n",
    "time_taken = []\n",
    "for batch_idx, batch_example in tqdm(enumerate(dataset.get_batches(1, \n",
    "                                                                    n_workers=1, \n",
    "                                                                    worker_index=0,\n",
    "                                                                    load_frames=False)), \n",
    "                                                desc=\"running iterative VQA inference\"):\n",
    "\n",
    "    batch_examples = [batch_example]\n",
    "    start = time.time()\n",
    "\n",
    "    # Take first frame (expect there to only be one frame)\n",
    "    batch_procedures = [example.procedure_description for example in batch_examples]\n",
    "    batch_frames = [Image.open(example.frames[0]) for example in batch_examples]\n",
    "\n",
    "    this_batch_size = len(batch_examples)\n",
    "\n",
    "    prompts = [\n",
    "        f'{DIALOG_START_TOKENS[VLM_NAME]}{USER_START_TOKENS[VLM_NAME]}{IMAGE_TOKENS[VLM_NAME]}{IVQA_PREAMBLE.format(procedure=procedure)}' \n",
    "        for procedure in batch_procedures\n",
    "    ]\n",
    "    questions = [[] for _ in range(this_batch_size)]\n",
    "    frames = [[] for _ in range(this_batch_size)]\n",
    "    candidate_questions = [[] for _ in range(this_batch_size)]\n",
    "    candidate_questions_scores = [[] for _ in range(this_batch_size)]\n",
    "    candidate_questions_sources = [[] for _ in range(this_batch_size)]\n",
    "    scores = [[] for _ in range(this_batch_size)]\n",
    "    answer_probs = [[] for _ in range(this_batch_size)] \n",
    "    answers = [[] for _ in range(this_batch_size)]\n",
    "    success_probs = [[] for _ in range(this_batch_size)]\n",
    "    success_probs_negated = [[] for _ in range(this_batch_size)]\n",
    "\n",
    "    # Iteratively generate questions\n",
    "    for question_idx in tqdm(range(MAX_ITERATIONS), desc=\"running iterative QA\"):\n",
    "        \n",
    "        # Generate a question (with beam search so we have several candidates)\n",
    "        prompts_q = [prompt + f\"{ASSISTANT_END_TOKENS[VLM_NAME] if question_idx != 0 else USER_END_TOKENS[VLM_NAME]}{USER_START_TOKENS[VLM_NAME]}Q:\" for prompt in prompts]\n",
    "        new_questions, _ = simple_lm_prompt_beam_search(lm,\n",
    "                                                        tokenizer,\n",
    "                                                        [prompt.replace(IMAGE_TOKENS[VLM_NAME], \"\") for prompt in prompts_q],\n",
    "                                                        max_new_tokens=20,\n",
    "                                                        batch_size=1,\n",
    "                                                        generation_kwargs=generation_kwargs)\n",
    "\n",
    "        new_questions = [[cleanup_generated_question(question) for question in beam_search_questions] for beam_search_questions in new_questions]                                \n",
    "        new_questions_sources = [[\"vlm\"] * len(beam_search_questions) for beam_search_questions in new_questions]\n",
    "\n",
    "        # Remove duplicate candidates\n",
    "        keep_idxs = [[question_idx for question_idx, question in enumerate(beam_search_outputs) if question not in beam_search_outputs[:question_idx]] for beam_search_outputs in new_questions]\n",
    "\n",
    "        # Try to remove any candidates that we've seen before (if we've seen all the candidates before, don't remove any)\n",
    "        keep_idxs_filtered = [[question_idx for question_idx, question in enumerate(beam_search_outputs) if question_idx in keep_idxs[batch_sub_idx] and question not in questions[batch_sub_idx]] for batch_sub_idx, beam_search_outputs in enumerate(new_questions)]\n",
    "        keep_idxs = [keep_idxs_filtered[batch_sub_idx] if len(keep_idxs_filtered[batch_sub_idx]) > 0 else keep_idxs[batch_sub_idx] for batch_sub_idx in range(this_batch_size)]\n",
    "\n",
    "        # Apply kept indices to new questions and their sources\n",
    "        new_questions = [[new_questions[batch_sub_idx][question_idx] for question_idx in this_keep_idxs] for batch_sub_idx, this_keep_idxs in enumerate(keep_idxs)]\n",
    "        new_questions_sources = [[new_questions_sources[batch_sub_idx][question_idx] for question_idx in this_keep_idxs] for batch_sub_idx, this_keep_idxs in enumerate(keep_idxs)]\n",
    "\n",
    "        # Save all candidates from beam search\n",
    "        for batch_sub_idx in range(len(candidate_questions)):\n",
    "            candidate_questions[batch_sub_idx].append(new_questions[batch_sub_idx])\n",
    "            candidate_questions_sources[batch_sub_idx].append(new_questions_sources[batch_sub_idx])\n",
    "\n",
    "        # Select best candidate question from pool\n",
    "        # Calculate coherence metrics for each candidate question\n",
    "        nli_outputs = question_coherence_metrics_nli(\n",
    "            nli_tokenizer, \n",
    "            nli_model,\n",
    "            tokenizer,\n",
    "            lm,\n",
    "            [procedure for procedure, beam_search_questions in zip(batch_procedures, new_questions) for _ in beam_search_questions],\n",
    "            [question for beam_search_questions in new_questions for question in beam_search_questions],\n",
    "            previous_questions=[[q for qi, q in enumerate(batch_idx_questions) if batch_idx_answers[qi] != \"Unsure\"] for batch_idx_questions, batch_idx_answers, beam_search_questions in zip(questions, answers, new_questions) for _ in beam_search_questions],\n",
    "            previous_answers=[[a for a in batch_idx_answers if a != \"Unsure\"] for batch_idx_answers, beam_search_questions in zip(answers, new_questions) for _ in beam_search_questions],\n",
    "            rephrase_batch_size=10\n",
    "        )\n",
    "\n",
    "        # Select best candidate based on coherence metrics\n",
    "        selected_questions = []\n",
    "        new_scores = []\n",
    "        parallel_idx = 0\n",
    "        ranking_key_mapping = {\n",
    "            \"relevance\": \"relevance_marginal\",\n",
    "            \"informativeness\": \"informativeness_marginal\",\n",
    "            \"coherence\": \"informativeness_marginal_x_relevance_marginal\",\n",
    "        }\n",
    "        for batch_sub_idx, beam_search_questions in enumerate(new_questions):\n",
    "            this_nli_outputs = [{k: round(float(nli_outputs[k][i]), 3) if type(nli_outputs[k][i]) != str else nli_outputs[k][i] for k in nli_outputs} for i in range(parallel_idx, parallel_idx + len(beam_search_questions))]\n",
    "            candidate_questions_scores[batch_sub_idx].append(this_nli_outputs)\n",
    "            parallel_idx += len(beam_search_questions)\n",
    "\n",
    "            # Use marginal relevance (consistency) and expected informativeness (verifiability) to rank candidates\n",
    "            candidate_scores = np.array(\n",
    "                [candidate_metrics[ranking_key_mapping[\"coherence\"]] for candidate_metrics in this_nli_outputs]\n",
    "            )\n",
    "\n",
    "            best_candidate = np.argmax(candidate_scores)\n",
    "            selected_questions.append(beam_search_questions[best_candidate])\n",
    "            new_scores.append(round(float(candidate_scores[best_candidate]), 6))\n",
    "        \n",
    "        new_questions = selected_questions\n",
    "                \n",
    "        # Save scores for best questions\n",
    "        for batch_sub_idx in range(this_batch_size):\n",
    "            scores[batch_sub_idx].append(new_scores[batch_sub_idx])\n",
    "\n",
    "        # Save generated questions\n",
    "        for batch_sub_idx in range(this_batch_size):\n",
    "            questions[batch_sub_idx].append(new_questions[batch_sub_idx])\n",
    "\n",
    "        # Run VQA with generated questions (and optional spatial filter)\n",
    "        prompts_a = [prompt + f' {question}{USER_END_TOKENS[VLM_NAME]}{ASSISTANT_START_TOKENS[VLM_NAME]}A:' for prompt, question in zip(prompts_q, new_questions)]\n",
    "\n",
    "        # Effective prompt for VQA depends on whether we want to exclude dialog history from prompt\n",
    "        use_prompts_a = [f'{USER_START_TOKENS[VLM_NAME]}{IMAGE_TOKENS[VLM_NAME]}Q: {question}{USER_END_TOKENS[VLM_NAME]}{ASSISTANT_START_TOKENS[VLM_NAME]}A:' for prompt, question in zip(prompts_q, new_questions)]\n",
    "\n",
    "        new_answers_logits = run_vqa_with_visual_filter(vlm_processor=vlm_processor, \n",
    "                                                        vlm=vlm, \n",
    "                                                        batch_examples=batch_examples, \n",
    "                                                        batch_frames=batch_frames, \n",
    "                                                        prompts_a=use_prompts_a, \n",
    "                                                        new_questions=new_questions, \n",
    "                                                        question_idx=question_idx,\n",
    "                                                        batch_size=1,\n",
    "                                                        visual_filter=None,\n",
    "                                                        nlp=NotImplemented,\n",
    "                                                        visual_filter_mode=None,\n",
    "                                                        frame_cache_dir=None,\n",
    "                                                        is_encoder_decoder=\"-t5-\" in VLM_NAME.lower())\n",
    "\n",
    "        # Gather up VQA outputs (which automatically calculates answer probabilities from logits)\n",
    "        new_answers = [\n",
    "            VQAOutputs(\n",
    "                task_name=MistakeDetectionTasks(\"ego4d_single\"),\n",
    "                example_id=example.example_id,\n",
    "                procedure_id=example.procedure_id,\n",
    "                frame=example.frames[0],\n",
    "                prompt=prompt,\n",
    "                expected_answer=None,\n",
    "                response_token_ids=response_token_ids,\n",
    "                logits=logits,\n",
    "                question=question,\n",
    "            ) for logits, example, prompt, question in zip(new_answers_logits, batch_examples, prompts_a, new_questions)\n",
    "        ]\n",
    "        new_answers_str = [output.predicted_answer.name if np.abs(output.answer_probs[VQAResponse.Yes] - 0.5) >= UNSURE_RANGE else \"Unsure\" for output in new_answers]\n",
    "\n",
    "        # Save answers and their probabilities\n",
    "        for batch_sub_idx in range(this_batch_size):\n",
    "            answer_probs[batch_sub_idx].append([round(float(new_answers[batch_sub_idx].answer_probs[VQAResponse(answer_idx)]), 6) for answer_idx in range(2)])\n",
    "            answers[batch_sub_idx].append(new_answers_str[batch_sub_idx])\n",
    "        \n",
    "        \n",
    "        prompts = [prompt + \" \" + output for prompt, output in zip(prompts_a, new_answers_str)]\n",
    "\n",
    "        # Ask VLM probability of success\n",
    "        questions_success = [\n",
    "            IVQA_SUCCESS_QUESTION.format(procedure=procedure)\n",
    "            for procedure in batch_procedures\n",
    "        ]\n",
    "        prompts_success = [\n",
    "            prompt + f'{ASSISTANT_END_TOKENS[VLM_NAME]}{USER_START_TOKENS[VLM_NAME]}Q: {question}{USER_END_TOKENS[VLM_NAME]}{ASSISTANT_START_TOKENS[VLM_NAME]}A: '\n",
    "            for prompt, question in zip(prompts, questions_success)\n",
    "        ]\n",
    "\n",
    "        success_vqa_outputs = run_vqa_with_visual_filter(vlm_processor=vlm_processor, \n",
    "                                                            vlm=vlm, \n",
    "                                                            batch_examples=batch_examples, \n",
    "                                                            batch_frames=batch_frames, \n",
    "                                                            prompts_a=prompts_success, \n",
    "                                                            new_questions=questions_success, \n",
    "                                                            question_idx=f\"{question_idx}_success\",\n",
    "                                                            batch_size=1,\n",
    "                                                            visual_filter=None,\n",
    "                                                            nlp=None,\n",
    "                                                            visual_filter_mode=None,\n",
    "                                                            frame_cache_dir=None,\n",
    "                                                            is_encoder_decoder=\"-t5-\" in VLM_NAME.lower(),\n",
    "                                                            ignore_frames=False)\n",
    "        success_vqa_outputs = [\n",
    "            VQAOutputs(\n",
    "                task_name=MistakeDetectionTasks(\"ego4d_single\"),\n",
    "                example_id=example.example_id,\n",
    "                procedure_id=example.procedure_id,\n",
    "                frame=example.frames[0],\n",
    "                prompt=prompt,\n",
    "                expected_answer=None,\n",
    "                response_token_ids=response_token_ids,\n",
    "                logits=logits,\n",
    "                question=question,\n",
    "            ) for logits, example, prompt, question in zip(success_vqa_outputs, batch_examples, prompts_a, new_questions)\n",
    "        ]               \n",
    "\n",
    "        # Save success probability for this turn\n",
    "        for batch_sub_idx in range(this_batch_size):\n",
    "            success_probs[batch_sub_idx].append(\n",
    "                round(float(success_vqa_outputs[batch_sub_idx].answer_probs[VQAResponse.Yes]), 6)\n",
    "            )\n",
    "\n",
    "        # Clear out VQA outputs now because they occupy a lot of memory\n",
    "        del new_answers\n",
    "        del success_vqa_outputs\n",
    "\n",
    "        # Check if we can stop based on early stopping criteria\n",
    "        # if success score doesn't change enough over 3 turns, stop incorporating questions\n",
    "        # (we still run inference across all questions for efficiency and simplicity, but later can make a proper demo script)\n",
    "        if question_idx >= 2:\n",
    "            if np.abs(success_probs[0][question_idx-1] - success_probs[0][question_idx-2]) < EARLY_STOP_DELTA and np.abs(success_probs[0][question_idx] - success_probs[0][question_idx-1]) < EARLY_STOP_DELTA:\n",
    "                n_iterations_taken.append(question_idx+1)\n",
    "                print(\"Early stop!\")\n",
    "                break\n",
    "        # OR if success score is within confident delta, stop\n",
    "        if success_probs[0][-1] < CONFIDENT_RANGE or 1.0 - success_probs[0][-1] < CONFIDENT_RANGE:\n",
    "            n_iterations_taken.append(question_idx+1)\n",
    "            print(\"Early stop!\")\n",
    "            break\n",
    "        # If it's the last iteration, record\n",
    "        if question_idx == MAX_ITERATIONS-1:\n",
    "            n_iterations_taken.append(MAX_ITERATIONS)\n",
    "\n",
    "    end = time.time()\n",
    "    time_taken.append(end-start)\n",
    "\n",
    "print(\"Avg. # iterations:\", np.mean(n_iterations_taken))\n",
    "print(\"Std. # iterations:\", np.std(n_iterations_taken))\n",
    "print(\"Avg. time (sec.):\", np.mean(time_taken))\n",
    "print(\"Std. time (sec.):\", np.std(time_taken))\n",
    "print(\"Avg. runtime per iteration (sec.):\", np.mean([t / i for i, t in zip(n_iterations_taken, time_taken)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778afe5f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# + In-Context Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ded02ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Modify below values to tuned values for each experiment\n",
    "EARLY_STOP_DELTA = 0.1\n",
    "CONFIDENT_RANGE = 0.1\n",
    "\n",
    "n_iterations_taken = []\n",
    "time_taken = []\n",
    "for batch_idx, batch_example in tqdm(enumerate(dataset.get_batches(1, \n",
    "                                                                    n_workers=1, \n",
    "                                                                    worker_index=0,\n",
    "                                                                    load_frames=False)), \n",
    "                                                desc=\"running iterative VQA inference\"):\n",
    "\n",
    "    batch_examples = [batch_example]\n",
    "    start = time.time()\n",
    "\n",
    "    # Take first frame (expect there to only be one frame)\n",
    "    batch_procedures = [example.procedure_description for example in batch_examples]\n",
    "    batch_frames = [Image.open(example.frames[0]) for example in batch_examples]\n",
    "\n",
    "    this_batch_size = len(batch_examples)\n",
    "\n",
    "    prompts = [\n",
    "        f'{DIALOG_START_TOKENS[VLM_NAME]}{USER_START_TOKENS[VLM_NAME]}{IMAGE_TOKENS[VLM_NAME]}{IVQA_PREAMBLE.format(procedure=procedure)}' \n",
    "        for procedure in batch_procedures\n",
    "    ]\n",
    "    questions = [[] for _ in range(this_batch_size)]\n",
    "    frames = [[] for _ in range(this_batch_size)]\n",
    "    candidate_questions = [[] for _ in range(this_batch_size)]\n",
    "    candidate_questions_scores = [[] for _ in range(this_batch_size)]\n",
    "    candidate_questions_sources = [[] for _ in range(this_batch_size)]\n",
    "    scores = [[] for _ in range(this_batch_size)]\n",
    "    answer_probs = [[] for _ in range(this_batch_size)] \n",
    "    answers = [[] for _ in range(this_batch_size)]\n",
    "    success_probs = [[] for _ in range(this_batch_size)]\n",
    "    success_probs_negated = [[] for _ in range(this_batch_size)]\n",
    "\n",
    "    # Iteratively generate questions\n",
    "    for question_idx in tqdm(range(MAX_ITERATIONS), desc=\"running iterative QA\"):\n",
    "        \n",
    "        # Generate a question (with beam search so we have several candidates)\n",
    "        prompts_q = [prompt + f\"{ASSISTANT_END_TOKENS[VLM_NAME] if question_idx != 0 else USER_END_TOKENS[VLM_NAME]}{USER_START_TOKENS[VLM_NAME]}Q:\" for prompt in prompts]\n",
    "        new_questions, _ = simple_lm_prompt_beam_search(lm,\n",
    "                                                        tokenizer,\n",
    "                                                        [prompt.replace(IMAGE_TOKENS[VLM_NAME], \"\") for prompt in prompts_q],\n",
    "                                                        max_new_tokens=20,\n",
    "                                                        batch_size=1,\n",
    "                                                        generation_kwargs=generation_kwargs)\n",
    "\n",
    "        new_questions = [[cleanup_generated_question(question) for question in beam_search_questions] for beam_search_questions in new_questions]                                \n",
    "        new_questions_sources = [[\"vlm\"] * len(beam_search_questions) for beam_search_questions in new_questions]\n",
    "\n",
    "        # Optionally inject more candidates from original VQG ICL code\n",
    "        icl_prompts = [generate_vqg_prompt_icl(procedure, N_ICL_DEMONSTRATIONS, include_answers=False) for procedure in batch_procedures] # Create ICL prompt\n",
    "        icl_prompts = [\n",
    "            prompt + '\\n'.join([str(pqi+1) + ' ' + pq for pqi, pq in enumerate(previous_questions[-2:])]) + (\"\\n\" if len(previous_questions) > 0 else \"\") + f\"{len(previous_questions) + 1}. \" \n",
    "            for prompt, previous_questions in zip(icl_prompts, questions)\n",
    "        ] # Add some previous questions if possible (take last 2 that were asked)\n",
    "        icl_new_questions, _ = simple_lm_prompt_beam_search(lm,\n",
    "                                                            tokenizer,\n",
    "                                                            icl_prompts,\n",
    "                                                            max_new_tokens=20,\n",
    "                                                            batch_size=1,\n",
    "                                                            generation_kwargs=generation_kwargs)\n",
    "        \n",
    "        icl_new_questions = [[cleanup_generated_question(question) for question in beam_search_questions] for beam_search_questions in icl_new_questions]\n",
    "        \n",
    "        for batch_sub_idx in range(this_batch_size):\n",
    "            new_questions[batch_sub_idx] += icl_new_questions[batch_sub_idx]\n",
    "            new_questions_sources[batch_sub_idx] += [\"icl\"] * len(icl_new_questions[batch_sub_idx])\n",
    "\n",
    "        # Remove duplicate candidates\n",
    "        keep_idxs = [[question_idx for question_idx, question in enumerate(beam_search_outputs) if question not in beam_search_outputs[:question_idx]] for beam_search_outputs in new_questions]\n",
    "\n",
    "        # Try to remove any candidates that we've seen before (if we've seen all the candidates before, don't remove any)\n",
    "        keep_idxs_filtered = [[question_idx for question_idx, question in enumerate(beam_search_outputs) if question_idx in keep_idxs[batch_sub_idx] and question not in questions[batch_sub_idx]] for batch_sub_idx, beam_search_outputs in enumerate(new_questions)]\n",
    "        keep_idxs = [keep_idxs_filtered[batch_sub_idx] if len(keep_idxs_filtered[batch_sub_idx]) > 0 else keep_idxs[batch_sub_idx] for batch_sub_idx in range(this_batch_size)]\n",
    "\n",
    "        # Apply kept indices to new questions and their sources\n",
    "        new_questions = [[new_questions[batch_sub_idx][question_idx] for question_idx in this_keep_idxs] for batch_sub_idx, this_keep_idxs in enumerate(keep_idxs)]\n",
    "        new_questions_sources = [[new_questions_sources[batch_sub_idx][question_idx] for question_idx in this_keep_idxs] for batch_sub_idx, this_keep_idxs in enumerate(keep_idxs)]\n",
    "\n",
    "        # Save all candidates from beam search\n",
    "        for batch_sub_idx in range(len(candidate_questions)):\n",
    "            candidate_questions[batch_sub_idx].append(new_questions[batch_sub_idx])\n",
    "            candidate_questions_sources[batch_sub_idx].append(new_questions_sources[batch_sub_idx])\n",
    "\n",
    "        # Select best candidate question from pool\n",
    "        # Calculate coherence metrics for each candidate question\n",
    "        nli_outputs = question_coherence_metrics_nli(\n",
    "            nli_tokenizer, \n",
    "            nli_model,\n",
    "            tokenizer,\n",
    "            lm,\n",
    "            [procedure for procedure, beam_search_questions in zip(batch_procedures, new_questions) for _ in beam_search_questions],\n",
    "            [question for beam_search_questions in new_questions for question in beam_search_questions],\n",
    "            previous_questions=[[q for qi, q in enumerate(batch_idx_questions) if batch_idx_answers[qi] != \"Unsure\"] for batch_idx_questions, batch_idx_answers, beam_search_questions in zip(questions, answers, new_questions) for _ in beam_search_questions],\n",
    "            previous_answers=[[a for a in batch_idx_answers if a != \"Unsure\"] for batch_idx_answers, beam_search_questions in zip(answers, new_questions) for _ in beam_search_questions],\n",
    "            rephrase_batch_size=10\n",
    "        )\n",
    "\n",
    "        # Select best candidate based on coherence metrics\n",
    "        selected_questions = []\n",
    "        new_scores = []\n",
    "        parallel_idx = 0\n",
    "        ranking_key_mapping = {\n",
    "            \"relevance\": \"relevance_marginal\",\n",
    "            \"informativeness\": \"informativeness_marginal\",\n",
    "            \"coherence\": \"informativeness_marginal_x_relevance_marginal\",\n",
    "        }\n",
    "        for batch_sub_idx, beam_search_questions in enumerate(new_questions):\n",
    "            this_nli_outputs = [{k: round(float(nli_outputs[k][i]), 3) if type(nli_outputs[k][i]) != str else nli_outputs[k][i] for k in nli_outputs} for i in range(parallel_idx, parallel_idx + len(beam_search_questions))]\n",
    "            candidate_questions_scores[batch_sub_idx].append(this_nli_outputs)\n",
    "            parallel_idx += len(beam_search_questions)\n",
    "\n",
    "            # Use marginal relevance (consistency) and expected informativeness (verifiability) to rank candidates\n",
    "            candidate_scores = np.array(\n",
    "                [candidate_metrics[ranking_key_mapping[\"coherence\"]] for candidate_metrics in this_nli_outputs]\n",
    "            )\n",
    "\n",
    "            best_candidate = np.argmax(candidate_scores)\n",
    "            selected_questions.append(beam_search_questions[best_candidate])\n",
    "            new_scores.append(round(float(candidate_scores[best_candidate]), 6))\n",
    "        \n",
    "        new_questions = selected_questions\n",
    "                \n",
    "        # Save scores for best questions\n",
    "        for batch_sub_idx in range(this_batch_size):\n",
    "            scores[batch_sub_idx].append(new_scores[batch_sub_idx])\n",
    "\n",
    "        # Save generated questions\n",
    "        for batch_sub_idx in range(this_batch_size):\n",
    "            questions[batch_sub_idx].append(new_questions[batch_sub_idx])\n",
    "\n",
    "        # Run VQA with generated questions (and optional spatial filter)\n",
    "        prompts_a = [prompt + f' {question}{USER_END_TOKENS[VLM_NAME]}{ASSISTANT_START_TOKENS[VLM_NAME]}A:' for prompt, question in zip(prompts_q, new_questions)]\n",
    "\n",
    "        # Effective prompt for VQA depends on whether we want to exclude dialog history from prompt\n",
    "        use_prompts_a = [f'{USER_START_TOKENS[VLM_NAME]}{IMAGE_TOKENS[VLM_NAME]}Q: {question}{USER_END_TOKENS[VLM_NAME]}{ASSISTANT_START_TOKENS[VLM_NAME]}A:' for prompt, question in zip(prompts_q, new_questions)]\n",
    "\n",
    "        new_answers_logits = run_vqa_with_visual_filter(vlm_processor=vlm_processor, \n",
    "                                                        vlm=vlm, \n",
    "                                                        batch_examples=batch_examples, \n",
    "                                                        batch_frames=batch_frames, \n",
    "                                                        prompts_a=use_prompts_a, \n",
    "                                                        new_questions=new_questions, \n",
    "                                                        question_idx=question_idx,\n",
    "                                                        batch_size=1,\n",
    "                                                        visual_filter=None,\n",
    "                                                        nlp=NotImplemented,\n",
    "                                                        visual_filter_mode=None,\n",
    "                                                        frame_cache_dir=None,\n",
    "                                                        is_encoder_decoder=\"-t5-\" in VLM_NAME.lower())\n",
    "\n",
    "        # Gather up VQA outputs (which automatically calculates answer probabilities from logits)\n",
    "        new_answers = [\n",
    "            VQAOutputs(\n",
    "                task_name=MistakeDetectionTasks(\"ego4d_single\"),\n",
    "                example_id=example.example_id,\n",
    "                procedure_id=example.procedure_id,\n",
    "                frame=example.frames[0],\n",
    "                prompt=prompt,\n",
    "                expected_answer=None,\n",
    "                response_token_ids=response_token_ids,\n",
    "                logits=logits,\n",
    "                question=question,\n",
    "            ) for logits, example, prompt, question in zip(new_answers_logits, batch_examples, prompts_a, new_questions)\n",
    "        ]\n",
    "        new_answers_str = [output.predicted_answer.name if np.abs(output.answer_probs[VQAResponse.Yes] - 0.5) >= UNSURE_RANGE else \"Unsure\" for output in new_answers]\n",
    "\n",
    "        # Save answers and their probabilities\n",
    "        for batch_sub_idx in range(this_batch_size):\n",
    "            answer_probs[batch_sub_idx].append([round(float(new_answers[batch_sub_idx].answer_probs[VQAResponse(answer_idx)]), 6) for answer_idx in range(2)])\n",
    "            answers[batch_sub_idx].append(new_answers_str[batch_sub_idx])\n",
    "        \n",
    "        \n",
    "        prompts = [prompt + \" \" + output for prompt, output in zip(prompts_a, new_answers_str)]\n",
    "\n",
    "        # Ask VLM probability of success\n",
    "        questions_success = [\n",
    "            IVQA_SUCCESS_QUESTION.format(procedure=procedure)\n",
    "            for procedure in batch_procedures\n",
    "        ]\n",
    "        prompts_success = [\n",
    "            prompt + f'{ASSISTANT_END_TOKENS[VLM_NAME]}{USER_START_TOKENS[VLM_NAME]}Q: {question}{USER_END_TOKENS[VLM_NAME]}{ASSISTANT_START_TOKENS[VLM_NAME]}A: '\n",
    "            for prompt, question in zip(prompts, questions_success)\n",
    "        ]\n",
    "\n",
    "        success_vqa_outputs = run_vqa_with_visual_filter(vlm_processor=vlm_processor, \n",
    "                                                            vlm=vlm, \n",
    "                                                            batch_examples=batch_examples, \n",
    "                                                            batch_frames=batch_frames, \n",
    "                                                            prompts_a=prompts_success, \n",
    "                                                            new_questions=questions_success, \n",
    "                                                            question_idx=f\"{question_idx}_success\",\n",
    "                                                            batch_size=1,\n",
    "                                                            visual_filter=None,\n",
    "                                                            nlp=None,\n",
    "                                                            visual_filter_mode=None,\n",
    "                                                            frame_cache_dir=None,\n",
    "                                                            is_encoder_decoder=\"-t5-\" in VLM_NAME.lower(),\n",
    "                                                            ignore_frames=False)\n",
    "        success_vqa_outputs = [\n",
    "            VQAOutputs(\n",
    "                task_name=MistakeDetectionTasks(\"ego4d_single\"),\n",
    "                example_id=example.example_id,\n",
    "                procedure_id=example.procedure_id,\n",
    "                frame=example.frames[0],\n",
    "                prompt=prompt,\n",
    "                expected_answer=None,\n",
    "                response_token_ids=response_token_ids,\n",
    "                logits=logits,\n",
    "                question=question,\n",
    "            ) for logits, example, prompt, question in zip(success_vqa_outputs, batch_examples, prompts_a, new_questions)\n",
    "        ]               \n",
    "\n",
    "        # Save success probability for this turn\n",
    "        for batch_sub_idx in range(this_batch_size):\n",
    "            success_probs[batch_sub_idx].append(\n",
    "                round(float(success_vqa_outputs[batch_sub_idx].answer_probs[VQAResponse.Yes]), 6)\n",
    "            )\n",
    "\n",
    "        # Clear out VQA outputs now because they occupy a lot of memory\n",
    "        del new_answers\n",
    "        del success_vqa_outputs\n",
    "\n",
    "        # Check if we can stop based on early stopping criteria\n",
    "        # if success score doesn't change enough over 3 turns, stop incorporating questions\n",
    "        # (we still run inference across all questions for efficiency and simplicity, but later can make a proper demo script)\n",
    "        if question_idx >= 2:\n",
    "            if np.abs(success_probs[0][question_idx-1] - success_probs[0][question_idx-2]) < EARLY_STOP_DELTA and np.abs(success_probs[0][question_idx] - success_probs[0][question_idx-1]) < EARLY_STOP_DELTA:\n",
    "                n_iterations_taken.append(question_idx+1)\n",
    "                print(\"Early stop!\")\n",
    "                break\n",
    "        # OR if success score is within confident delta, stop\n",
    "        if success_probs[0][-1] < CONFIDENT_RANGE or 1.0 - success_probs[0][-1] < CONFIDENT_RANGE:\n",
    "            n_iterations_taken.append(question_idx+1)\n",
    "            print(\"Early stop!\")\n",
    "            break\n",
    "        # If it's the last iteration, record\n",
    "        if question_idx == MAX_ITERATIONS-1:\n",
    "            n_iterations_taken.append(MAX_ITERATIONS)\n",
    "\n",
    "    end = time.time()\n",
    "    time_taken.append(end-start)\n",
    "\n",
    "print(\"Avg. # iterations:\", np.mean(n_iterations_taken))\n",
    "print(\"Std. # iterations:\", np.std(n_iterations_taken))\n",
    "print(\"Avg. time (sec.):\", np.mean(time_taken))\n",
    "print(\"Std. time (sec.):\", np.std(time_taken))\n",
    "print(\"Avg. runtime per iteration (sec.):\", np.mean([t / i for i, t in zip(n_iterations_taken, time_taken)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8313dcaa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# + DPO Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e402724",
   "metadata": {},
   "outputs": [],
   "source": [
    "VQG_ADAPTER_PATH = \"path/to/trained/adapter/directory\"\n",
    "lm.load_adapter(VQG_ADAPTER_PATH, adapter_name=\"vqg\")\n",
    "print(\"Loaded VQG adapter at\", VQG_ADAPTER_PATH)\n",
    "print(lm.active_adapters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e113e308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Modify below values to tuned values for each experiment\n",
    "EARLY_STOP_DELTA = 0.1\n",
    "CONFIDENT_RANGE = 0.1\n",
    "\n",
    "n_iterations_taken = []\n",
    "time_taken = []\n",
    "for batch_idx, batch_example in tqdm(enumerate(dataset.get_batches(1, \n",
    "                                                                    n_workers=1, \n",
    "                                                                    worker_index=0,\n",
    "                                                                    load_frames=False)), \n",
    "                                                desc=\"running iterative VQA inference\"):\n",
    "\n",
    "    batch_examples = [batch_example]\n",
    "    start = time.time()\n",
    "\n",
    "    # Take first frame (expect there to only be one frame)\n",
    "    batch_procedures = [example.procedure_description for example in batch_examples]\n",
    "    batch_frames = [Image.open(example.frames[0]) for example in batch_examples]\n",
    "\n",
    "    this_batch_size = len(batch_examples)\n",
    "\n",
    "    prompts = [\n",
    "        f'{DIALOG_START_TOKENS[VLM_NAME]}{USER_START_TOKENS[VLM_NAME]}{IMAGE_TOKENS[VLM_NAME]}{IVQA_PREAMBLE.format(procedure=procedure)}' \n",
    "        for procedure in batch_procedures\n",
    "    ]\n",
    "    questions = [[] for _ in range(this_batch_size)]\n",
    "    frames = [[] for _ in range(this_batch_size)]\n",
    "    candidate_questions = [[] for _ in range(this_batch_size)]\n",
    "    candidate_questions_scores = [[] for _ in range(this_batch_size)]\n",
    "    candidate_questions_sources = [[] for _ in range(this_batch_size)]\n",
    "    scores = [[] for _ in range(this_batch_size)]\n",
    "    answer_probs = [[] for _ in range(this_batch_size)] \n",
    "    answers = [[] for _ in range(this_batch_size)]\n",
    "    success_probs = [[] for _ in range(this_batch_size)]\n",
    "    success_probs_negated = [[] for _ in range(this_batch_size)]\n",
    "\n",
    "    # Iteratively generate questions\n",
    "    for question_idx in tqdm(range(MAX_ITERATIONS), desc=\"running iterative QA\"):\n",
    "\n",
    "        # If we have an adapter available for VQG, enable it (this should only be used for the dialog-based VQG, not in-context learning)\n",
    "        lm.enable_adapters()\n",
    "\n",
    "        # Generate a question (with beam search so we have several candidates)\n",
    "        prompts_q = [prompt + f\"{ASSISTANT_END_TOKENS[VLM_NAME] if question_idx != 0 else USER_END_TOKENS[VLM_NAME]}{USER_START_TOKENS[VLM_NAME]}Q:\" for prompt in prompts]\n",
    "        new_questions, _ = simple_lm_prompt_beam_search(lm,\n",
    "                                                        tokenizer,\n",
    "                                                        [prompt.replace(IMAGE_TOKENS[VLM_NAME], \"\") for prompt in prompts_q],\n",
    "                                                        max_new_tokens=20,\n",
    "                                                        batch_size=1,\n",
    "                                                        generation_kwargs=generation_kwargs)\n",
    "\n",
    "        new_questions = [[cleanup_generated_question(question) for question in beam_search_questions] for beam_search_questions in new_questions]                                \n",
    "        new_questions_sources = [[\"vlm\"] * len(beam_search_questions) for beam_search_questions in new_questions]\n",
    "\n",
    "        lm.disable_adapters()\n",
    "\n",
    "        # Optionally inject more candidates from original VQG ICL code\n",
    "        icl_prompts = [generate_vqg_prompt_icl(procedure, N_ICL_DEMONSTRATIONS, include_answers=False) for procedure in batch_procedures] # Create ICL prompt\n",
    "        icl_prompts = [\n",
    "            prompt + '\\n'.join([str(pqi+1) + ' ' + pq for pqi, pq in enumerate(previous_questions[-2:])]) + (\"\\n\" if len(previous_questions) > 0 else \"\") + f\"{len(previous_questions) + 1}. \" \n",
    "            for prompt, previous_questions in zip(icl_prompts, questions)\n",
    "        ] # Add some previous questions if possible (take last 2 that were asked)\n",
    "        icl_new_questions, _ = simple_lm_prompt_beam_search(lm,\n",
    "                                                            tokenizer,\n",
    "                                                            icl_prompts,\n",
    "                                                            max_new_tokens=20,\n",
    "                                                            batch_size=1,\n",
    "                                                            generation_kwargs=generation_kwargs)\n",
    "        \n",
    "        icl_new_questions = [[cleanup_generated_question(question) for question in beam_search_questions] for beam_search_questions in icl_new_questions]\n",
    "        \n",
    "        for batch_sub_idx in range(this_batch_size):\n",
    "            new_questions[batch_sub_idx] += icl_new_questions[batch_sub_idx]\n",
    "            new_questions_sources[batch_sub_idx] += [\"icl\"] * len(icl_new_questions[batch_sub_idx])\n",
    "\n",
    "        # Remove duplicate candidates\n",
    "        keep_idxs = [[question_idx for question_idx, question in enumerate(beam_search_outputs) if question not in beam_search_outputs[:question_idx]] for beam_search_outputs in new_questions]\n",
    "\n",
    "        # Try to remove any candidates that we've seen before (if we've seen all the candidates before, don't remove any)\n",
    "        keep_idxs_filtered = [[question_idx for question_idx, question in enumerate(beam_search_outputs) if question_idx in keep_idxs[batch_sub_idx] and question not in questions[batch_sub_idx]] for batch_sub_idx, beam_search_outputs in enumerate(new_questions)]\n",
    "        keep_idxs = [keep_idxs_filtered[batch_sub_idx] if len(keep_idxs_filtered[batch_sub_idx]) > 0 else keep_idxs[batch_sub_idx] for batch_sub_idx in range(this_batch_size)]\n",
    "\n",
    "        # Apply kept indices to new questions and their sources\n",
    "        new_questions = [[new_questions[batch_sub_idx][question_idx] for question_idx in this_keep_idxs] for batch_sub_idx, this_keep_idxs in enumerate(keep_idxs)]\n",
    "        new_questions_sources = [[new_questions_sources[batch_sub_idx][question_idx] for question_idx in this_keep_idxs] for batch_sub_idx, this_keep_idxs in enumerate(keep_idxs)]\n",
    "\n",
    "        # Save all candidates from beam search\n",
    "        for batch_sub_idx in range(len(candidate_questions)):\n",
    "            candidate_questions[batch_sub_idx].append(new_questions[batch_sub_idx])\n",
    "            candidate_questions_sources[batch_sub_idx].append(new_questions_sources[batch_sub_idx])\n",
    "\n",
    "        # Select best candidate question from pool\n",
    "        # Calculate coherence metrics for each candidate question\n",
    "        nli_outputs = question_coherence_metrics_nli(\n",
    "            nli_tokenizer, \n",
    "            nli_model,\n",
    "            tokenizer,\n",
    "            lm,\n",
    "            [procedure for procedure, beam_search_questions in zip(batch_procedures, new_questions) for _ in beam_search_questions],\n",
    "            [question for beam_search_questions in new_questions for question in beam_search_questions],\n",
    "            previous_questions=[[q for qi, q in enumerate(batch_idx_questions) if batch_idx_answers[qi] != \"Unsure\"] for batch_idx_questions, batch_idx_answers, beam_search_questions in zip(questions, answers, new_questions) for _ in beam_search_questions],\n",
    "            previous_answers=[[a for a in batch_idx_answers if a != \"Unsure\"] for batch_idx_answers, beam_search_questions in zip(answers, new_questions) for _ in beam_search_questions],\n",
    "            rephrase_batch_size=10\n",
    "        )\n",
    "\n",
    "        # Select best candidate based on coherence metrics\n",
    "        selected_questions = []\n",
    "        new_scores = []\n",
    "        parallel_idx = 0\n",
    "        ranking_key_mapping = {\n",
    "            \"relevance\": \"relevance_marginal\",\n",
    "            \"informativeness\": \"informativeness_marginal\",\n",
    "            \"coherence\": \"informativeness_marginal_x_relevance_marginal\",\n",
    "        }\n",
    "        for batch_sub_idx, beam_search_questions in enumerate(new_questions):\n",
    "            this_nli_outputs = [{k: round(float(nli_outputs[k][i]), 3) if type(nli_outputs[k][i]) != str else nli_outputs[k][i] for k in nli_outputs} for i in range(parallel_idx, parallel_idx + len(beam_search_questions))]\n",
    "            candidate_questions_scores[batch_sub_idx].append(this_nli_outputs)\n",
    "            parallel_idx += len(beam_search_questions)\n",
    "\n",
    "            # Use marginal relevance (consistency) and expected informativeness (verifiability) to rank candidates\n",
    "            candidate_scores = np.array(\n",
    "                [candidate_metrics[ranking_key_mapping[\"coherence\"]] for candidate_metrics in this_nli_outputs]\n",
    "            )\n",
    "\n",
    "            best_candidate = np.argmax(candidate_scores)\n",
    "            selected_questions.append(beam_search_questions[best_candidate])\n",
    "            new_scores.append(round(float(candidate_scores[best_candidate]), 6))\n",
    "        \n",
    "        new_questions = selected_questions\n",
    "                \n",
    "        # Save scores for best questions\n",
    "        for batch_sub_idx in range(this_batch_size):\n",
    "            scores[batch_sub_idx].append(new_scores[batch_sub_idx])\n",
    "\n",
    "        # Save generated questions\n",
    "        for batch_sub_idx in range(this_batch_size):\n",
    "            questions[batch_sub_idx].append(new_questions[batch_sub_idx])\n",
    "\n",
    "        # Run VQA with generated questions (and optional spatial filter)\n",
    "        prompts_a = [prompt + f' {question}{USER_END_TOKENS[VLM_NAME]}{ASSISTANT_START_TOKENS[VLM_NAME]}A:' for prompt, question in zip(prompts_q, new_questions)]\n",
    "\n",
    "        # Effective prompt for VQA depends on whether we want to exclude dialog history from prompt\n",
    "        use_prompts_a = [f'{USER_START_TOKENS[VLM_NAME]}{IMAGE_TOKENS[VLM_NAME]}Q: {question}{USER_END_TOKENS[VLM_NAME]}{ASSISTANT_START_TOKENS[VLM_NAME]}A:' for prompt, question in zip(prompts_q, new_questions)]\n",
    "\n",
    "        new_answers_logits = run_vqa_with_visual_filter(vlm_processor=vlm_processor, \n",
    "                                                        vlm=vlm, \n",
    "                                                        batch_examples=batch_examples, \n",
    "                                                        batch_frames=batch_frames, \n",
    "                                                        prompts_a=use_prompts_a, \n",
    "                                                        new_questions=new_questions, \n",
    "                                                        question_idx=question_idx,\n",
    "                                                        batch_size=1,\n",
    "                                                        visual_filter=None,\n",
    "                                                        nlp=NotImplemented,\n",
    "                                                        visual_filter_mode=None,\n",
    "                                                        frame_cache_dir=None,\n",
    "                                                        is_encoder_decoder=\"-t5-\" in VLM_NAME.lower())\n",
    "\n",
    "        # Gather up VQA outputs (which automatically calculates answer probabilities from logits)\n",
    "        new_answers = [\n",
    "            VQAOutputs(\n",
    "                task_name=MistakeDetectionTasks(\"ego4d_single\"),\n",
    "                example_id=example.example_id,\n",
    "                procedure_id=example.procedure_id,\n",
    "                frame=example.frames[0],\n",
    "                prompt=prompt,\n",
    "                expected_answer=None,\n",
    "                response_token_ids=response_token_ids,\n",
    "                logits=logits,\n",
    "                question=question,\n",
    "            ) for logits, example, prompt, question in zip(new_answers_logits, batch_examples, prompts_a, new_questions)\n",
    "        ]\n",
    "        new_answers_str = [output.predicted_answer.name if np.abs(output.answer_probs[VQAResponse.Yes] - 0.5) >= UNSURE_RANGE else \"Unsure\" for output in new_answers]\n",
    "\n",
    "        # Save answers and their probabilities\n",
    "        for batch_sub_idx in range(this_batch_size):\n",
    "            answer_probs[batch_sub_idx].append([round(float(new_answers[batch_sub_idx].answer_probs[VQAResponse(answer_idx)]), 6) for answer_idx in range(2)])\n",
    "            answers[batch_sub_idx].append(new_answers_str[batch_sub_idx])\n",
    "        \n",
    "        \n",
    "        prompts = [prompt + \" \" + output for prompt, output in zip(prompts_a, new_answers_str)]\n",
    "\n",
    "        # Ask VLM probability of success\n",
    "        questions_success = [\n",
    "            IVQA_SUCCESS_QUESTION.format(procedure=procedure)\n",
    "            for procedure in batch_procedures\n",
    "        ]\n",
    "        prompts_success = [\n",
    "            prompt + f'{ASSISTANT_END_TOKENS[VLM_NAME]}{USER_START_TOKENS[VLM_NAME]}Q: {question}{USER_END_TOKENS[VLM_NAME]}{ASSISTANT_START_TOKENS[VLM_NAME]}A: '\n",
    "            for prompt, question in zip(prompts, questions_success)\n",
    "        ]\n",
    "\n",
    "        success_vqa_outputs = run_vqa_with_visual_filter(vlm_processor=vlm_processor, \n",
    "                                                            vlm=vlm, \n",
    "                                                            batch_examples=batch_examples, \n",
    "                                                            batch_frames=batch_frames, \n",
    "                                                            prompts_a=prompts_success, \n",
    "                                                            new_questions=questions_success, \n",
    "                                                            question_idx=f\"{question_idx}_success\",\n",
    "                                                            batch_size=1,\n",
    "                                                            visual_filter=None,\n",
    "                                                            nlp=None,\n",
    "                                                            visual_filter_mode=None,\n",
    "                                                            frame_cache_dir=None,\n",
    "                                                            is_encoder_decoder=\"-t5-\" in VLM_NAME.lower(),\n",
    "                                                            ignore_frames=False)\n",
    "        success_vqa_outputs = [\n",
    "            VQAOutputs(\n",
    "                task_name=MistakeDetectionTasks(\"ego4d_single\"),\n",
    "                example_id=example.example_id,\n",
    "                procedure_id=example.procedure_id,\n",
    "                frame=example.frames[0],\n",
    "                prompt=prompt,\n",
    "                expected_answer=None,\n",
    "                response_token_ids=response_token_ids,\n",
    "                logits=logits,\n",
    "                question=question,\n",
    "            ) for logits, example, prompt, question in zip(success_vqa_outputs, batch_examples, prompts_a, new_questions)\n",
    "        ]               \n",
    "\n",
    "        # Save success probability for this turn\n",
    "        for batch_sub_idx in range(this_batch_size):\n",
    "            success_probs[batch_sub_idx].append(\n",
    "                round(float(success_vqa_outputs[batch_sub_idx].answer_probs[VQAResponse.Yes]), 6)\n",
    "            )\n",
    "\n",
    "        # Clear out VQA outputs now because they occupy a lot of memory\n",
    "        del new_answers\n",
    "        del success_vqa_outputs\n",
    "\n",
    "        # Check if we can stop based on early stopping criteria\n",
    "        # if success score doesn't change enough over 3 turns, stop incorporating questions\n",
    "        # (we still run inference across all questions for efficiency and simplicity, but later can make a proper demo script)\n",
    "        if question_idx >= 2:\n",
    "            if np.abs(success_probs[0][question_idx-1] - success_probs[0][question_idx-2]) < EARLY_STOP_DELTA and np.abs(success_probs[0][question_idx] - success_probs[0][question_idx-1]) < EARLY_STOP_DELTA:\n",
    "                n_iterations_taken.append(question_idx+1)\n",
    "                print(\"Early stop!\")\n",
    "                break\n",
    "        # OR if success score is within confident delta, stop\n",
    "        if success_probs[0][-1] < CONFIDENT_RANGE or 1.0 - success_probs[0][-1] < CONFIDENT_RANGE:\n",
    "            n_iterations_taken.append(question_idx+1)\n",
    "            print(\"Early stop!\")\n",
    "            break\n",
    "        # If it's the last iteration, record\n",
    "        if question_idx == MAX_ITERATIONS-1:\n",
    "            n_iterations_taken.append(MAX_ITERATIONS)\n",
    "\n",
    "    end = time.time()\n",
    "    time_taken.append(end-start)\n",
    "\n",
    "print(\"Avg. # iterations:\", np.mean(n_iterations_taken))\n",
    "print(\"Std. # iterations:\", np.std(n_iterations_taken))\n",
    "print(\"Avg. time (sec.):\", np.mean(time_taken))\n",
    "print(\"Std. time (sec.):\", np.std(time_taken))\n",
    "print(\"Avg. runtime per iteration (sec.):\", np.mean([t / i for i, t in zip(n_iterations_taken, time_taken)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68924293-116f-472e-abc6-185d01ca5210",
   "metadata": {},
   "source": [
    "# DPO Adapter Without ICL and Coherence Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52c23e6-169f-43d7-bb8b-9403fd28e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "VQG_ADAPTER_PATH = \"/path/to/trained/adapter/directory\"\n",
    "lm.load_adapter(VQG_ADAPTER_PATH, adapter_name=\"vqg\")\n",
    "print(\"Loaded VQG adapter at\", VQG_ADAPTER_PATH)\n",
    "print(lm.active_adapters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cdeada-2657-40b2-8d13-5d96b28f5204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Modify below values to tuned values for each experiment\n",
    "EARLY_STOP_DELTA = 0.1\n",
    "CONFIDENT_RANGE = 0.1\n",
    "\n",
    "n_iterations_taken = []\n",
    "time_taken = []\n",
    "for batch_idx, batch_example in tqdm(enumerate(dataset.get_batches(1, \n",
    "                                                                    n_workers=1, \n",
    "                                                                    worker_index=0,\n",
    "                                                                    load_frames=False)), \n",
    "                                                desc=\"running iterative VQA inference\"):\n",
    "\n",
    "    batch_examples = [batch_example]\n",
    "    start = time.time()\n",
    "\n",
    "    # Take first frame (expect there to only be one frame)\n",
    "    batch_procedures = [example.procedure_description for example in batch_examples]\n",
    "    batch_frames = [Image.open(example.frames[0]) for example in batch_examples]\n",
    "\n",
    "    this_batch_size = len(batch_examples)\n",
    "\n",
    "    prompts = [\n",
    "        f'{DIALOG_START_TOKENS[VLM_NAME]}{USER_START_TOKENS[VLM_NAME]}{IMAGE_TOKENS[VLM_NAME]}{IVQA_PREAMBLE.format(procedure=procedure)}' \n",
    "        for procedure in batch_procedures\n",
    "    ]\n",
    "    questions = [[] for _ in range(this_batch_size)]\n",
    "    frames = [[] for _ in range(this_batch_size)]\n",
    "    candidate_questions = [[] for _ in range(this_batch_size)]\n",
    "    candidate_questions_scores = [[] for _ in range(this_batch_size)]\n",
    "    candidate_questions_sources = [[] for _ in range(this_batch_size)]\n",
    "    scores = [[] for _ in range(this_batch_size)]\n",
    "    answer_probs = [[] for _ in range(this_batch_size)] \n",
    "    answers = [[] for _ in range(this_batch_size)]\n",
    "    success_probs = [[] for _ in range(this_batch_size)]\n",
    "    success_probs_negated = [[] for _ in range(this_batch_size)]\n",
    "\n",
    "    # Iteratively generate questions\n",
    "    for question_idx in tqdm(range(MAX_ITERATIONS), desc=\"running iterative QA\"):\n",
    "        \n",
    "        lm.enable_adapters()\n",
    "        \n",
    "        # Generate a question (with beam search so we have several candidates)\n",
    "        prompts_q = [prompt + f\"{ASSISTANT_END_TOKENS[VLM_NAME] if question_idx != 0 else USER_END_TOKENS[VLM_NAME]}{USER_START_TOKENS[VLM_NAME]}Q:\" for prompt in prompts]\n",
    "        new_questions, _ = simple_lm_prompt_beam_search(lm,\n",
    "                                                        tokenizer,\n",
    "                                                        [prompt.replace(IMAGE_TOKENS[VLM_NAME], \"\") for prompt in prompts_q],\n",
    "                                                        max_new_tokens=20,\n",
    "                                                        batch_size=1,\n",
    "                                                        generation_kwargs=generation_kwargs)\n",
    "\n",
    "        new_questions = [[cleanup_generated_question(question) for question in beam_search_questions] for beam_search_questions in new_questions]                                \n",
    "        new_questions_sources = [[\"vlm\"] * len(beam_search_questions) for beam_search_questions in new_questions]\n",
    "\n",
    "        lm.disable_adapters()\n",
    "        \n",
    "        # Remove duplicate candidates\n",
    "        keep_idxs = [[question_idx for question_idx, question in enumerate(beam_search_outputs) if question not in beam_search_outputs[:question_idx]] for beam_search_outputs in new_questions]\n",
    "\n",
    "        # Try to remove any candidates that we've seen before (if we've seen all the candidates before, don't remove any)\n",
    "        keep_idxs_filtered = [[question_idx for question_idx, question in enumerate(beam_search_outputs) if question_idx in keep_idxs[batch_sub_idx] and question not in questions[batch_sub_idx]] for batch_sub_idx, beam_search_outputs in enumerate(new_questions)]\n",
    "        keep_idxs = [keep_idxs_filtered[batch_sub_idx] if len(keep_idxs_filtered[batch_sub_idx]) > 0 else keep_idxs[batch_sub_idx] for batch_sub_idx in range(this_batch_size)]\n",
    "\n",
    "        # Apply kept indices to new questions and their sources\n",
    "        new_questions = [[new_questions[batch_sub_idx][question_idx] for question_idx in this_keep_idxs] for batch_sub_idx, this_keep_idxs in enumerate(keep_idxs)]\n",
    "        new_questions_sources = [[new_questions_sources[batch_sub_idx][question_idx] for question_idx in this_keep_idxs] for batch_sub_idx, this_keep_idxs in enumerate(keep_idxs)]\n",
    "\n",
    "        # Save all candidates from beam search\n",
    "        for batch_sub_idx in range(len(candidate_questions)):\n",
    "            candidate_questions[batch_sub_idx].append(new_questions[batch_sub_idx])\n",
    "            candidate_questions_sources[batch_sub_idx].append(new_questions_sources[batch_sub_idx])\n",
    "\n",
    "        # Select best candidate question from pool\n",
    "        generation_scores = compute_completion_log_likelihoods(lm, tokenizer, [prompt.replace(IMAGE_TOKENS[VLM_NAME], \"\") for prompt in prompts_q], new_questions, batch_size=1)\n",
    "\n",
    "        # Select most likely question (first one in list)\n",
    "        selected_questions = []\n",
    "        new_scores = []\n",
    "        for batch_sub_idx, (beam_search_questions, beam_search_scores) in enumerate(zip(new_questions, generation_scores)):                    \n",
    "            assert len(beam_search_questions) == len(beam_search_scores), \"Expected candidate questions and their scores to have the same shape!\"\n",
    "\n",
    "            # Save all candidate scores\n",
    "            candidate_questions_scores[batch_sub_idx].append(beam_search_scores)\n",
    "\n",
    "            candidate_idxs = list(range(len(beam_search_questions)))\n",
    "\n",
    "            # Then pick candidate with highest score\n",
    "            best_candidate = max(candidate_idxs, key=lambda x: beam_search_scores[x] == max(beam_search_scores))\n",
    "            selected_questions.append(beam_search_questions[best_candidate])\n",
    "            new_scores.append(beam_search_scores[best_candidate])\n",
    "\n",
    "        new_questions = selected_questions\n",
    "\n",
    "        # Save scores for best questions\n",
    "        for batch_sub_idx in range(this_batch_size):\n",
    "            scores[batch_sub_idx].append(new_scores[batch_sub_idx])\n",
    "\n",
    "        # Save generated questions\n",
    "        for batch_sub_idx in range(this_batch_size):\n",
    "            questions[batch_sub_idx].append(new_questions[batch_sub_idx])\n",
    "\n",
    "        # Run VQA with generated questions (and optional spatial filter)\n",
    "        prompts_a = [prompt + f' {question}{USER_END_TOKENS[VLM_NAME]}{ASSISTANT_START_TOKENS[VLM_NAME]}A:' for prompt, question in zip(prompts_q, new_questions)]\n",
    "\n",
    "        # Effective prompt for VQA depends on whether we want to exclude dialog history from prompt\n",
    "        use_prompts_a = [f'{USER_START_TOKENS[VLM_NAME]}{IMAGE_TOKENS[VLM_NAME]}Q: {question}{USER_END_TOKENS[VLM_NAME]}{ASSISTANT_START_TOKENS[VLM_NAME]}A:' for prompt, question in zip(prompts_q, new_questions)]\n",
    "\n",
    "        new_answers_logits = run_vqa_with_visual_filter(vlm_processor=vlm_processor, \n",
    "                                                        vlm=vlm, \n",
    "                                                        batch_examples=batch_examples, \n",
    "                                                        batch_frames=batch_frames, \n",
    "                                                        prompts_a=use_prompts_a, \n",
    "                                                        new_questions=new_questions, \n",
    "                                                        question_idx=question_idx,\n",
    "                                                        batch_size=1,\n",
    "                                                        visual_filter=None,\n",
    "                                                        nlp=NotImplemented,\n",
    "                                                        visual_filter_mode=None,\n",
    "                                                        frame_cache_dir=None,\n",
    "                                                        is_encoder_decoder=\"-t5-\" in VLM_NAME.lower())\n",
    "\n",
    "        # Gather up VQA outputs (which automatically calculates answer probabilities from logits)\n",
    "        new_answers = [\n",
    "            VQAOutputs(\n",
    "                task_name=MistakeDetectionTasks(\"ego4d_single\"),\n",
    "                example_id=example.example_id,\n",
    "                procedure_id=example.procedure_id,\n",
    "                frame=example.frames[0],\n",
    "                prompt=prompt,\n",
    "                expected_answer=None,\n",
    "                response_token_ids=response_token_ids,\n",
    "                logits=logits,\n",
    "                question=question,\n",
    "            ) for logits, example, prompt, question in zip(new_answers_logits, batch_examples, prompts_a, new_questions)\n",
    "        ]\n",
    "        new_answers_str = [output.predicted_answer.name if np.abs(output.answer_probs[VQAResponse.Yes] - 0.5) >= UNSURE_RANGE else \"Unsure\" for output in new_answers]\n",
    "\n",
    "        # Save answers and their probabilities\n",
    "        for batch_sub_idx in range(this_batch_size):\n",
    "            answer_probs[batch_sub_idx].append([round(float(new_answers[batch_sub_idx].answer_probs[VQAResponse(answer_idx)]), 6) for answer_idx in range(2)])\n",
    "            answers[batch_sub_idx].append(new_answers_str[batch_sub_idx])\n",
    "        \n",
    "        \n",
    "        prompts = [prompt + \" \" + output for prompt, output in zip(prompts_a, new_answers_str)]\n",
    "\n",
    "        # Ask VLM probability of success\n",
    "        questions_success = [\n",
    "            IVQA_SUCCESS_QUESTION.format(procedure=procedure)\n",
    "            for procedure in batch_procedures\n",
    "        ]\n",
    "        prompts_success = [\n",
    "            prompt + f'{ASSISTANT_END_TOKENS[VLM_NAME]}{USER_START_TOKENS[VLM_NAME]}Q: {question}{USER_END_TOKENS[VLM_NAME]}{ASSISTANT_START_TOKENS[VLM_NAME]}A: '\n",
    "            for prompt, question in zip(prompts, questions_success)\n",
    "        ]\n",
    "\n",
    "        success_vqa_outputs = run_vqa_with_visual_filter(vlm_processor=vlm_processor, \n",
    "                                                            vlm=vlm, \n",
    "                                                            batch_examples=batch_examples, \n",
    "                                                            batch_frames=batch_frames, \n",
    "                                                            prompts_a=prompts_success, \n",
    "                                                            new_questions=questions_success, \n",
    "                                                            question_idx=f\"{question_idx}_success\",\n",
    "                                                            batch_size=1,\n",
    "                                                            visual_filter=None,\n",
    "                                                            nlp=None,\n",
    "                                                            visual_filter_mode=None,\n",
    "                                                            frame_cache_dir=None,\n",
    "                                                            is_encoder_decoder=\"-t5-\" in VLM_NAME.lower(),\n",
    "                                                            ignore_frames=False)\n",
    "        success_vqa_outputs = [\n",
    "            VQAOutputs(\n",
    "                task_name=MistakeDetectionTasks(\"ego4d_single\"),\n",
    "                example_id=example.example_id,\n",
    "                procedure_id=example.procedure_id,\n",
    "                frame=example.frames[0],\n",
    "                prompt=prompt,\n",
    "                expected_answer=None,\n",
    "                response_token_ids=response_token_ids,\n",
    "                logits=logits,\n",
    "                question=question,\n",
    "            ) for logits, example, prompt, question in zip(success_vqa_outputs, batch_examples, prompts_a, new_questions)\n",
    "        ]               \n",
    "\n",
    "        # Save success probability for this turn\n",
    "        for batch_sub_idx in range(this_batch_size):\n",
    "            success_probs[batch_sub_idx].append(\n",
    "                round(float(success_vqa_outputs[batch_sub_idx].answer_probs[VQAResponse.Yes]), 6)\n",
    "            )\n",
    "\n",
    "        # Clear out VQA outputs now because they occupy a lot of memory\n",
    "        del new_answers\n",
    "        del success_vqa_outputs\n",
    "\n",
    "        # Check if we can stop based on early stopping criteria\n",
    "        # if success score doesn't change enough over 3 turns, stop incorporating questions\n",
    "        # (we still run inference across all questions for efficiency and simplicity, but later can make a proper demo script)\n",
    "        if question_idx >= 2:\n",
    "            if np.abs(success_probs[0][question_idx-1] - success_probs[0][question_idx-2]) < EARLY_STOP_DELTA and np.abs(success_probs[0][question_idx] - success_probs[0][question_idx-1]) < EARLY_STOP_DELTA:\n",
    "                n_iterations_taken.append(question_idx+1)\n",
    "                print(\"Early stop!\")\n",
    "                break\n",
    "        # OR if success score is within confident delta, stop\n",
    "        if success_probs[0][-1] < CONFIDENT_RANGE or 1.0 - success_probs[0][-1] < CONFIDENT_RANGE:\n",
    "            n_iterations_taken.append(question_idx+1)\n",
    "            print(\"Early stop!\")\n",
    "            break\n",
    "        # If it's the last iteration, record\n",
    "        if question_idx == MAX_ITERATIONS-1:\n",
    "            n_iterations_taken.append(MAX_ITERATIONS)\n",
    "\n",
    "    end = time.time()\n",
    "    time_taken.append(end-start)\n",
    "\n",
    "print(\"Avg. # iterations:\", np.mean(n_iterations_taken))\n",
    "print(\"Std. # iterations:\", np.std(n_iterations_taken))\n",
    "print(\"Avg. time (sec.):\", np.mean(time_taken))\n",
    "print(\"Std. time (sec.):\", np.std(time_taken))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel",
   "language": "python",
   "name": "travel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
