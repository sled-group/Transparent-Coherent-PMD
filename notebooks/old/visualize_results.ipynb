{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609598b0-4653-4b42-85ba-2a38a01e4300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sstorks/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir(\"/nfs/turbo/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl\")\n",
    "from travel import init_travel\n",
    "init_travel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dfe59f",
   "metadata": {},
   "source": [
    "# Load preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26f0084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "RESULTS_PATH = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222/vqa_mistake_detection/ego4d_debug500/llava-1.5-7b-hf/VQG2VQA_ego4d_debug500_llava-1.5-7b-hf_spatial_norephrase1.0_20240713191244/preds_nli_val.json\" # with NLI correction (v2)\n",
    "# RESULTS_PATH = \"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/saved_results_222/vqa_mistake_detection/ego4d_debug500/llava-1.5-7b-hf/VQG2VQA_ego4d_debug500_llava-1.5-7b-hf_spatial_norephrase1.0_20240713091801/preds_nli_val.json\" # without NLI correction\n",
    "preds = json.load(open(RESULTS_PATH, \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7126031",
   "metadata": {},
   "source": [
    "# Visualize some outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890002a2-b551-42d0-bb92-91c4ce86232a",
   "metadata": {},
   "source": [
    "Randomly choose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81273356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices to visualize: [398, 55, 120, 145, 156, 16, 14, 246, 114, 227, 378, 368, 225, 231, 38, 396, 8, 52, 479, 214]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "output_indices_to_visualize = random.sample(list(range(len(preds))), 20)\n",
    "\n",
    "print(\"Indices to visualize:\", output_indices_to_visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6662176e-232c-49d2-9263-9fb840344edc",
   "metadata": {},
   "source": [
    "Or choose based on conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "509f5da5-8490-44e8-aeac-cab8946c1c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 28041.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 105 high-confidence incorrect examples to look at.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from travel.data.ego4d import Ego4DMistakeDetectionDataset\n",
    "from travel.data.mistake_detection import MistakeDetectionExample, get_cutoff_time_by_proportion\n",
    "from travel.model.vqa import VQAResponse\n",
    "from travel.model.mistake_detection import aggregate_mistake_probs_over_frames\n",
    "from travel.model.mistake_detection import DETECTION_FRAMES_PROPORTION\n",
    "\n",
    "output_indices_to_visualize = []\n",
    "for pred_idx, pred in enumerate(tqdm(preds.values())):\n",
    "    frame_times = pred['example']['frame_times']\n",
    "    cutoff_time = get_cutoff_time_by_proportion(frame_times, DETECTION_FRAMES_PROPORTION)\n",
    "    frame_times = [t for t in frame_times if t >= cutoff_time]\n",
    "    mistake_prob = aggregate_mistake_probs_over_frames(pred['mistake_detection'][\"0.0\"]['mistake_probs'], frame_times)\n",
    "    \n",
    "    predicted_mistake = mistake_prob >= 0.5\n",
    "    conf = mistake_prob if predicted_mistake else 1 - mistake_prob\n",
    "    correct = pred['example']['mistake'] == predicted_mistake\n",
    "    \n",
    "    if not correct and conf >= 0.95:\n",
    "        output_indices_to_visualize.append(pred_idx)\n",
    "        \n",
    "print(f\"Identified {len(output_indices_to_visualize)} high-confidence incorrect examples to look at.\")\n",
    "    \n",
    "random.shuffle(output_indices_to_visualize)\n",
    "output_indices_to_visualize = output_indices_to_visualize[:20] # randomly pick 20 to look at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec345ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "\n",
    "from travel.data.ego4d import Ego4DMistakeDetectionDataset\n",
    "from travel.data.mistake_detection import MistakeDetectionExample, get_cutoff_time_by_proportion\n",
    "from travel.model.vqa import VQAResponse\n",
    "from travel.model.mistake_detection import aggregate_mistake_probs_over_frames\n",
    "from travel.model.mistake_detection import DETECTION_FRAMES_PROPORTION\n",
    "\n",
    "show_frames = True\n",
    "\n",
    "# Gather up example dirs by example ID\n",
    "dataset_cache_dir = Ego4DMistakeDetectionDataset.get_cache_dir(\"val\",\n",
    "                                                         mismatch_augmentation=True,\n",
    "                                                         multi_frame=True,\n",
    "                                                         debug_n_examples_per_class=500)\n",
    "dataset_example_dirs = json.load(open(os.path.join(dataset_cache_dir, \"dataset.json\"), \"r\"))[\"example_dirs\"]\n",
    "dataset_example_dirs = {\"/\".join(d.split(\"/\")[-3:]): d for d in dataset_example_dirs}\n",
    "\n",
    "keys = list(preds.keys())\n",
    "for output_idx in output_indices_to_visualize:\n",
    "    print(f\"Displaying output index {output_idx}\\n\")\n",
    "    pred = preds[keys[output_idx]]\n",
    "\n",
    "    procedure_description = pred['example']['procedure_description']\n",
    "    mistake = pred[\"example\"][\"mistake\"]\n",
    "        \n",
    "    example_id = pred['example']['example_id']\n",
    "    example = Ego4DMistakeDetectionDataset.load_example_from_file(dataset_example_dirs[example_id])\n",
    "    example.cutoff_to_last_frames(DETECTION_FRAMES_PROPORTION)\n",
    "    pprint(example)\n",
    "    assert len(example.frames) == len(pred['vqa'])\n",
    "\n",
    "    # Extract the vqa list - just take the questions and answers for last frame\n",
    "    # Create a figure with a subplot for each entry\n",
    "    num_entries = len(pred['vqa'])\n",
    "    num_questions = len(pred['vqa'][0])\n",
    "    if show_frames:\n",
    "        fig, axs = plt.subplots(num_entries, num_questions, figsize=(10 * num_questions, 5 * num_entries))\n",
    "    print(f\"{len(pred['vqa'])} frames\")\n",
    "    for frame_idx in range(len(pred['vqa'])):\n",
    "        vqa_list = pred['vqa'][frame_idx]\n",
    "\n",
    "        for i, entry in enumerate(vqa_list):\n",
    "            if show_frames:\n",
    "                ax = axs[frame_idx, i] if num_questions > 1 else axs[frame_idx]\n",
    "                img = Image.open(entry[\"frame\"]) if \"frame\" in entry and entry[\"frame\"] != \"\" else example.frames[frame_idx]\n",
    "                ax.imshow(img)\n",
    "                ax.axis('off')\n",
    "\n",
    "            question = entry['question']\n",
    "            prompt = entry['prompt'].split(\"USER: <image>\")[1].split(\"ASSISTANT:\")[0].strip()\n",
    "            predicted_answer = entry['predicted_answer']\n",
    "            expected_answer = entry['expected_answer']\n",
    "\n",
    "            title = f\"Procedure: {procedure_description} ({'mistake' if mistake else 'success'}: {pred['example']['mistake_type']})\\n\"\n",
    "            title += f\"Q: {question} ({round(example.frame_times[frame_idx], 1)} sec.)\\n\"\n",
    "            title += f\"Transformed Q: {prompt}\\n\"\n",
    "            if i < len(pred['mistake_detection']['0.0']['mistake_probs'][frame_idx]):\n",
    "                mistake_prob = round(pred['mistake_detection']['0.0']['mistake_probs'][frame_idx][i], 3)\n",
    "            else:\n",
    "                mistake_prob = 1.0\n",
    "            title += f\"Predicted Answer: {VQAResponse(predicted_answer).name}, Expected Answer: {VQAResponse(expected_answer).name} ({mistake_prob if mistake_prob == 1.0 or predicted_answer != expected_answer else round(1 - mistake_prob, 3)})\"\n",
    "\n",
    "            if show_frames:\n",
    "                ax.set_title(title)\n",
    "\n",
    "    if show_frames:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    print(\"VQA outputs:\")\n",
    "    for frame_idx in range(len(pred['vqa'])):\n",
    "        for entry in pred['vqa'][frame_idx]:\n",
    "            print(entry[\"question\"])\n",
    "            pprint(entry[\"answer_probs\"])\n",
    "        print(\"\")\n",
    "        \n",
    "    # Print out probabilities\n",
    "    for k in [\"mistake_probs\", \"nli_relevance_probs\", \"nli_mistake_probs\", \"nli_final_mistake_probs\"]:\n",
    "        print(k + \":\")\n",
    "        pprint(pred['mistake_detection'][\"0.0\"][k])\n",
    "        \n",
    "    frame_times = pred['example']['frame_times']\n",
    "    cutoff_time = get_cutoff_time_by_proportion(frame_times, DETECTION_FRAMES_PROPORTION)\n",
    "    frame_times = [t for t in frame_times if t >= cutoff_time]\n",
    "\n",
    "    print(\"\\nMISTAKE CONFIDENCE AGGREGATION:\")\n",
    "    mistake_prob = aggregate_mistake_probs_over_frames(pred['mistake_detection'][\"0.0\"]['mistake_probs'], frame_times, verbose=True)\n",
    "    # print(mistake_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c36bf05-eb0c-4d72-8eaa-a82555219131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:33<00:00, 14.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped frames: 2436/4972 (0.4899436846339501)\n",
      "Masked frames: 1120/4972 (0.2252614641995173)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "from travel.data.ego4d import Ego4DMistakeDetectionDataset\n",
    "from travel.data.mistake_detection import MistakeDetectionExample, get_cutoff_time_by_proportion\n",
    "from travel.model.vqa import VQAResponse\n",
    "from travel.model.mistake_detection import aggregate_mistake_probs_over_frames\n",
    "from travel.model.mistake_detection import DETECTION_FRAMES_PROPORTION\n",
    "\n",
    "show_frames = True\n",
    "\n",
    "# Gather up example dirs by example ID\n",
    "dataset_cache_dir = Ego4DMistakeDetectionDataset.get_cache_dir(\"val\",\n",
    "                                                         mismatch_augmentation=True,\n",
    "                                                         multi_frame=True,\n",
    "                                                         debug_n_examples_per_class=250)\n",
    "dataset_example_dirs = json.load(open(os.path.join(dataset_cache_dir, \"dataset.json\"), \"r\"))[\"example_dirs\"]\n",
    "dataset_example_dirs = {\"/\".join(d.split(\"/\")[-3:]): d for d in dataset_example_dirs}\n",
    "\n",
    "keys = list(preds.keys())\n",
    "\n",
    "n_cropped_frames = 0\n",
    "n_masked_frames = 0\n",
    "n_total_frames = 0\n",
    "for pred in tqdm(preds.values()):\n",
    "    procedure_description = pred['example']['procedure_description']\n",
    "    mistake = pred[\"example\"][\"mistake\"]\n",
    "        \n",
    "    example_id = pred['example']['example_id']\n",
    "    example = Ego4DMistakeDetectionDataset.load_example_from_file(dataset_example_dirs[example_id])\n",
    "    example.cutoff_to_last_frames(DETECTION_FRAMES_PROPORTION)\n",
    "    assert len(example.frames) == len(pred['vqa'])\n",
    "\n",
    "    for frame_idx in range(len(pred['vqa'])):\n",
    "        vqa_list = pred['vqa'][frame_idx]\n",
    "\n",
    "        for i, entry in enumerate(vqa_list):\n",
    "            img = Image.open(entry[\"frame\"]) if \"frame\" in entry and entry[\"frame\"] != \"\" else example.frames[frame_idx]\n",
    "                        \n",
    "            n_total_frames += 1\n",
    "            \n",
    "            # Check the image size to see if it has been cropped by a visual filter\n",
    "            if img.size != (400, 300):\n",
    "                n_cropped_frames += 1\n",
    "                \n",
    "            # Count number of black pixels to detect whether masking has been applied by a visual filter\n",
    "            black_pixels = np.asarray(img)\n",
    "            black_pixels = np.reshape(black_pixels, (-1, 3))\n",
    "            black_pixels = np.sum(black_pixels, axis=-1)\n",
    "            n_pixels = len(black_pixels)\n",
    "            n_black_pixels = n_pixels - np.count_nonzero(black_pixels)\n",
    "            \n",
    "            if n_black_pixels > 50:\n",
    "                n_masked_frames += 1\n",
    "            \n",
    "print(f\"Cropped frames: {n_cropped_frames}/{n_total_frames} ({n_cropped_frames / n_total_frames})\")\n",
    "print(f\"Masked frames: {n_masked_frames}/{n_total_frames} ({n_masked_frames / n_total_frames})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda3375-67eb-406d-ba3a-61443d7bd7b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel",
   "language": "python",
   "name": "travel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
