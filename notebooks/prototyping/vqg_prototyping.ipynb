{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609598b0-4653-4b42-85ba-2a38a01e4300",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from travel import init_travel\n",
    "init_travel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58246e2",
   "metadata": {},
   "source": [
    "# Test VQG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe63f90",
   "metadata": {},
   "source": [
    "Load LM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dd008ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up LM(s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sstorks/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3897208ab43488da93e009bb73a2d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, BitsAndBytesConfig\n",
    "from travel.constants import HF_TOKEN\n",
    "\n",
    "# Load LM(s)\n",
    "print(\"Setting up LM(s)...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "model_kwargs = {\"quantization_config\": bnb_config}\n",
    "lm = pipeline(\"text-generation\", \n",
    "            model=\"meta-llama/Llama-2-7b-hf\", \n",
    "            token=HF_TOKEN,\n",
    "            model_kwargs=model_kwargs)\n",
    "lm.tokenizer.padding_side = \"left\"\n",
    "lm.tokenizer.pad_token_id = lm.model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "04b51d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from travel.data.vqg import N_GENERATED_QUESTIONS, VQGOutputs\n",
    "from travel.data.vqg import VQG_DEMONSTRATIONS\n",
    "import random\n",
    "\n",
    "VQG_PROMPT_TEMPLATE = 'The instructions say \"{instruction_step}\". To visually verify that this procedure is complete, what are {n_questions} questions we could ask about an image of the scene and their expected answers?\\n'\n",
    "VQG_EXAMPLE_TEMPLATE = VQG_PROMPT_TEMPLATE + \\\n",
    "                       \"{question_list}\"\n",
    "VQG_QUESTION_TEMPLATE = \"{question_number}. {question} (yes/no) {answer}\"\n",
    "\n",
    "def generate_vqg_prompt(instruction_step: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns a prompt for VQG, i.e., for zero-shot inference or to come after several in-context demonstrations.\n",
    "\n",
    "    :param instruction_step: Recipe or instruction step to generate instructions for. Should usually be a sentence in imperative form.\n",
    "    :return: String including a prompt to generate `n_questions` questions to verify the success of `instruction_step`.\n",
    "    \"\"\"\n",
    "    return VQG_PROMPT_TEMPLATE.format(instruction_step=instruction_step,\n",
    "                                      n_questions=str(N_GENERATED_QUESTIONS))\n",
    "\n",
    "def generate_vqg_example(vqg_output: VQGOutputs) -> str:\n",
    "    \"\"\"\n",
    "    Returns a full VQG prompt example for in-context learning.\n",
    "\n",
    "    :param vqg_output: VQGOutputs object for in-context VQG example.\n",
    "    :return: String including a full demonstration of a prompt and several questions and expected answers for generating visual verification questions.\n",
    "    \"\"\"\n",
    "    return VQG_EXAMPLE_TEMPLATE.format(instruction_step=vqg_output.procedure_description,\n",
    "                                       n_questions = len(vqg_output.questions),\n",
    "                                       question_list=\"\\n\".join([VQG_QUESTION_TEMPLATE.format(\n",
    "                                            question_number=question_idx + 1,\n",
    "                                            question=question,\n",
    "                                            answer=answer.name\n",
    "                                       ) for question_idx, (question, answer) in enumerate(zip(vqg_output.questions, vqg_output.answers))]))\n",
    "\n",
    "def generate_vqg_prompt_icl(procedure_description: str, n_demonstrations: int=3) -> str:\n",
    "    \"\"\"\n",
    "    Returns a prompt for VQG including in-context demonstrations.\n",
    "\n",
    "    :param procedure_description: String description of a procedure (e.g., recipe step) to generate visual questions for.\n",
    "    :param n_demonstrations: Number of in-context demonstrations to include from `VQG_DEMONSTRATIONS`.\n",
    "    :return: Prompt for VQG including in-context demonstrations.\n",
    "    \"\"\"\n",
    "    assert n_demonstrations <= len(VQG_DEMONSTRATIONS), f\"Requested {n_demonstrations} in-context demonstrations for VQG, but only {len(VQG_DEMONSTRATIONS)} are available in travel.model.vqg.VQG_DEMONSTRATIONS.\"\n",
    "    demonstrations = VQG_DEMONSTRATIONS[:n_demonstrations]\n",
    "    random.shuffle(demonstrations) # Shuffle demonstrations for each prompt to ensure the ordering is not sub-optimal\n",
    "    examples = [generate_vqg_example(demo) for demo in demonstrations]\n",
    "    examples += [generate_vqg_prompt(procedure_description)]\n",
    "    return \"\\n\\n\".join(examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "10bdb4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '1. Are the onions in the bowl? (yes/no) No\\n2. Are there any onions that are not in the bowl? (yes/no) Yes\\n\\nThe instructions say \"Put the water in the kettle\". To visually verify that this procedure is complete, what are 2 questions we could ask about an image of the scene and their expected answers?\\n1. Is the kettle full? (yes/no) Yes\\n2. Is the kettle empty? (yes/no) No\\n\\nThe instructions say \"Pour the water into the k'}]\n"
     ]
    }
   ],
   "source": [
    "lm.model.generation_config.temperature = 0.4\n",
    "lm.model.generation_config.do_sample = True\n",
    "lm.model.generation_config.top_p = 0.9\n",
    "\n",
    "# step = \"In a bowl, add the cut cherry tomatoes\"\n",
    "# step = \"Chop the onions on the cutting board with a knife\"\n",
    "# step = \"Pour the tomatoes from the bowl into the cup\"\n",
    "step = \"Take the onions from the bowl\"\n",
    "# step = \"Dump out the cup of water into the sink\"\n",
    "\n",
    "prompt = generate_vqg_prompt_icl(step, n_demonstrations=20)\n",
    "\n",
    "vqg_output = lm(prompt, max_new_tokens=128, return_full_text=False)\n",
    "\n",
    "print(vqg_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d55527-c56d-4298-8764-8255a79c9631",
   "metadata": {},
   "source": [
    "# Test self-reflection step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f05f4bcb-c1bf-44f8-a567-d0e6db7d8de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32000])\n",
      "{<VQAResponse.No: 0>: 0.61878043, <VQAResponse.Yes: 1>: 0.38121957}\n"
     ]
    }
   ],
   "source": [
    "from travel.data.vqa import get_vqa_response_token_ids, VQAOutputs, VQAResponse\n",
    "import torch\n",
    "\n",
    "lm.model.generation_config.temperature = None\n",
    "lm.model.generation_config.do_sample = False\n",
    "lm.model.generation_config.top_p = None\n",
    "\n",
    "VQG_REFLECTION_TEMPLATE_TEMPLATE = 'Question: The instructions say \"{instruction_step}\". After this procedure is complete, {question}? (yes/no) Answer:'\n",
    "\n",
    "step = \"Take the onions from the bowl\"\n",
    "# question = \"Is the onion in the bowl?\"\n",
    "question = \"Is the onion in someone's hand?\"\n",
    "\n",
    "prompt = VQG_REFLECTION_TEMPLATE_TEMPLATE.format(instruction_step=step, question=question[0].lower() + question[1:])\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = lm.tokenizer(prompt, return_tensors=\"pt\")\n",
    "    logits = lm.model(**inputs).logits[0, -1, :]\n",
    "    print(logits.shape)\n",
    "    response_token_ids = get_vqa_response_token_ids(lm.tokenizer)\n",
    "    this_probs = torch.stack([logits[response_token_ids[response_type]] for response_type in VQAResponse], dim=0)\n",
    "    this_probs = torch.softmax(this_probs, dim=0)\n",
    "\n",
    "predicted_answer = VQAResponse(torch.argmax(this_probs, dim=0).numpy())\n",
    "\n",
    "this_probs = this_probs.numpy()\n",
    "answer_probs = {response_type: this_probs[response_type.value] for response_type in VQAResponse}\n",
    "\n",
    "print(answer_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9368e3-bd7f-40d5-acfe-f53f31ac9c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7403eb-9529-4b6c-b40b-4fa7723ba26c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel",
   "language": "python",
   "name": "travel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
