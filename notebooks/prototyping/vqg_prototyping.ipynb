{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "609598b0-4653-4b42-85ba-2a38a01e4300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir(\"/nfs/turbo/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl\")\n",
    "from travel import init_travel\n",
    "init_travel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58246e2",
   "metadata": {},
   "source": [
    "# Test VQG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe63f90",
   "metadata": {},
   "source": [
    "Load LM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dd008ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sstorks/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up LM(s)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e59f9e4d00e42ab82f6eb223c60a6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, BitsAndBytesConfig\n",
    "from travel.constants import HF_TOKEN\n",
    "\n",
    "# Load LM(s)\n",
    "print(\"Setting up LM(s)...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "model_kwargs = {\"quantization_config\": bnb_config}\n",
    "lm = pipeline(\"text-generation\", \n",
    "            model=\"meta-llama/Llama-2-7b-hf\", \n",
    "            token=HF_TOKEN,\n",
    "            model_kwargs=model_kwargs)\n",
    "lm.tokenizer.padding_side = \"left\"\n",
    "lm.tokenizer.pad_token_id = lm.model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe64114-fad3-4468-af13-241559da868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from transformers import PhrasalConstraint, DisjunctiveConstraint\n",
    "from travel.data.vqa import get_vqa_response_token_ids, VQAResponse\n",
    "\n",
    "# kwargs to force question generations to have a \"?\" and start with \"Is\" or \"Are\"\n",
    "question_generation_constraints = [    \n",
    "    PhrasalConstraint(\n",
    "        [vlm_processor.tokenizer(\"Is it blue?\", add_special_tokens=False).input_ids[-1]]\n",
    "    ),\n",
    "]\n",
    "yes_no_q_tokens = [\n",
    "    vlm_processor.tokenizer(\"Is it blue?\", add_special_tokens=False).input_ids[0], \n",
    "    vlm_processor.tokenizer(\"Are they blue?\", add_special_tokens=False).input_ids[0],\n",
    "    vlm_processor.tokenizer(\"Does it look blue?\", add_special_tokens=False).input_ids[0],\n",
    "    vlm_processor.tokenizer(\"Do they look blue?\", add_special_tokens=False).input_ids[0],\n",
    "]\n",
    "begin_suppress_tokens = [t for t in list(range(vlm_processor.tokenizer.vocab_size)) if t not in yes_no_q_tokens]\n",
    "question_generation_kwargs = {\n",
    "    \"constraints\": question_generation_constraints,\n",
    "    \"begin_suppress_tokens\": begin_suppress_tokens,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10bdb4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '1. Are there cherry tomatoes in the bowl? Yes\\n'\n",
      "                    '2. Are there any cherry tomatoes that are not in the '\n",
      "                    'bowl? No\\n'\n",
      "                    '\\n'\n",
      "                    'The instructions say \"Put the pizza in the oven\". To '\n",
      "                    'visually verify that this procedure is complete, what are '\n",
      "                    '2 questions we'},\n",
      " {'generated_text': '1. Are the tomatoes in the bowl? Yes\\n'\n",
      "                    '2. Are there any tomatoes that are not in the bowl? No\\n'\n",
      "                    '\\n'\n",
      "                    'The instructions say \"Put the pizza in the oven\". To '\n",
      "                    'visually verify that this procedure is complete, what are '\n",
      "                    '2 questions we could ask about an'},\n",
      " {'generated_text': '1. Is there a bowl? Yes\\n'\n",
      "                    '2. Is there a cut cherry tomato in the bowl? Yes\\n'\n",
      "                    '\\n'\n",
      "                    'The instructions say \"Put the pizza in the oven\". To '\n",
      "                    'visually verify that this procedure is complete, what are '\n",
      "                    '2 questions we could ask about an image of the'},\n",
      " {'generated_text': '1. Are the cherry tomatoes in a bowl? Yes\\n'\n",
      "                    \"2. Is the bowl in someone's hand? Yes\\n\"\n",
      "                    '\\n'\n",
      "                    'The instructions say \"Put the pizza in the oven\". To '\n",
      "                    'visually verify that this procedure is complete, what are '\n",
      "                    '2 questions we could ask about an image'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from travel.data.vqg import generate_vqg_prompt_icl\n",
    "\n",
    "lm.model.generation_config.do_sample = False\n",
    "lm.model.generation_config.num_beams = 4\n",
    "lm.model.generation_config.num_beam_groups = 4\n",
    "lm.model.generation_config.diversity_penalty = 1.0\n",
    "lm.model.generation_config.num_return_sequences = 4\n",
    "\n",
    "step = \"In a bowl, add the cut cherry tomatoes\"\n",
    "# step = \"Chop the onions on the cutting board with a knife\"\n",
    "# step = \"Pour the tomatoes from the bowl into the cup\"\n",
    "# step = \"Take the onions from the bowl\"\n",
    "# step = \"Dump out the cup of water into the sink\"\n",
    "\n",
    "prompt = generate_vqg_prompt_icl(step, n_demonstrations=20)\n",
    "\n",
    "vqg_output = lm(prompt, max_new_tokens=64, return_full_text=False)\n",
    "\n",
    "pprint(vqg_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bd91992-2579-4675-9565-cd4a0473e405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 13.73it/s]\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 14.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQGOutputs(procedure_id=0,\n",
      "           procedure_description='In a bowl, add the cut cherry tomatoes',\n",
      "           questions=['Are there cherry tomatoes in the bowl?',\n",
      "                      'Are there any cherry tomatoes that are not in the '\n",
      "                      'bowl?'],\n",
      "           answers_str=['Yes', 'No'],\n",
      "           answers=[<VQAResponse.Yes: 1>, <VQAResponse.No: 0>],\n",
      "           target_object=None)\n",
      "{'consistency': [0.889, 0.398],\n",
      " 'informativeness': [0.961, 0.636],\n",
      " 'relevance': [0.924, 0.626]}\n",
      "\n",
      "VQGOutputs(procedure_id=1,\n",
      "           procedure_description='In a bowl, add the cut cherry tomatoes',\n",
      "           questions=['Are the tomatoes in the bowl?',\n",
      "                      'Are there any tomatoes that are not in the bowl?'],\n",
      "           answers_str=['Yes', 'No'],\n",
      "           answers=[<VQAResponse.Yes: 1>, <VQAResponse.No: 0>],\n",
      "           target_object=None)\n",
      "{'consistency': [0.876, 0.397],\n",
      " 'informativeness': [0.982, 0.642],\n",
      " 'relevance': [0.892, 0.62]}\n",
      "\n",
      "VQGOutputs(procedure_id=2,\n",
      "           procedure_description='In a bowl, add the cut cherry tomatoes',\n",
      "           questions=['Is there a bowl?',\n",
      "                      'Is there a cut cherry tomato in the bowl?'],\n",
      "           answers_str=['Yes', 'Yes'],\n",
      "           answers=[<VQAResponse.Yes: 1>, <VQAResponse.Yes: 1>],\n",
      "           target_object=None)\n",
      "{'consistency': [0.281, 0.92],\n",
      " 'informativeness': [0.542, 0.969],\n",
      " 'relevance': [0.518, 0.949]}\n",
      "\n",
      "VQGOutputs(procedure_id=3,\n",
      "           procedure_description='In a bowl, add the cut cherry tomatoes',\n",
      "           questions=['Are the cherry tomatoes in a bowl?',\n",
      "                      \"Is the bowl in someone's hand?\"],\n",
      "           answers_str=['Yes', 'Yes'],\n",
      "           answers=[<VQAResponse.Yes: 1>, <VQAResponse.Yes: 1>],\n",
      "           target_object=None)\n",
      "{'consistency': [0.929, 0.412],\n",
      " 'informativeness': [0.99, 0.657],\n",
      " 'relevance': [0.939, 0.627]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from travel.data.vqg import VQGOutputs, parse_vqg_outputs\n",
    "from travel.model.metrics import consistency_metrics_vqg\n",
    "\n",
    "all_vqg_outputs = {si: parse_vqg_outputs(s['generated_text'], si, step) for si, s in enumerate(vqg_output)}\n",
    "metrics = consistency_metrics_vqg(all_vqg_outputs)\n",
    "\n",
    "for output in all_vqg_outputs.values():\n",
    "    pprint(output)\n",
    "    pprint(metrics['metrics_by_output'][output.procedure_id])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d55527-c56d-4298-8764-8255a79c9631",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Test self-reflection step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f05f4bcb-c1bf-44f8-a567-d0e6db7d8de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32000])\n",
      "{<VQAResponse.No: 0>: 0.61878043, <VQAResponse.Yes: 1>: 0.38121957}\n"
     ]
    }
   ],
   "source": [
    "from travel.data.vqa import get_vqa_response_token_ids, VQAOutputs, VQAResponse\n",
    "import torch\n",
    "\n",
    "lm.model.generation_config.temperature = None\n",
    "lm.model.generation_config.do_sample = False\n",
    "lm.model.generation_config.top_p = None\n",
    "\n",
    "VQG_REFLECTION_TEMPLATE_TEMPLATE = 'Question: The instructions say \"{instruction_step}\". After this procedure is complete, {question}? (yes/no) Answer:'\n",
    "\n",
    "step = \"Take the onions from the bowl\"\n",
    "# question = \"Is the onion in the bowl?\"\n",
    "question = \"Is the onion in someone's hand?\"\n",
    "\n",
    "prompt = VQG_REFLECTION_TEMPLATE_TEMPLATE.format(instruction_step=step, question=question[0].lower() + question[1:])\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = lm.tokenizer(prompt, return_tensors=\"pt\")\n",
    "    logits = lm.model(**inputs).logits[0, -1, :]\n",
    "    print(logits.shape)\n",
    "    response_token_ids = get_vqa_response_token_ids(lm.tokenizer)\n",
    "    this_probs = torch.stack([logits[response_token_ids[response_type]] for response_type in VQAResponse], dim=0)\n",
    "    this_probs = torch.softmax(this_probs, dim=0)\n",
    "\n",
    "predicted_answer = VQAResponse(torch.argmax(this_probs, dim=0).numpy())\n",
    "\n",
    "this_probs = this_probs.numpy()\n",
    "answer_probs = {response_type: this_probs[response_type.value] for response_type in VQAResponse}\n",
    "\n",
    "print(answer_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9368e3-bd7f-40d5-acfe-f53f31ac9c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7403eb-9529-4b6c-b40b-4fa7723ba26c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel",
   "language": "python",
   "name": "travel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
