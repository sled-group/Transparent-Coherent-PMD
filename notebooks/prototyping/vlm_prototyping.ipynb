{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir(\"/nfs/turbo/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl\")\n",
    "from travel import init_travel\n",
    "init_travel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sstorks/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/sstorks/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:104: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n",
      "  warnings.warn(\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b15efa1d6e54d23aa2cec4392e7dbb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "\n",
    "VLM_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "vlm = AutoModelForVision2Seq.from_pretrained(VLM_NAME, \n",
    "                                            quantization_config=bnb_config)\n",
    "visual_filter = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "vlm.generation_config.temperature = None\n",
    "vlm.generation_config.top_p = None\n",
    "vlm.generation_config.do_sample = False\n",
    "# vlm.generation_config.diversity_penalty = 1.0\n",
    "vlm.generation_config.num_beams = 4\n",
    "# vlm.generation_config.num_beam_groups = 1\n",
    "vlm.generation_config.num_return_sequences=4\n",
    "\n",
    "vlm.language_model.generation_config.temperature = None\n",
    "vlm.language_model.generation_config.top_p = None\n",
    "vlm.language_model.generation_config.do_sample = False\n",
    "# vlm.language_model.generation_config.diversity_penalty = 1.0\n",
    "vlm.language_model.generation_config.num_beams = 4\n",
    "# vlm.language_model.generation_config.num_beam_groups = 1\n",
    "vlm.language_model.generation_config.num_return_sequences=4\n",
    "\n",
    "vlm_processor = AutoProcessor.from_pretrained(VLM_NAME)\n",
    "vlm_processor.tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: set up visual filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import spacy\n",
    "import torch\n",
    "from travel.model.grounding import SpatialVisualFilter, ImageMaskTypes, ContrastiveRegionFilter\n",
    "\n",
    "# visual_filter = SpatialVisualFilter(rephrase_questions=False, mask_strength=40.0, mask_type=ImageMaskTypes.Blur, device=\"cuda:0\")\n",
    "visual_filter = ContrastiveRegionFilter(device=\"cuda:0\", mask_strength=100.0, mask_type=ImageMaskTypes.Blur)\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Run VQA with yes-no questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "from travel.data.vqa import VQG2VQA_PROMPT_TEMPLATES, get_vqa_response_token_ids\n",
    "\n",
    "# Kitchen tap examples\n",
    "# questions = [\"Do you see the tap in the image?\", \n",
    "#              \"Do you see the tap in the image?\", \n",
    "#              \"Is there lettuce on the stove?\",\n",
    "#              \"Look at the stove. Is there lettuce on the stove?\",\n",
    "#              \"Is the tap turned on?\",\n",
    "#             \"Are there cherry tomatoes in the bowl?\",\n",
    "#             \"Look in the bowl. Are there any cherry tomatoes in the bowl?\",\n",
    "#             \"Are there cherry tomatoes in the bowl?\",\n",
    "#             \"Look in the bowl. Are there any cherry tomatoes in the bowl?\",]\n",
    "# frames = [Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition4of4/1ab9d5f7-0181-458e-a5e7-72ce87501f3e/38/pos/frames/frame_1ab9d5f7-0181-458e-a5e7-72ce87501f3e-38-pos_0.jpg\"),\n",
    "#           Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition4of4/1ab9d5f7-0181-458e-a5e7-72ce87501f3e/38/pos/frames/frame_1ab9d5f7-0181-458e-a5e7-72ce87501f3e-38-pos_0.jpg\"),\n",
    "#           Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition4of4/1ab9d5f7-0181-458e-a5e7-72ce87501f3e/38/pos/frames/frame_1ab9d5f7-0181-458e-a5e7-72ce87501f3e-38-pos_0.jpg\"),\n",
    "#          Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition3of4/90093217-6a61-48ce-b737-e581499cf491/48/pos/frames/frame_90093217-6a61-48ce-b737-e581499cf491-48-pos_4.jpg\"),\n",
    "#          Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition3of4/90093217-6a61-48ce-b737-e581499cf491/48/pos/frames/frame_90093217-6a61-48ce-b737-e581499cf491-48-pos_4.jpg\"),\n",
    "#           Image.open(\"demo_images/demo_frame0.png\").convert(\"RGB\"),\n",
    "#           Image.open(\"demo_images/demo_frame0.png\").convert(\"RGB\"),\n",
    "#           Image.open(\"demo_images/demo_frame2.png\").convert(\"RGB\"),\n",
    "#           Image.open(\"demo_images/demo_frame2.png\").convert(\"RGB\"),\n",
    "#          ]\n",
    "\n",
    "# Glue tube and drawer examples\n",
    "# questions = [\n",
    "#     \"Is there a glue tube in the drawer?\",\n",
    "#     \"Is there a glue tube in someone's hand?\",\n",
    "#     \"Are the floorboards connected?\",\n",
    "#     \"Is there a hammer in someone's hand?\",\n",
    "#     \"Do you see a glue tube in the image?\",\n",
    "#     \"Do you see a drawer in the image?\",\n",
    "#     'The instructions say to \"hammer the floorboards\". Do you see someone working on this?',\n",
    "#     'The instructions say to \"take the glue tube from the drawer\". Do you see someone working on this?'\n",
    "# ]\n",
    "# frames = [\n",
    "#     Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition3of4/da32ff6e-27b2-47e6-b38f-dbf7a36b5f4d/70/easyneg_MisalignSRL_V_ARG1_898afb02-0bde-4fe7-81f8-645b94ac4899_4034.461679999999/frames/frame_da32ff6e-27b2-47e6-b38f-dbf7a36b5f4d-70-easyneg_MisalignSRL_V_ARG1_898afb02-0bde-4fe7-81f8-645b94ac4899_4034.461679999999_3.jpg\"),\n",
    "#     Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition3of4/da32ff6e-27b2-47e6-b38f-dbf7a36b5f4d/70/easyneg_MisalignSRL_V_ARG1_898afb02-0bde-4fe7-81f8-645b94ac4899_4034.461679999999/frames/frame_da32ff6e-27b2-47e6-b38f-dbf7a36b5f4d-70-easyneg_MisalignSRL_V_ARG1_898afb02-0bde-4fe7-81f8-645b94ac4899_4034.461679999999_3.jpg\"),\n",
    "#     Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition3of4/da32ff6e-27b2-47e6-b38f-dbf7a36b5f4d/70/easyneg_MisalignSRL_V_ARG1_898afb02-0bde-4fe7-81f8-645b94ac4899_4034.461679999999/frames/frame_da32ff6e-27b2-47e6-b38f-dbf7a36b5f4d-70-easyneg_MisalignSRL_V_ARG1_898afb02-0bde-4fe7-81f8-645b94ac4899_4034.461679999999_3.jpg\"),\n",
    "#     Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition3of4/da32ff6e-27b2-47e6-b38f-dbf7a36b5f4d/70/easyneg_MisalignSRL_V_ARG1_898afb02-0bde-4fe7-81f8-645b94ac4899_4034.461679999999/frames/frame_da32ff6e-27b2-47e6-b38f-dbf7a36b5f4d-70-easyneg_MisalignSRL_V_ARG1_898afb02-0bde-4fe7-81f8-645b94ac4899_4034.461679999999_3.jpg\"),\n",
    "#     Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition3of4/da32ff6e-27b2-47e6-b38f-dbf7a36b5f4d/70/easyneg_MisalignSRL_V_ARG1_898afb02-0bde-4fe7-81f8-645b94ac4899_4034.461679999999/frames/frame_da32ff6e-27b2-47e6-b38f-dbf7a36b5f4d-70-easyneg_MisalignSRL_V_ARG1_898afb02-0bde-4fe7-81f8-645b94ac4899_4034.461679999999_3.jpg\"),\n",
    "#     Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition3of4/da32ff6e-27b2-47e6-b38f-dbf7a36b5f4d/70/easyneg_MisalignSRL_V_ARG1_898afb02-0bde-4fe7-81f8-645b94ac4899_4034.461679999999/frames/frame_da32ff6e-27b2-47e6-b38f-dbf7a36b5f4d-70-easyneg_MisalignSRL_V_ARG1_898afb02-0bde-4fe7-81f8-645b94ac4899_4034.461679999999_3.jpg\"),\n",
    "#     Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition3of4/da32ff6e-27b2-47e6-b38f-dbf7a36b5f4d/70/easyneg_MisalignSRL_V_ARG1_898afb02-0bde-4fe7-81f8-645b94ac4899_4034.461679999999/frames/frame_da32ff6e-27b2-47e6-b38f-dbf7a36b5f4d-70-easyneg_MisalignSRL_V_ARG1_898afb02-0bde-4fe7-81f8-645b94ac4899_4034.461679999999_3.jpg\"),    \n",
    "#     Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition3of4/da32ff6e-27b2-47e6-b38f-dbf7a36b5f4d/70/easyneg_MisalignSRL_V_ARG1_898afb02-0bde-4fe7-81f8-645b94ac4899_4034.461679999999/frames/frame_da32ff6e-27b2-47e6-b38f-dbf7a36b5f4d-70-easyneg_MisalignSRL_V_ARG1_898afb02-0bde-4fe7-81f8-645b94ac4899_4034.461679999999_3.jpg\"),    \n",
    "# ]\n",
    "\n",
    "# # Thread and bag examples\n",
    "# questions = [\n",
    "#     \"Is the thread in someone's hand?\",\n",
    "#     \"Is the thread in the bag?\",\n",
    "#     \"Do you see a thread in the image?\",\n",
    "#     \"Do you see a bag in the image?\",\n",
    "#     'The instructions say to \"Pick up the thread\". Do you see someone working on this?',\n",
    "#     'The instructions say to \"Continue cutting the chicken\". Do you see someone working on this?',\n",
    "# ]\n",
    "# frames = [\n",
    "#     Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition1of4/79c62f24-488e-4a69-8220-3b20cb4bf72b/398/easyneg_MisalignSRL_V_ARG1_c8f8ebf1-5613-4f4a-802c-ada5e4c4b651_326.6214286/frames/frame_79c62f24-488e-4a69-8220-3b20cb4bf72b-398-easyneg_MisalignSRL_V_ARG1_c8f8ebf1-5613-4f4a-802c-ada5e4c4b651_326.6214286_3.jpg\"),\n",
    "#     Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition1of4/79c62f24-488e-4a69-8220-3b20cb4bf72b/398/easyneg_MisalignSRL_V_ARG1_c8f8ebf1-5613-4f4a-802c-ada5e4c4b651_326.6214286/frames/frame_79c62f24-488e-4a69-8220-3b20cb4bf72b-398-easyneg_MisalignSRL_V_ARG1_c8f8ebf1-5613-4f4a-802c-ada5e4c4b651_326.6214286_3.jpg\"),    \n",
    "#     Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition1of4/79c62f24-488e-4a69-8220-3b20cb4bf72b/398/easyneg_MisalignSRL_V_ARG1_c8f8ebf1-5613-4f4a-802c-ada5e4c4b651_326.6214286/frames/frame_79c62f24-488e-4a69-8220-3b20cb4bf72b-398-easyneg_MisalignSRL_V_ARG1_c8f8ebf1-5613-4f4a-802c-ada5e4c4b651_326.6214286_3.jpg\"),    \n",
    "#     Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition1of4/79c62f24-488e-4a69-8220-3b20cb4bf72b/398/easyneg_MisalignSRL_V_ARG1_c8f8ebf1-5613-4f4a-802c-ada5e4c4b651_326.6214286/frames/frame_79c62f24-488e-4a69-8220-3b20cb4bf72b-398-easyneg_MisalignSRL_V_ARG1_c8f8ebf1-5613-4f4a-802c-ada5e4c4b651_326.6214286_3.jpg\"),    \n",
    "#     Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition1of4/79c62f24-488e-4a69-8220-3b20cb4bf72b/398/easyneg_MisalignSRL_V_ARG1_c8f8ebf1-5613-4f4a-802c-ada5e4c4b651_326.6214286/frames/frame_79c62f24-488e-4a69-8220-3b20cb4bf72b-398-easyneg_MisalignSRL_V_ARG1_c8f8ebf1-5613-4f4a-802c-ada5e4c4b651_326.6214286_3.jpg\"),    \n",
    "#     Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition1of4/79c62f24-488e-4a69-8220-3b20cb4bf72b/398/easyneg_MisalignSRL_V_ARG1_c8f8ebf1-5613-4f4a-802c-ada5e4c4b651_326.6214286/frames/frame_79c62f24-488e-4a69-8220-3b20cb4bf72b-398-easyneg_MisalignSRL_V_ARG1_c8f8ebf1-5613-4f4a-802c-ada5e4c4b651_326.6214286_3.jpg\"),    \n",
    "# ]\n",
    "\n",
    "# Brush and wall examples\n",
    "questions = [\n",
    "    \"Is the brush on the wall?\",\n",
    "    \"Is the brush in someone's hand?\",\n",
    "    \"Do you see a brush in the image?\",\n",
    "    \"Do you see a wall in the image?\",\n",
    "    'The instructions say to \"Rub the brush on the wall\". Do you see someone working on this?',\n",
    "    'The instructions say to \"Rub the clippers on the towel\". Do you see someone working on this?',\n",
    "]\n",
    "frames = [\n",
    "    Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition1of4/5b97f47f-f015-46f3-8879-3fcc2a61a728/177/easyneg_MisalignSRL_ARG1_2dbdd409-0be0-447c-8ae8-3f107fe9af80_152.6393286/frames/frame_5b97f47f-f015-46f3-8879-3fcc2a61a728-177-easyneg_MisalignSRL_ARG1_2dbdd409-0be0-447c-8ae8-3f107fe9af80_152.6393286_1.jpg\"),\n",
    "    Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition1of4/5b97f47f-f015-46f3-8879-3fcc2a61a728/177/easyneg_MisalignSRL_ARG1_2dbdd409-0be0-447c-8ae8-3f107fe9af80_152.6393286/frames/frame_5b97f47f-f015-46f3-8879-3fcc2a61a728-177-easyneg_MisalignSRL_ARG1_2dbdd409-0be0-447c-8ae8-3f107fe9af80_152.6393286_1.jpg\"),\n",
    "    Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition1of4/5b97f47f-f015-46f3-8879-3fcc2a61a728/177/easyneg_MisalignSRL_ARG1_2dbdd409-0be0-447c-8ae8-3f107fe9af80_152.6393286/frames/frame_5b97f47f-f015-46f3-8879-3fcc2a61a728-177-easyneg_MisalignSRL_ARG1_2dbdd409-0be0-447c-8ae8-3f107fe9af80_152.6393286_1.jpg\"),\n",
    "    Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition1of4/5b97f47f-f015-46f3-8879-3fcc2a61a728/177/easyneg_MisalignSRL_ARG1_2dbdd409-0be0-447c-8ae8-3f107fe9af80_152.6393286/frames/frame_5b97f47f-f015-46f3-8879-3fcc2a61a728-177-easyneg_MisalignSRL_ARG1_2dbdd409-0be0-447c-8ae8-3f107fe9af80_152.6393286_1.jpg\"),\n",
    "    Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition1of4/5b97f47f-f015-46f3-8879-3fcc2a61a728/177/easyneg_MisalignSRL_ARG1_2dbdd409-0be0-447c-8ae8-3f107fe9af80_152.6393286/frames/frame_5b97f47f-f015-46f3-8879-3fcc2a61a728-177-easyneg_MisalignSRL_ARG1_2dbdd409-0be0-447c-8ae8-3f107fe9af80_152.6393286_1.jpg\"),\n",
    "    Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition1of4/5b97f47f-f015-46f3-8879-3fcc2a61a728/177/easyneg_MisalignSRL_ARG1_2dbdd409-0be0-447c-8ae8-3f107fe9af80_152.6393286/frames/frame_5b97f47f-f015-46f3-8879-3fcc2a61a728-177-easyneg_MisalignSRL_ARG1_2dbdd409-0be0-447c-8ae8-3f107fe9af80_152.6393286_1.jpg\"),\n",
    "]\n",
    "\n",
    "\n",
    "prompts = [VQG2VQA_PROMPT_TEMPLATES[type(vlm)].format(question=question) for question in questions]\n",
    "\n",
    "response_token_ids = get_vqa_response_token_ids(vlm_processor.tokenizer)\n",
    "\n",
    "frames_distorted = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: corrupt images with noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def add_diffusion_noise(image: Image.Image, noise_step: int) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Add diffusion noise to a PIL Image.\n",
    "    \n",
    "    Parameters:\n",
    "    - image (PIL.Image.Image): The input image to which noise will be added.\n",
    "    - noise_step (int): The diffusion step to determine the noise level.\n",
    "    \n",
    "    Returns:\n",
    "    - PIL.Image.Image: The image with added diffusion noise.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the image to a NumPy array and then to a PyTorch tensor\n",
    "    image_array = np.array(image).astype(np.float32) / 255.0\n",
    "    image_tensor = torch.tensor(image_array).permute(2, 0, 1).unsqueeze(0)  # Shape: (1, C, H, W)\n",
    "\n",
    "    num_steps = 1000  # Number of diffusion steps\n",
    "\n",
    "    # Decide beta in each step\n",
    "    betas = torch.linspace(-6, 6, num_steps)\n",
    "    betas = torch.sigmoid(betas) * (0.5e-2 - 1e-5) + 1e-5\n",
    "\n",
    "    # Decide alphas in each step\n",
    "    alphas = 1 - betas\n",
    "    alphas_prod = torch.cumprod(alphas, dim=0)\n",
    "    alphas_prod_p = torch.cat([torch.tensor([1.0]), alphas_prod[:-1]], 0)  # p for previous\n",
    "    alphas_bar_sqrt = torch.sqrt(alphas_prod)\n",
    "    one_minus_alphas_bar_sqrt = torch.sqrt(1 - alphas_prod)\n",
    "\n",
    "    def q_x(x_0, t):\n",
    "        noise = torch.randn_like(x_0)\n",
    "        alphas_t = alphas_bar_sqrt[t]\n",
    "        alphas_1_m_t = one_minus_alphas_bar_sqrt[t]\n",
    "        return alphas_t * x_0 + alphas_1_m_t * noise\n",
    "\n",
    "    # Apply noise at the specified noise step\n",
    "    noisy_image_tensor = q_x(image_tensor, noise_step).squeeze(0).permute(1, 2, 0)  # Shape: (H, W, C)\n",
    "\n",
    "    # Convert the noisy image tensor back to a NumPy array\n",
    "    noisy_image_array = (noisy_image_tensor.numpy() * 255.0).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "    # Convert the NumPy array back to a PIL Image\n",
    "    noisy_image = Image.fromarray(noisy_image_array)\n",
    "\n",
    "    return noisy_image\n",
    "\n",
    "def add_gaussian_noise(image: Image.Image, mean: float=0.0, std_dev: float=25.0) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to a PIL Image.\n",
    "    \n",
    "    Parameters:\n",
    "    - image (PIL.Image.Image): The input image to which noise will be added.\n",
    "    - mean (float): The mean of the Gaussian noise.\n",
    "    - std_dev (float): The standard deviation of the Gaussian noise.\n",
    "    \n",
    "    Returns:\n",
    "    - PIL.Image.Image: The image with added Gaussian noise.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the image to a NumPy array\n",
    "    image_array = np.array(image)\n",
    "    \n",
    "    # Generate Gaussian noise\n",
    "    noise = np.random.normal(mean, std_dev, image_array.shape)\n",
    "    \n",
    "    # Add the Gaussian noise to the image\n",
    "    noisy_image_array = image_array + noise\n",
    "    \n",
    "    # Clip the values to be in the valid range [0, 255] and convert to uint8\n",
    "    noisy_image_array = np.clip(noisy_image_array, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    # Convert the noisy image array back to a PIL Image\n",
    "    noisy_image = Image.fromarray(noisy_image_array)\n",
    "    \n",
    "    return noisy_image\n",
    "\n",
    "frames_distorted = [add_diffusion_noise(frame, noise_step=500) for frame in frames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run VQA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running VQA (cuda:0): 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "running VQA (cuda:0): 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from travel.data.mistake_detection import MistakeDetectionTasks\n",
    "from travel.data.vqa import VQAOutputs, VQAResponse\n",
    "from travel.model.vqa import run_vqa\n",
    "\n",
    "result = run_vqa(vlm, vlm_processor, prompts, frames, batch_size=10)\n",
    "result = [VQAOutputs(task_name=MistakeDetectionTasks.Ego4D, example_id=\"\", procedure_id=0, frame=\"\", prompt=\"\", expected_answer=VQAResponse.No, response_token_ids=response_token_ids, logits=result[i]) for i in range(result.shape[0])]\n",
    "\n",
    "if visual_filter is not None:\n",
    "    frames_filter = visual_filter(nlp, frames, questions)\n",
    "    result_filter = run_vqa(vlm, vlm_processor, prompts, frames_filter, batch_size=10)\n",
    "    result_filter = [VQAOutputs(task_name=MistakeDetectionTasks.Ego4D, example_id=\"\", procedure_id=0, frame=\"\", prompt=\"\", expected_answer=VQAResponse.No, response_token_ids=response_token_ids, logits=result_filter[i]) for i in range(result_filter.shape[0])]\n",
    "    \n",
    "if frames_distorted is not None:\n",
    "    result_distorted = run_vqa(vlm, vlm_processor, prompts, frames_distorted, batch_size=10)\n",
    "    result_distorted = [VQAOutputs(task_name=MistakeDetectionTasks.Ego4D, example_id=\"\", procedure_id=0, frame=\"\", prompt=\"\", expected_answer=VQAResponse.No, response_token_ids=response_token_ids, logits=result_distorted[i]) for i in range(result_distorted.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_vqg_results(frames, questions, result, result_distorted_filtered=None, combine_results_alpha=1.0):\n",
    "    # Assuming new_frames is a list of image data\n",
    "    fig, axarr = plt.subplots(1, len(frames), figsize=(22, 4))\n",
    "    fig.suptitle(\"FRAMES\")\n",
    "\n",
    "    # Ensure axarr is always iterable\n",
    "    if len(frames) == 1:\n",
    "        axarr = [axarr]\n",
    "\n",
    "    for frame, ax in zip(frames, axarr):\n",
    "        if frame is not None:\n",
    "            ax.imshow(frame)\n",
    "            ax.axis('off')  # Hide the axes for better visualization\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Vanilla VQA probs:\")\n",
    "    for q, r in zip(questions, result):\n",
    "        print(q)\n",
    "        pprint(r.answer_probs)\n",
    "        print(\"\")\n",
    "    \n",
    "    if result_distorted_filtered is not None:\n",
    "        print(\"\\nDistorted/filtered VQA probs:\")\n",
    "        for q, r in zip(questions, result_distorted_filtered):\n",
    "            print(q)\n",
    "            pprint(r.answer_probs)\n",
    "            print(\"\")\n",
    "            \n",
    "        print(\"\\nCombined VQA probs:\")\n",
    "        for q, r0, r1 in zip(questions, result, result_distorted_filtered):\n",
    "            logits_r0 = r0.logits\n",
    "            logits_r1 = r1.logits\n",
    "            \n",
    "            logits_combined = (1 + combine_results_alpha) * logits_r0 - combine_results_alpha * logits_r1\n",
    "            output_combined = VQAOutputs(task_name=MistakeDetectionTasks.Ego4D, example_id=\"\", procedure_id=0, frame=\"\", prompt=\"\", expected_answer=VQAResponse.No, response_token_ids=response_token_ids, logits=logits_combined)\n",
    "            \n",
    "            print(q)\n",
    "            pprint(output_combined.answer_probs)\n",
    "            print(\"\")\n",
    "    \n",
    "if visual_filter is not None:\n",
    "    result_distorted_filtered = result_filter\n",
    "    frames_to_display = frames_filter\n",
    "elif frames_distorted is not None:\n",
    "    result_distorted_filtered = result_distorted\n",
    "    frames_to_display = frames_distorted\n",
    "else:\n",
    "    result_distorted_filtered = None\n",
    "    frames_to_display = frames\n",
    "display_vqg_results(frames_to_display, questions, result, result_distorted_filtered=result_distorted_filtered, combine_results_alpha=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Run image captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "from travel.data.vqa import COMPLETION_PROMPT_TEMPLATES, get_vqa_response_token_ids\n",
    "\n",
    "frames = [\n",
    "    # Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition4of4/1ab9d5f7-0181-458e-a5e7-72ce87501f3e/38/pos/frames/frame_1ab9d5f7-0181-458e-a5e7-72ce87501f3e-38-pos_0.jpg\"),\n",
    "    Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition3of4/da32ff6e-27b2-47e6-b38f-dbf7a36b5f4d/70/easyneg_MisalignSRL_V_ARG1_898afb02-0bde-4fe7-81f8-645b94ac4899_4034.461679999999/frames/frame_da32ff6e-27b2-47e6-b38f-dbf7a36b5f4d-70-easyneg_MisalignSRL_V_ARG1_898afb02-0bde-4fe7-81f8-645b94ac4899_4034.461679999999_3.jpg\"),\n",
    "    Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition1of4/79c62f24-488e-4a69-8220-3b20cb4bf72b/398/easyneg_MisalignSRL_V_ARG1_c8f8ebf1-5613-4f4a-802c-ada5e4c4b651_326.6214286/frames/frame_79c62f24-488e-4a69-8220-3b20cb4bf72b-398-easyneg_MisalignSRL_V_ARG1_c8f8ebf1-5613-4f4a-802c-ada5e4c4b651_326.6214286_3.jpg\"),    \n",
    "    Image.open(\"/home/sstorks/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl_cache_dir/ego4d_val_seed222_mismatch_multiframe_partition1of4/5b97f47f-f015-46f3-8879-3fcc2a61a728/177/easyneg_MisalignSRL_ARG1_2dbdd409-0be0-447c-8ae8-3f107fe9af80_152.6393286/frames/frame_5b97f47f-f015-46f3-8879-3fcc2a61a728-177-easyneg_MisalignSRL_ARG1_2dbdd409-0be0-447c-8ae8-3f107fe9af80_152.6393286_1.jpg\"),\n",
    "]\n",
    "\n",
    "prompts = [COMPLETION_PROMPT_TEMPLATES[type(vlm)] for _ in frames]\n",
    "\n",
    "response_token_ids = get_vqa_response_token_ids(vlm_processor.tokenizer)\n",
    "\n",
    "candidate_procedures = [\n",
    "    # [\"Open up the avocado\", \"Open the cabinet\", \"Open the tap\"],\n",
    "    [\"Take the glue tube from the drawer\", \"Hammer the floorboard\"],\n",
    "    [\"Cut the meat\", \"Pick up the thread\"],\n",
    "    [\"Trim the nails\", \"Rub the clippers on the towel\", \"Rub the brush on the wall\", \"Paint the wall with the paint brush\"],\n",
    "]\n",
    "\n",
    "followup_questions = [\n",
    "    [\"Is there a glue tube in the image?\", \"Is the glue tube in the drawer?\", \"Is the glue tube in someone's hand?\", \"Are the floorboards connected?\", \"Is the person using a hammer?\"],\n",
    "    [\"Is there a thread in the image?\", \"Is the thread in someone's hand?\", \"Is the thread in the bag?\", \"Is the meat cut?\", \"Is there a knife in someone's hand?\"],\n",
    "    [\"Is there a brush in the image?\", \"Is the brush on the wall?\", \"Is the brush being held by someone?\", \"Are the clippers being held?\", \"Are the clippers touching the towel?\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = vlm_processor(text=prompts, images=frames, return_tensors=\"pt\", padding=True).to(vlm.device)\n",
    "\n",
    "generated_ids = vlm.generate(**inputs, max_length=256)\n",
    "captions = vlm_processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "# Assuming new_frames is a list of image data\n",
    "fig, axarr = plt.subplots(1, len(frames), figsize=(22, 4))\n",
    "fig.suptitle(\"FRAMES\")\n",
    "\n",
    "# Ensure axarr is always iterable\n",
    "if len(frames) == 1:\n",
    "    axarr = [axarr]\n",
    "\n",
    "for frame, ax in zip(frames, axarr):\n",
    "    if frame is not None:\n",
    "        ax.imshow(frame)\n",
    "        ax.axis('off')  # Hide the axes for better visualization\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for caption in captions:\n",
    "    print(caption.replace(\"ASSISTANT:\", \"\\nASSISTANT:\"))  \n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running VQA (cuda:0): 100%|██████████| 2/2 [00:05<00:00,  2.86s/it]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from travel.data.mistake_detection import MistakeDetectionTasks\n",
    "from travel.data.vqa import VQAOutputs, VQAResponse\n",
    "from travel.model.vqa import run_vqa\n",
    "\n",
    "prompts = [caption.replace(\"USER:\", \"USER: <image>\\n\") + \" \" + f\"USER: {question} ASSISTANT: \"  for prompt, caption, questions in zip(prompts, captions, followup_questions) for question in questions]\n",
    "frames = [frame for frame, procedures in zip(frames, followup_questions) for _ in procedures]\n",
    "\n",
    "result = run_vqa(vlm, vlm_processor, prompts, frames, batch_size=10)\n",
    "result = [VQAOutputs(task_name=MistakeDetectionTasks.Ego4D, example_id=\"\", procedure_id=0, frame=\"\", prompt=\"\", expected_answer=VQAResponse.No, response_token_ids=response_token_ids, logits=result[i]) for i in range(result.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "# Assuming new_frames is a list of image data\n",
    "fig, axarr = plt.subplots(1, len(frames), figsize=(22, 4))\n",
    "fig.suptitle(\"FRAMES\")\n",
    "\n",
    "# Ensure axarr is always iterable\n",
    "if len(frames) == 1:\n",
    "    axarr = [axarr]\n",
    "\n",
    "for frame, ax in zip(frames, axarr):\n",
    "    if frame is not None:\n",
    "        ax.imshow(frame)\n",
    "        ax.axis('off')  # Hide the axes for better visualization\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for p, r in zip(prompts, result):\n",
    "    print(p.replace(\"ASSISTANT:\", \"\\nASSISTANT:\"))  \n",
    "    pprint(r.answer_probs)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use NLI model to judge captions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sstorks/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, BitsAndBytesConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from travel.model.mistake_detection import NLI_MODEL_PATH\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(NLI_MODEL_PATH, quantization_config=bnb_config)\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(NLI_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "premises = [caption.split(\"ASSISTANT:\")[1].strip() for caption, procedures in zip(captions, candidate_procedures) for procedure in procedures]\n",
    "hypotheses = [f\"The person is going to {procedure[0].lower() + procedure[1:]}\" for caption, procedures in zip(captions, candidate_procedures) for procedure in procedures]\n",
    "\n",
    "x = nli_tokenizer.batch_encode_plus(list(zip(premises, hypotheses)), \n",
    "                                        return_tensors='pt',\n",
    "                                        padding=\"longest\",\n",
    "                                        truncation='only_first')\n",
    "logits = nli_model(**x.to(nli_model.device))[0]\n",
    "logits = logits.cpu()\n",
    "logits = logits[:,[0, 2]] # Take logits for contradiction and entailment only\n",
    "probs = logits.softmax(dim=1).detach().numpy()\n",
    "probs = probs[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p, h, prob in zip(premises, hypotheses, probs):\n",
    "    print(f\"Premise: {p}\")\n",
    "    print(f\"Hypothesis: {h}\")\n",
    "    print(f\"Probability (entailment): {prob}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-turn VQG2VQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up helper methods for prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from transformers import PhrasalConstraint, DisjunctiveConstraint\n",
    "from travel.data.vqa import get_vqa_response_token_ids, VQAResponse\n",
    "\n",
    "# kwargs to force question generations to have a \"?\" and start with \"Is\" or \"Are\"\n",
    "question_generation_constraints = [    \n",
    "    PhrasalConstraint(\n",
    "        [vlm_processor.tokenizer(\"Is it blue?\", add_special_tokens=False).input_ids[-1]]\n",
    "    ),\n",
    "]\n",
    "yes_no_q_tokens = [\n",
    "    vlm_processor.tokenizer(\"Is it blue?\", add_special_tokens=False).input_ids[0], \n",
    "    vlm_processor.tokenizer(\"Are they blue?\", add_special_tokens=False).input_ids[0],\n",
    "    vlm_processor.tokenizer(\"Does it look blue?\", add_special_tokens=False).input_ids[0],\n",
    "    vlm_processor.tokenizer(\"Do they look blue?\", add_special_tokens=False).input_ids[0],\n",
    "]\n",
    "begin_suppress_tokens = [t for t in list(range(vlm_processor.tokenizer.vocab_size)) if t not in yes_no_q_tokens]\n",
    "question_generation_kwargs = {\n",
    "    \"constraints\": question_generation_constraints,\n",
    "    \"begin_suppress_tokens\": begin_suppress_tokens,\n",
    "}\n",
    "# pprint(question_generation_kwargs)\n",
    "\n",
    "# TODO: make only \"is/are\" allowed for first token in generating questions?\n",
    "def simple_prompt(frame, prompt, max_new_tokens=20, avoid_str=[], generation_kwargs={}):\n",
    "   \n",
    "    inputs = vlm_processor(text=prompt, images=frame, padding=True, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(vlm.device)\n",
    "\n",
    "    outputs = vlm.generate(**inputs, max_new_tokens=max_new_tokens, return_dict_in_generate=True, output_scores=True, **generation_kwargs)\n",
    "    outputs = vlm_processor.batch_decode(outputs.sequences, skip_special_tokens=True)\n",
    "    \n",
    "    outputs = [output.replace(\"USER:  \", \"USER: <image>\") for output in outputs]\n",
    "    outputs = [output.replace(prompt, \"\") for output in outputs]\n",
    "    \n",
    "    pprint(outputs)\n",
    "    for output in outputs:\n",
    "        if avoid_str and any(s in output for s in avoid_str):\n",
    "            continue\n",
    "        return output\n",
    "    \n",
    "    if avoid_str:\n",
    "        print(\"Warning: returning avoid_str!\")\n",
    "    return outputs[0]\n",
    "\n",
    "def simple_prompt_textonly(prompt, max_new_tokens=20, avoid_str=[], generation_kwargs={}):\n",
    "   \n",
    "    inputs = vlm_processor.tokenizer(text=prompt, padding=True, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(vlm.device)\n",
    "\n",
    "    outputs = vlm.language_model.generate(**inputs, max_new_tokens=max_new_tokens, return_dict_in_generate=True, output_scores=True, **generation_kwargs)\n",
    "    outputs = vlm_processor.batch_decode(outputs.sequences, skip_special_tokens=True)\n",
    "    \n",
    "    outputs = [output.replace(prompt, \"\") for output in outputs]\n",
    "    \n",
    "    pprint(outputs)\n",
    "    for output in outputs:\n",
    "        if avoid_str and any(s in output for s in avoid_str):\n",
    "            continue\n",
    "        return output\n",
    "    \n",
    "    if avoid_str:\n",
    "        print(\"Warning: returning avoid_str!\")\n",
    "    return outputs[0]\n",
    "\n",
    "def yes_no_prompt(frame, prompt):\n",
    "    response_token_ids = get_vqa_response_token_ids(vlm_processor.tokenizer)\n",
    "    inputs = vlm_processor(text=prompt, images=frame, padding=True, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(vlm.device)\n",
    "\n",
    "    logits = vlm(**inputs).logits\n",
    "    logits = logits[0, -1].detach().cpu()\n",
    "    \n",
    "    this_probs = torch.stack([logits[response_token_ids[response_type]] for response_type in VQAResponse], dim=0)\n",
    "    this_probs = torch.softmax(this_probs, dim=0)\n",
    "\n",
    "    predicted_answer = VQAResponse(torch.argmax(this_probs, dim=0).numpy())\n",
    "\n",
    "    this_probs = this_probs.numpy()\n",
    "    answer_probs = {response_type: this_probs[response_type.value] for response_type in VQAResponse}\n",
    "    return predicted_answer, answer_probs    \n",
    "\n",
    "def cleanup_question(question):\n",
    "    question = question.split(\"?\")[0].strip() + \"?\"\n",
    "    if \".\" in question:\n",
    "        question = question.split(\".\")[1].strip()    \n",
    "    return question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some data to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting ',' delimiter: line 42536 column 242 (char 8795924)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtravel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mego4d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ego4DMistakeDetectionDataset\n\u001b[0;32m----> 4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mEgo4DMistakeDetectionDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mmismatch_augmentation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mmulti_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mdebug_n_examples_per_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m random_example_idx \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset))))\n\u001b[1;32m      9\u001b[0m random_example \u001b[38;5;241m=\u001b[39m dataset[random_example_idx]\n",
      "File \u001b[0;32m/nfs/turbo/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/travel/data/ego4d/__init__.py:739\u001b[0m, in \u001b[0;36mEgo4DMistakeDetectionDataset.__init__\u001b[0;34m(self, data_split, mismatch_augmentation, multi_frame, debug_n_examples_per_class, n_workers, worker_index)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmismatch_augmentation \u001b[38;5;241m=\u001b[39m mismatch_augmentation\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_frame \u001b[38;5;241m=\u001b[39m multi_frame\n\u001b[0;32m--> 739\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmismatch_augmentation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmismatch_augmentation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmulti_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_frame\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mdebug_n_examples_per_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug_n_examples_per_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mn_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mworker_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworker_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nfs/turbo/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/travel/data/mistake_detection.py:127\u001b[0m, in \u001b[0;36mMistakeDetectionDataset.__init__\u001b[0;34m(self, data_split, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_generated: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_dir):\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m# Loading dataset for the first time\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# Dataset directory already exists, but may or may not be fully generated\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_dataset_metadata()\n",
      "File \u001b[0;32m/nfs/turbo/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/travel/data/ego4d/__init__.py:808\u001b[0m, in \u001b[0;36mEgo4DMistakeDetectionDataset.generate_examples\u001b[0;34m(self, data_split, mismatch_augmentation, multi_frame, debug_n_examples_per_class, n_workers, worker_index)\u001b[0m\n\u001b[1;32m    806\u001b[0m full_cache_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_cache_dir(data_split, mismatch_augmentation\u001b[38;5;241m=\u001b[39mmismatch_augmentation, multi_frame\u001b[38;5;241m=\u001b[39mmulti_frame, debug_n_examples_per_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(full_cache_dir):\n\u001b[0;32m--> 808\u001b[0m     full_example_dirs \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_cache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_dirs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    810\u001b[0m     positive_dirs \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m full_example_dirs \u001b[38;5;28;01mif\u001b[39;00m d\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m    811\u001b[0m     random\u001b[38;5;241m.\u001b[39mshuffle(positive_dirs)\n",
      "File \u001b[0;32m~/.conda/envs/python310/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/python310/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/.conda/envs/python310/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/.conda/envs/python310/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 42536 column 242 (char 8795924)"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from travel.data.ego4d import Ego4DMistakeDetectionDataset\n",
    "dataset = Ego4DMistakeDetectionDataset(data_split=\"val\",\n",
    "                                               mismatch_augmentation=True,\n",
    "                                               multi_frame=False,\n",
    "                                               debug_n_examples_per_class=1000)\n",
    "random_example_idx = random.choice(list(range(len(dataset))))\n",
    "random_example = dataset[random_example_idx]\n",
    "\n",
    "frame = random_example.frames[0]\n",
    "procedure = random_example.procedure_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{procedure} ({\"mistake\" if random_example.mistake else \"success\"}: {random_example.mistake_type})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "frames = [\n",
    "    Image.open(\"demo_images/demo_frame0.png\").convert(\"RGB\"),\n",
    "    Image.open(\"demo_images/demo_frame1.png\").convert(\"RGB\"),\n",
    "    Image.open(\"demo_images/demo_frame2.png\").convert(\"RGB\"),\n",
    "]\n",
    "frame = frames[1]\n",
    "procedure = \"In a bowl, add the cut cherry tomatoes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run iterative question generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "prompt_prefixes = []\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "prompt_prefixes.append(\n",
    "    f'USER: <image>\\nThis is a photo of someone working on the procedure \"{procedure}\". I will ask a series of different yes/no questions about the state of the scene to determine whether the person has successfully executed the procedure. The goal is to extract as much relevant information as possible from the scene, so I will not repeat questions.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Is the bowl visible in the photo?\\nA: Yes',\n",
      " ' Is the bowl on the table?\\nA: Yes',\n",
      " ' Is the bowl visible in the photo? A\\nA: Yes',\n",
      " ' Is the bowl visible in the photo?\\nA: No']\n",
      "Is the bowl visible in the photo?\n",
      "Yes {<VQAResponse.No: 0>: 0.060975183, <VQAResponse.Yes: 1>: 0.9390248}\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to generate a question and answer recursively\n",
    "from travel.data.vqg import VQG_DEMONSTRATIONS, generate_vqg_prompt_icl\n",
    "\n",
    "prompt_question = prompt_prefixes[-1] + \" USER: Q: \"\n",
    "question = cleanup_question(simple_prompt_textonly(prompt_question.replace(\"<image>\\n\", \"\"), max_new_tokens=20, avoid_str=questions, generation_kwargs=question_generation_kwargs))\n",
    "questions.append(question)\n",
    "\n",
    "prompt_answer = prompt_question + f'{question} ASSISTANT: A (yes/no): '\n",
    "pred, probs = yes_no_prompt(frame, prompt_answer)\n",
    "answer = pred.name\n",
    "answers.append((answer, probs))\n",
    "\n",
    "prompt_prefixes.append(prompt_answer + answer)\n",
    "\n",
    "print(question)\n",
    "print(answer, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Are there any cherry tomatoes that are not in the bowl?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there any cherry tomatoes that are not in the bowl?\n",
      "Yes {<VQAResponse.No: 0>: 0.4765797, <VQAResponse.Yes: 1>: 0.5234204}\n"
     ]
    }
   ],
   "source": [
    "# Manually specify a question and add to prompt\n",
    "from travel.data.vqg import VQG_DEMONSTRATIONS, generate_vqg_prompt_icl\n",
    "\n",
    "prompt_question = prompt_prefixes[-1] + \" USER: Q: \"\n",
    "question = input()\n",
    "questions.append(question)\n",
    "\n",
    "prompt_answer = prompt_question + f'{question} ASSISTANT: A (yes/no): '\n",
    "pred, probs = yes_no_prompt(frame, prompt_answer)\n",
    "answer = pred.name\n",
    "answers.append((answer, probs))\n",
    "\n",
    "prompt_prefixes.append(prompt_answer + answer)\n",
    "\n",
    "print(question)\n",
    "print(answer, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Is the bowl visible in the photo?',\n",
      "  ('Yes', {<VQAResponse.No: 0>: 0.060975183, <VQAResponse.Yes: 1>: 0.9390248})),\n",
      " ('Are there cherry tomatoes in the bowl?',\n",
      "  ('Yes', {<VQAResponse.No: 0>: 0.0269635, <VQAResponse.Yes: 1>: 0.9730365})),\n",
      " ('Are there any cherry tomatoes that are not in the bowl?',\n",
      "  ('Yes', {<VQAResponse.No: 0>: 0.4765797, <VQAResponse.Yes: 1>: 0.5234204}))]\n",
      "==============\n",
      "No {<VQAResponse.No: 0>: 0.70578504, <VQAResponse.Yes: 1>: 0.29421496}\n"
     ]
    }
   ],
   "source": [
    "# Ask about success to get final probability\n",
    "pprint(list(zip(questions, answers)))\n",
    "print(\"==============\")\n",
    "\n",
    "prompt_final = prompt_prefixes[-1] + f' USER: Q: Based on the above information, is the procedure \"{procedure}\" 100% finished?'\n",
    "pred, probs = yes_no_prompt(frame, prompt_final)\n",
    "answer_final = pred.name\n",
    "\n",
    "print(answer_final, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the bowl visible in the photo? ->\n",
      "No {<VQAResponse.No: 0>: 0.6334103, <VQAResponse.Yes: 1>: 0.36658975}\n",
      "\n",
      "Is the bowl filled with cherry tomatoes? ->\n",
      "Yes {<VQAResponse.No: 0>: 0.20434189, <VQAResponse.Yes: 1>: 0.79565805}\n",
      "\n",
      "Are the cherry tomatoes cut? ->\n",
      "Yes {<VQAResponse.No: 0>: 0.2925953, <VQAResponse.Yes: 1>: 0.70740473}\n",
      "\n",
      "Are the cherry tomatoes in the bowl? ->\n",
      "Yes {<VQAResponse.No: 0>: 0.24944724, <VQAResponse.Yes: 1>: 0.7505527}\n",
      "\n",
      "Are there any cherry tomatoes outside of the bowl? ->\n",
      "Yes {<VQAResponse.No: 0>: 0.49804685, <VQAResponse.Yes: 1>: 0.50195307}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try individual questions one at a time instead of in a dialog\n",
    "for question, (answer, _) in zip(questions, answers):\n",
    "    prompt_only_question = prompt_prefixes[0] + f\" USER: Q: {question} + ASSISTANT: A: \"\n",
    "    pred, probs = yes_no_prompt(frame, prompt_only_question)\n",
    "    new_answer = pred.name\n",
    "    \n",
    "    \n",
    "    prompt_final = prompt_prefixes[0] + f\" USER: Q: {question} + ASSISTANT: A: {new_answer}\" + f' USER: Q: Based on this information, is the procedure \"{procedure}\" 100% finished?'\n",
    "    pred, probs = yes_no_prompt(frame, prompt_final)\n",
    "    answer_final = pred.name\n",
    "\n",
    "    print(question + \" ->\")\n",
    "    print(answer_final, probs)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel",
   "language": "python",
   "name": "travel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
