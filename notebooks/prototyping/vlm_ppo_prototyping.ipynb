{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir(\"/nfs/turbo/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl\")\n",
    "from travel import init_travel\n",
    "init_travel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-2\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 1\n",
    "LORA_R=32\n",
    "LORA_ALPHA=32\n",
    "global_rank = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "from travel.constants import RESULTS_DIR\n",
    "\n",
    "# Set up output directory, training args, and wandb\n",
    "timestamp = datetime.datetime.now()\n",
    "this_results_dir = os.path.join(RESULTS_DIR, f\"vqg_learning/PPO_VLM_prototyping/PPO_lr{LEARNING_RATE}_bs{BATCH_SIZE}_e{EPOCHS}_r{LORA_R}_alpha{LORA_ALPHA}_{timestamp.strftime('%Y%m%d%H%M%S')}\")\n",
    "wandb_run_name = f\"PPO_lr{LEARNING_RATE}_bs{BATCH_SIZE}_e{EPOCHS}_r{LORA_R}_alpha{LORA_ALPHA}_{timestamp.strftime('%Y%m%d%H%M%S')}\"\n",
    "if not os.path.exists(this_results_dir):\n",
    "    os.makedirs(this_results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(task_type=\"CAUSAL_LM\",  # configured for causal LM\n",
    "                        inference_mode=False,           # enable training - for inference, we can pre-compute the weight update matrix\n",
    "                        r=LORA_R,                           # dimension of low-rank matrices\n",
    "                        lora_alpha=LORA_ALPHA,                  # scaling coefficient of weight update\n",
    "                        # target_modules=\"all-linear\",\n",
    "                        # lora_dropout=0.1,               # dropout regularization on LoRA weights\n",
    "                        bias=\"none\")                     # use LoRA to train \"all\" biases (alternatives: \"none\", \"lora_only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sstorks/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/sstorks/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:104: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n",
      "  warnings.warn(\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d93967a53394b76bcad7a8775038e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "from trl import PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "\n",
    "VLM_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "vlm = AutoModelForVision2Seq.from_pretrained(VLM_NAME, \n",
    "                                            quantization_config=bnb_config)\n",
    "lm_ppo = PeftModelForCausalLM(vlm.language_model, peft_config)\n",
    "lm_ppo = AutoModelForCausalLMWithValueHead.from_pretrained(lm_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "vlm.generation_config.temperature = None\n",
    "vlm.generation_config.top_p = None\n",
    "vlm.generation_config.do_sample = False\n",
    "# vlm.generation_config.diversity_penalty = 1.0\n",
    "vlm.generation_config.num_beams = 4\n",
    "# vlm.generation_config.num_beam_groups = 1\n",
    "vlm.generation_config.num_return_sequences=4\n",
    "\n",
    "vlm.language_model.generation_config.temperature = None\n",
    "vlm.language_model.generation_config.top_p = None\n",
    "vlm.language_model.generation_config.do_sample = False\n",
    "# vlm.language_model.generation_config.diversity_penalty = 1.0\n",
    "vlm.language_model.generation_config.num_beams = 4\n",
    "# vlm.language_model.generation_config.num_beam_groups = 1\n",
    "vlm.language_model.generation_config.num_return_sequences=4\n",
    "\n",
    "vlm_processor = AutoProcessor.from_pretrained(VLM_NAME)\n",
    "vlm_processor.tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from transformers import PhrasalConstraint, DisjunctiveConstraint\n",
    "from travel.data.vqa import get_vqa_response_token_ids, VQAResponse\n",
    "\n",
    "# kwargs to force question generations to have a \"?\" and start with \"Is\" or \"Are\"\n",
    "question_generation_constraints = [    \n",
    "    PhrasalConstraint(\n",
    "        [vlm_processor.tokenizer(\"Is it blue?\", add_special_tokens=False).input_ids[-1]]\n",
    "    ),\n",
    "]\n",
    "yes_no_q_tokens = [\n",
    "    vlm_processor.tokenizer(\"Is it blue?\", add_special_tokens=False).input_ids[0], \n",
    "    vlm_processor.tokenizer(\"Are they blue?\", add_special_tokens=False).input_ids[0],\n",
    "    vlm_processor.tokenizer(\"Does it look blue?\", add_special_tokens=False).input_ids[0],\n",
    "    vlm_processor.tokenizer(\"Do they look blue?\", add_special_tokens=False).input_ids[0],\n",
    "]\n",
    "begin_suppress_tokens = [t for t in list(range(vlm_processor.tokenizer.vocab_size)) if t not in yes_no_q_tokens]\n",
    "question_generation_kwargs = {\n",
    "    \"constraints\": question_generation_constraints,\n",
    "    \"begin_suppress_tokens\": begin_suppress_tokens,\n",
    "}\n",
    "\n",
    "qmark_token_id = vlm_processor.tokenizer(\"Is it blue?\", add_special_tokens=False).input_ids[-1]\n",
    "newline_token_id = vlm_processor.tokenizer.encode(\"\\n\", add_special_tokens=False)[1] # this should be 13 for LLaMA 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "from travel.data.vqa import VQAResponse\n",
    "from travel.model.nli import NLI_HYPOTHESIS_TEMPLATE, NLI_MODEL_PATH\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Set up NLI model for online feedback\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(NLI_MODEL_PATH, quantization_config=bnb_config)\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(NLI_MODEL_PATH)\n",
    "\n",
    "def get_entailment_probability(premise, hypothesis):\n",
    "    probs_expected = run_nli(nli_tokenizer, nli_model, list(zip(premise, hypothesis)))\n",
    "    return probs_expected[:, 1].numpy()\n",
    "\n",
    "def calculate_relevance_informativeness(procedure, question, vlm_answer, is_mistake):\n",
    "    hypothesis = NLI_HYPOTHESIS_TEMPLATE.format(procedure=procedure)\n",
    "    prob_e = get_entailment_probability([question + \" \" + vlm_answer.name], [hypothesis] * 2)\n",
    "    prob_u = get_entailment_probability([question + \" \" + VQAResponse(1-vlm_answer.value).name], [hypothesis] * 2)\n",
    "    \n",
    "    negation_based_relevance = float(np.abs(prob_e - prob_u))\n",
    "    informativeness = float(prob_e if not is_mistake else 1.0 - prob_e)\n",
    "\n",
    "    return negation_based_relevance, informativeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also add some helper prompting methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from travel.data.vqa import get_vqa_response_token_ids, VQAResponse\n",
    "\n",
    "def simple_prompt(vlm, vlm_processor, frame, prompt, max_new_tokens=20, avoid_str=[], generation_kwargs={}):\n",
    "   \n",
    "    inputs = vlm_processor(text=prompt, images=frame, padding=True, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(vlm.device)\n",
    "\n",
    "    outputs = vlm.generate(**inputs, max_new_tokens=max_new_tokens, return_dict_in_generate=True, output_scores=True, **generation_kwargs)\n",
    "    outputs = vlm_processor.batch_decode(outputs.sequences, skip_special_tokens=True)\n",
    "    \n",
    "    outputs = [output.replace(\"USER:  \", \"USER: <image>\") for output in outputs]\n",
    "    outputs = [output.replace(prompt, \"\") for output in outputs]\n",
    "    \n",
    "    for output in outputs:\n",
    "        if avoid_str and any(s in output for s in avoid_str):\n",
    "            continue\n",
    "        return output\n",
    "    \n",
    "    if avoid_str:\n",
    "        print(\"Warning: returning avoid_str!\")\n",
    "    return outputs[0]\n",
    "\n",
    "def simple_prompt_textonly(lm, tokenizer, prompt, max_new_tokens=20, avoid_str=[], generation_kwargs={}):\n",
    "   \n",
    "    inputs = tokenizer(text=prompt, padding=True, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(lm.device)\n",
    "\n",
    "    outputs = lm.generate(**inputs, max_new_tokens=max_new_tokens, return_dict_in_generate=True, output_scores=True, **generation_kwargs)\n",
    "    outputs = tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)\n",
    "    \n",
    "    outputs = [output.replace(prompt, \"\") for output in outputs]\n",
    "    \n",
    "    for output in outputs:\n",
    "        if avoid_str and any(s in output for s in avoid_str):\n",
    "            continue\n",
    "        return output\n",
    "    \n",
    "    if avoid_str:\n",
    "        print(\"Warning: returning avoid_str!\")\n",
    "    return outputs[0]\n",
    "\n",
    "def yes_no_prompt(vlm, vlm_processor, frame, prompt):\n",
    "    response_token_ids = get_vqa_response_token_ids(vlm_processor.tokenizer)\n",
    "    inputs = vlm_processor(text=prompt, images=frame, padding=True, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(vlm.device)\n",
    "\n",
    "    logits = vlm(**inputs).logits\n",
    "    logits = logits[0, -1].detach().cpu()\n",
    "    \n",
    "    this_probs = torch.stack([logits[response_token_ids[response_type]] for response_type in VQAResponse], dim=0)\n",
    "    this_probs = torch.softmax(this_probs, dim=0)\n",
    "\n",
    "    predicted_answer = VQAResponse(torch.argmax(this_probs, dim=0).numpy())\n",
    "\n",
    "    this_probs = this_probs.numpy()\n",
    "    answer_probs = {response_type: this_probs[response_type.value] for response_type in VQAResponse}\n",
    "    return predicted_answer, answer_probs    \n",
    "\n",
    "def cleanup_question(question):\n",
    "    question = question.split(\"?\")[0].strip() + \"?\"\n",
    "    if \".\" in question:\n",
    "        question = question.split(\".\")[1].strip()    \n",
    "    return question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0) Preparing training data...\n",
      "train data partition: 1000 examples\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "from travel.constants import DATA_CACHE_DIR\n",
    "from travel.data.ego4d import Ego4DMistakeDetectionDataset\n",
    "\n",
    "dataset = Ego4DMistakeDetectionDataset(data_split=\"train\",\n",
    "                                        mismatch_augmentation=True,\n",
    "                                        multi_frame=False,\n",
    "                                        debug_n_examples_per_class=500)\n",
    "\n",
    "\n",
    "# Prepare training examples from Ego4D mistake detection dataset\n",
    "print(f\"({global_rank}) Preparing training data...\")\n",
    "dataset_path = os.path.join(DATA_CACHE_DIR, f\"ppo_training_dataset_icl8_debug500\")\n",
    "ppo_dataset = Dataset.load_from_disk(dataset_path=dataset_path)\n",
    "\n",
    "# Balance mistake/success examples\n",
    "positive_examples = [example for example in ppo_dataset if \"pos\" in example['example_id']]\n",
    "negative_examples = [example for example in ppo_dataset if \"pos\" not in example['example_id']]\n",
    "if len(positive_examples) < len(negative_examples):\n",
    "    print(f\"Upsampling {len(negative_examples) - len(positive_examples)} more positive examples.\")\n",
    "    ppo_dataset += random.sample(positive_examples, len(negative_examples) - len(positive_examples))\n",
    "elif len(positive_examples) > len(negative_examples):\n",
    "    print(f\"Upsampling {len(positive_examples) - len(negative_examples)} more negative examples.\")\n",
    "    ppo_dataset += random.sample(negative_examples, len(positive_examples) - len(negative_examples))\n",
    "\n",
    "print(f\"train data partition: {len(ppo_dataset)} examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up PPO trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Need this call at the beginning of every script to set random seeds and set the HF cache\n",
    "from travel import init_travel\n",
    "init_travel()\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from trl import PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "import wandb\n",
    "\n",
    "from travel.constants import HF_TOKEN, DATA_CACHE_DIR, RANDOM_SEED\n",
    "from travel.data.mistake_detection import MistakeDetectionTasks\n",
    "from travel.data.vqa import VQAResponse\n",
    "from travel.data.vqg import generate_vqg_prompt_icl, VQGOutputs\n",
    "from travel.model.nli import run_nli, NLI_HYPOTHESIS_TEMPLATE\n",
    "from travel.model.ppo_trainer import PerTokenPPOTrainer as PPOTrainer\n",
    "from travel.model.vqg import parse_vqg_outputs\n",
    "\n",
    "# Set up PPO trainer\n",
    "def collator(data):\n",
    "    return {key: [d[key] for d in data] for key in data[0]}\n",
    "ppo_config = PPOConfig(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    batch_size=4,\n",
    "    mini_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    remove_unused_columns=False,\n",
    "    optimize_cuda_cache=True,\n",
    "    early_stopping=True,\n",
    "    is_peft_model=True,\n",
    "    seed=RANDOM_SEED,\n",
    ")\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model=lm_ppo,\n",
    "    ref_model=vlm.language_model,\n",
    "    config=ppo_config,\n",
    "    dataset=ppo_dataset,\n",
    "    tokenizer=vlm_processor.tokenizer,\n",
    "    data_collator=collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:5iy56zpt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede8776e003243d88eb2364b1a666e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.028 MB of 0.028 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>objective/entropy</td><td>▁▄█▅</td></tr><tr><td>objective/kl</td><td>█▃▁▂</td></tr><tr><td>objective/kl_coef</td><td>▅█▅▁</td></tr><tr><td>ppo/epoch</td><td>▁▁▁▁</td></tr><tr><td>ppo/learning_rate</td><td>▁▁▁▁</td></tr><tr><td>ppo/loss/policy</td><td>█▁▁▂</td></tr><tr><td>ppo/loss/total</td><td>▁▃█▃</td></tr><tr><td>ppo/loss/value</td><td>▁▄█▄</td></tr><tr><td>ppo/mean_non_score_reward</td><td>▁▆█▇</td></tr><tr><td>ppo/mean_scores</td><td>▄█▁▃</td></tr><tr><td>ppo/policy/advantages_mean</td><td>▃█▄▁</td></tr><tr><td>ppo/policy/approxkl</td><td>█▄▁▁</td></tr><tr><td>ppo/policy/clipfrac</td><td>▃█▂▁</td></tr><tr><td>ppo/policy/entropy</td><td>█▃▁▃</td></tr><tr><td>ppo/policy/policykl</td><td>██▁▅</td></tr><tr><td>ppo/returns/mean</td><td>▁▅██</td></tr><tr><td>ppo/returns/var</td><td>▁▄█▅</td></tr><tr><td>ppo/std_scores</td><td>██▁▅</td></tr><tr><td>ppo/val/clipfrac</td><td>▁██▅</td></tr><tr><td>ppo/val/error</td><td>▁▄█▄</td></tr><tr><td>ppo/val/mean</td><td>▁▂▆█</td></tr><tr><td>ppo/val/var</td><td>█▆▁▂</td></tr><tr><td>ppo/val/var_explained</td><td>█▃▁▃</td></tr><tr><td>ppo/val/vpred</td><td>▁▄▆█</td></tr><tr><td>rewards/consistency</td><td>▄█▁▃</td></tr><tr><td>time/ppo/calc_stats</td><td>▁▅█▄</td></tr><tr><td>time/ppo/compute_advantages</td><td>█▃▄▁</td></tr><tr><td>time/ppo/compute_rewards</td><td>▄▃█▁</td></tr><tr><td>time/ppo/forward_pass</td><td>█▁▂▄</td></tr><tr><td>time/ppo/optimize_step</td><td>█▂▁▃</td></tr><tr><td>time/ppo/total</td><td>█▁▁▃</td></tr><tr><td>tokens/queries_len_mean</td><td>▁▁▁▁</td></tr><tr><td>tokens/queries_len_std</td><td>▁▁▁▁</td></tr><tr><td>tokens/responses_len_mean</td><td>█▁▄▁</td></tr><tr><td>tokens/responses_len_std</td><td>█▇▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>objective/entropy</td><td>931.00043</td></tr><tr><td>objective/kl</td><td>-48.2746</td></tr><tr><td>objective/kl_coef</td><td>0.19998</td></tr><tr><td>ppo/epoch</td><td>0</td></tr><tr><td>ppo/learning_rate</td><td>0.0001</td></tr><tr><td>ppo/loss/policy</td><td>-0.07328</td></tr><tr><td>ppo/loss/total</td><td>0.10311</td></tr><tr><td>ppo/loss/value</td><td>1.76387</td></tr><tr><td>ppo/mean_non_score_reward</td><td>0.08269</td></tr><tr><td>ppo/mean_scores</td><td>0.18112</td></tr><tr><td>ppo/policy/advantages_mean</td><td>-0.0</td></tr><tr><td>ppo/policy/approxkl</td><td>0.05952</td></tr><tr><td>ppo/policy/clipfrac</td><td>0.35171</td></tr><tr><td>ppo/policy/entropy</td><td>5.51582</td></tr><tr><td>ppo/policy/policykl</td><td>-0.02126</td></tr><tr><td>ppo/returns/mean</td><td>1.53826</td></tr><tr><td>ppo/returns/var</td><td>1.86913</td></tr><tr><td>ppo/std_scores</td><td>0.22767</td></tr><tr><td>ppo/val/clipfrac</td><td>0.43094</td></tr><tr><td>ppo/val/error</td><td>3.14489</td></tr><tr><td>ppo/val/mean</td><td>0.02033</td></tr><tr><td>ppo/val/var</td><td>0.14393</td></tr><tr><td>ppo/val/var_explained</td><td>-0.68254</td></tr><tr><td>ppo/val/vpred</td><td>0.29665</td></tr><tr><td>rewards/consistency</td><td>0.18112</td></tr><tr><td>time/ppo/calc_stats</td><td>0.00297</td></tr><tr><td>time/ppo/compute_advantages</td><td>0.01019</td></tr><tr><td>time/ppo/compute_rewards</td><td>0.00156</td></tr><tr><td>time/ppo/forward_pass</td><td>0.49855</td></tr><tr><td>time/ppo/optimize_step</td><td>2.24207</td></tr><tr><td>time/ppo/total</td><td>2.75546</td></tr><tr><td>tokens/queries_len_mean</td><td>0.0</td></tr><tr><td>tokens/queries_len_std</td><td>0.0</td></tr><tr><td>tokens/responses_len_mean</td><td>158.25</td></tr><tr><td>tokens/responses_len_std</td><td>7.5</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">PPO_lr1e-05_bs4_e1_r32_alpha32_20240731223250</strong> at: <a href='https://wandb.ai/shanestorks-research/TRAVEl/runs/5iy56zpt' target=\"_blank\">https://wandb.ai/shanestorks-research/TRAVEl/runs/5iy56zpt</a><br/> View project at: <a href='https://wandb.ai/shanestorks-research/TRAVEl' target=\"_blank\">https://wandb.ai/shanestorks-research/TRAVEl</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240731_224303-5iy56zpt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:5iy56zpt). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef59b0bd7c348238855c4cdaeaf6a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112092073178953, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/turbo/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl/wandb/run-20240731_224659-3szzab79</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shanestorks-research/TRAVEl/runs/3szzab79' target=\"_blank\">PPO_lr1e-05_bs4_e1_r32_alpha32_20240731223250</a></strong> to <a href='https://wandb.ai/shanestorks-research/TRAVEl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shanestorks-research/TRAVEl' target=\"_blank\">https://wandb.ai/shanestorks-research/TRAVEl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shanestorks-research/TRAVEl/runs/3szzab79' target=\"_blank\">https://wandb.ai/shanestorks-research/TRAVEl/runs/3szzab79</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(0) epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "(0) batch:   0%|          | 0/250 [00:00<?, ?it/s]\u001b[ASetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.25it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.07it/s]\n",
      "/tmp/ipykernel_1580706/710827801.py:19: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  negation_based_relevance = float(np.abs(prob_e - prob_u))\n",
      "/tmp/ipykernel_1580706/710827801.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  informativeness = float(prob_e if not is_mistake else 1.0 - prob_e)\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.29it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.36it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.32it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.13it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.41it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.49it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 14.84it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.25it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.55it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.52it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.52it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.54it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.36it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.72it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.57it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.42it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.80it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.07it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.11it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.97it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.09it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procedure:\n",
      "Press a button on the phone\n",
      "Generated questions, answers, and scores:\n",
      "[\"Is the phone in the person's hand?\",\n",
      " \"Is the phone in the person's right hand?\",\n",
      " \"Is the phone in the person's left hand?\"]\n",
      "[('Yes', {<VQAResponse.No: 0>: 0.026155619, <VQAResponse.Yes: 1>: 0.9738444}),\n",
      " ('Yes', {<VQAResponse.No: 0>: 0.23651622, <VQAResponse.Yes: 1>: 0.7634837}),\n",
      " ('No', {<VQAResponse.No: 0>: 0.9829547, <VQAResponse.Yes: 1>: 0.017045317})]\n",
      "[0.19792819023132324, 0.17500615119934082, 0.7260103225708008]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sstorks/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1308: UserWarning: KL divergence is starting to become negative: -58.79 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "/home/sstorks/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1387: UserWarning: The game logs will not be logged because the batch does not contain the keys 'query' and 'response'. \n",
      "  warnings.warn(\n",
      "/home/sstorks/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/sstorks/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/sstorks/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1426: UserWarning: Cannot retrieve user information assuming you are running in offline mode.\n",
      "  warnings.warn(\"Cannot retrieve user information assuming you are running in offline mode.\")\n",
      "\n",
      "(0) batch:   0%|          | 1/250 [00:39<2:45:55, 39.98s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.22it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.27it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.52it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.56it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.68it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.66it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.46it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.61it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.73it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.75it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.98it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.92it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.95it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.93it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.04it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.15it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.68it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.54it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.61it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.62it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.55it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.55it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.55it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procedure:\n",
      "Hold the leaves\n",
      "Generated questions, answers, and scores:\n",
      "['Is the person holding the leaves?',\n",
      " \"Are the leaves in the person's hand?\",\n",
      " 'Are the leaves green?']\n",
      "[('Yes', {<VQAResponse.No: 0>: 0.09670579, <VQAResponse.Yes: 1>: 0.9032942}),\n",
      " ('Yes', {<VQAResponse.No: 0>: 0.054600604, <VQAResponse.Yes: 1>: 0.94539934}),\n",
      " ('Yes', {<VQAResponse.No: 0>: 0.03410043, <VQAResponse.Yes: 1>: 0.9658996})]\n",
      "[0.6898252964019775, 0.9153685569763184, 0.7953414916992188]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sstorks/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1308: UserWarning: KL divergence is starting to become negative: -94.03 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "\n",
      "(0) batch:   1%|          | 2/250 [01:19<2:44:04, 39.69s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.53it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.52it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.64it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.05it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.80it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.83it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.06it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.06it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.50it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.34it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.72it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.94it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.80it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.71it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.76it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.93it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.58it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.77it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.67it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.00it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.98it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.95it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.91it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procedure:\n",
      "Pack flour from a plate with your hand\n",
      "Generated questions, answers, and scores:\n",
      "['Is the flour on the plate?',\n",
      " \"Is the flour on the person's hand?\",\n",
      " 'Is the flour on the table?']\n",
      "[('Yes', {<VQAResponse.No: 0>: 0.18713269, <VQAResponse.Yes: 1>: 0.81286734}),\n",
      " ('No', {<VQAResponse.No: 0>: 0.6095242, <VQAResponse.Yes: 1>: 0.39047584}),\n",
      " ('Yes', {<VQAResponse.No: 0>: 0.48633155, <VQAResponse.Yes: 1>: 0.5136685})]\n",
      "[0.907508134841919, 0.014972209930419922, 0.5350005626678467]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sstorks/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1308: UserWarning: KL divergence is starting to become negative: -131.02 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "\n",
      "(0) batch:   1%|          | 3/250 [02:11<3:05:53, 45.16s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.46it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.42it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.66it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.68it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.65it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.50it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.59it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.67it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.49it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.45it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.00it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.13it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.09it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.12it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.07it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.06it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.51it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.32it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.65it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.71it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.66it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.67it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.61it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procedure:\n",
      "Wash the avocado pear under running water\n",
      "Generated questions, answers, and scores:\n",
      "[\"Is the avocado pear in the person's hand?\",\n",
      " 'Is the avocado pear in the sink?',\n",
      " 'Is the sink full of water?']\n",
      "[('No', {<VQAResponse.No: 0>: 0.61323655, <VQAResponse.Yes: 1>: 0.38676345}),\n",
      " ('Yes', {<VQAResponse.No: 0>: 0.29746994, <VQAResponse.Yes: 1>: 0.7025301}),\n",
      " ('Yes', {<VQAResponse.No: 0>: 0.2991052, <VQAResponse.Yes: 1>: 0.70089483})]\n",
      "[0.19784927368164062, 0.010212421417236328, 0.24869012832641602]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sstorks/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1308: UserWarning: KL divergence is starting to become negative: -121.80 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "\n",
      "(0) batch:   2%|▏         | 4/250 [02:50<2:56:30, 43.05s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.54it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.45it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.71it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.80it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.67it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.79it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.74it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.58it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.74it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.87it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.16it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.17it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.15it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.16it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.16it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.24it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.20it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.67it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.76it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.92it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.96it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.93it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.95it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procedure:\n",
      "Shuffle a card\n",
      "Generated questions, answers, and scores:\n",
      "['Is the person holding a deck of cards?',\n",
      " \"Is the deck of cards in the person's hand?\",\n",
      " \"Is the deck of cards in the person's hand upside down?\"]\n",
      "[('Yes', {<VQAResponse.No: 0>: 0.07316472, <VQAResponse.Yes: 1>: 0.92683524}),\n",
      " ('Yes', {<VQAResponse.No: 0>: 0.051082738, <VQAResponse.Yes: 1>: 0.94891727}),\n",
      " ('No', {<VQAResponse.No: 0>: 0.64779824, <VQAResponse.Yes: 1>: 0.3522018})]\n",
      "[0.6431422233581543, 0.6291365623474121, 0.024912066757678986]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sstorks/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1308: UserWarning: KL divergence is starting to become negative: -126.23 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "\n",
      "(0) batch:   2%|▏         | 5/250 [03:30<2:50:48, 41.83s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.82it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.97it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.17it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.22it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.17it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.14it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.15it/s]\n",
      "\n",
      "\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.04it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "(0) batch:   2%|▏         | 5/250 [03:50<3:08:20, 46.13s/it]\n",
      "(0) epoch:   0%|          | 0/1 [03:50<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question_generated \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Generate a question\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     prompts_q \u001b[38;5;241m=\u001b[39m [prompt \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m USER: Q: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m---> 29\u001b[0m     new_questions \u001b[38;5;241m=\u001b[39m [simple_prompt_textonly(vlm\u001b[38;5;241m.\u001b[39mlanguage_model,\n\u001b[1;32m     30\u001b[0m                                             vlm_processor\u001b[38;5;241m.\u001b[39mtokenizer,\n\u001b[1;32m     31\u001b[0m                                             prompt\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<image>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     32\u001b[0m                                             max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m     33\u001b[0m                                             avoid_str\u001b[38;5;241m=\u001b[39mquestions[prompt_idx],\n\u001b[1;32m     34\u001b[0m                                             generation_kwargs\u001b[38;5;241m=\u001b[39mquestion_generation_kwargs) \u001b[38;5;28;01mfor\u001b[39;00m prompt_idx, prompt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(prompts_q)]\n\u001b[1;32m     35\u001b[0m     new_questions \u001b[38;5;241m=\u001b[39m [cleanup_question(question) \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m new_questions]\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m qi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(new_questions)):\n",
      "Cell \u001b[0;32mIn[56], line 29\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question_generated \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Generate a question\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     prompts_q \u001b[38;5;241m=\u001b[39m [prompt \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m USER: Q: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m---> 29\u001b[0m     new_questions \u001b[38;5;241m=\u001b[39m [\u001b[43msimple_prompt_textonly\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mvlm_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<image>\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mavoid_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion_generation_kwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m prompt_idx, prompt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(prompts_q)]\n\u001b[1;32m     35\u001b[0m     new_questions \u001b[38;5;241m=\u001b[39m [cleanup_question(question) \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m new_questions]\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m qi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(new_questions)):\n",
      "Cell \u001b[0;32mIn[44], line 29\u001b[0m, in \u001b[0;36msimple_prompt_textonly\u001b[0;34m(lm, tokenizer, prompt, max_new_tokens, avoid_str, generation_kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text\u001b[38;5;241m=\u001b[39mprompt, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(lm\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 29\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m outputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs\u001b[38;5;241m.\u001b[39msequences, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mreplace(prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1757\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1750\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1751\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1752\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   1753\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1754\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1755\u001b[0m     )\n\u001b[1;32m   1756\u001b[0m     \u001b[38;5;66;03m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constrained_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconstrained_beam_scorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstrained_beam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mcache_implementation \u001b[38;5;129;01min\u001b[39;00m NEED_SETUP_CACHE_CLASSES_MAPPING:\n\u001b[1;32m   1772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_reset_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)):\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:4249\u001b[0m, in \u001b[0;36mGenerationMixin._constrained_beam_search\u001b[0;34m(self, input_ids, constrained_beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   4246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m constrained_beam_scorer\u001b[38;5;241m.\u001b[39mis_done \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mall\u001b[39m(stopping_criteria(input_ids, scores)):\n\u001b[1;32m   4247\u001b[0m         this_peer_finished \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 4249\u001b[0m sequence_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mconstrained_beam_scorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4250\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeam_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4255\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeam_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_prompt_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_prompt_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate:\n\u001b[1;32m   4262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_scores:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/transformers/generation/beam_search.py:907\u001b[0m, in \u001b[0;36mConstrainedBeamSearchScorer.finalize\u001b[0;34m(self, input_ids, final_beam_scores, final_beam_tokens, final_beam_indices, max_length, pad_token_id, eos_token_id, beam_indices, decoder_prompt_len)\u001b[0m\n\u001b[1;32m    904\u001b[0m decoded[i, : sent_lengths[i]] \u001b[38;5;241m=\u001b[39m hypo\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 907\u001b[0m     indices[i, : \u001b[38;5;28mlen\u001b[39m(best_idx)] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sent_lengths[i] \u001b[38;5;241m<\u001b[39m sent_max_len:\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;66;03m# inserting only the first eos_token_id\u001b[39;00m\n\u001b[1;32m    911\u001b[0m     decoded[i, sent_lengths[i]] \u001b[38;5;241m=\u001b[39m eos_token_id[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of NoneType"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "wandb.init(name=wandb_run_name)\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), f\"({global_rank}) epoch\"):\n",
    "    for batch_idx, batch in enumerate(tqdm(ppo_trainer.dataloader, desc=f\"({global_rank}) batch\")):\n",
    "        # if batch_idx == 0:\n",
    "        #     keep_batch = batch\n",
    "        # else:\n",
    "        #     batch = keep_batch\n",
    "        \n",
    "        this_batch_size = len(batch[\"procedure_description\"])\n",
    "\n",
    "        # Load examples\n",
    "        examples = [dataset.load_example_from_file(example_dir, load_frames=False) for example_dir in batch['example_dir']]\n",
    "        frames = [Image.open(example.frames[0]) for example in examples]\n",
    "\n",
    "        prompts = [\n",
    "            f'USER: <image>\\nThis is a photo of someone working on the procedure \"{procedure}\". I will ask a series of different yes/no questions about the state of the scene to determine whether the person has successfully executed the procedure. The goal is to extract as much relevant information as possible from the scene, so I will not repeat questions.' \n",
    "            for procedure in batch['procedure_description']\n",
    "        ]\n",
    "        questions = [[] for _ in range(this_batch_size)]\n",
    "        answers = [[] for _ in range(this_batch_size)]\n",
    "        scores = [[] for _ in range(this_batch_size)]\n",
    "        for question_generated in range(3):\n",
    "            # Generate a question\n",
    "            prompts_q = [prompt + \" USER: Q: \" for prompt in prompts]\n",
    "            new_questions = [simple_prompt_textonly(vlm.language_model,\n",
    "                                                    vlm_processor.tokenizer,\n",
    "                                                    prompt.replace(\"<image>\\n\", \"\"),\n",
    "                                                    max_new_tokens=20,\n",
    "                                                    avoid_str=questions[prompt_idx],\n",
    "                                                    generation_kwargs=question_generation_kwargs) for prompt_idx, prompt in enumerate(prompts_q)]\n",
    "            new_questions = [cleanup_question(question) for question in new_questions]\n",
    "            for qi in range(len(new_questions)):\n",
    "                questions[qi].append(new_questions[qi])\n",
    "\n",
    "            # Predict an answer (yes/no)\n",
    "            prompts_a = [prompt + f'{question} ASSISTANT: A (yes/no): ' for prompt, question in zip(prompts_q, new_questions)]\n",
    "            new_answers = [yes_no_prompt(vlm, vlm_processor, frame, prompt) for frame, prompt in zip(frames, prompts_a)]\n",
    "            for ai in range(len(new_questions)):\n",
    "                answers[ai].append(new_answers[ai])\n",
    "\n",
    "            # Update prompts with answers\n",
    "            prompts = [prompt + pred.name for prompt, (pred, _) in zip(prompts_a, new_answers)]\n",
    "\n",
    "            # Score questions using NLI model\n",
    "            new_scores = [\n",
    "                calculate_relevance_informativeness(procedure, question, answer[0], example.mistake)\n",
    "                for procedure, question, answer, example in zip(batch['procedure_description'], new_questions, new_answers, examples)\n",
    "            ]\n",
    "            new_scores = [rel * inf for rel, inf in new_scores]\n",
    "            for si in range(len(new_scores)):\n",
    "                scores[si].append(new_scores[si])\n",
    "\n",
    "        # TODO: also incorporate model's final answer into scores\n",
    "        # TODO: add a chance that generated question will come from LM + ICL\n",
    "\n",
    "        print(\"Procedure:\")\n",
    "        print(batch[\"procedure_description\"][0])        \n",
    "        \n",
    "        print(\"Generated questions, answers, and scores:\")\n",
    "        pprint(questions[0])\n",
    "        pprint([(pred.name, probs) for pred, probs in answers[0]])\n",
    "        pprint(scores[0])\n",
    "        \n",
    "        # Re-encode final prompts to find indices to apply reward\n",
    "        query_tensors = [torch.zeros([0]).long() for _ in range(this_batch_size)] # use empty responses        \n",
    "        response_tensors = [vlm_processor.tokenizer.encode(prompt, return_tensors=\"pt\")[0] for prompt in prompts]\n",
    "        reward_indices = [(response_tensor == qmark_token_id).long().cpu() for response_tensor in response_tensors]\n",
    "        for i in range(len(reward_indices)):\n",
    "            reward_indices[i][reward_indices[i] == 1] = torch.cumsum((reward_indices[i] == 1).int(), dim=0)[reward_indices[i] == 1]\n",
    "            reward_indices[i][reward_indices[i] == 0] = -1\n",
    "            reward_indices[i][reward_indices[i] != -1] -= 1\n",
    "        reward = torch.tensor(scores).float()\n",
    "            \n",
    "        #### Run PPO step\n",
    "        stats = ppo_trainer.step(query_tensors, response_tensors, reward, reward_indices)\n",
    "        ppo_trainer.log_stats(stats, batch, reward, columns_to_log=(\"prompt\", \"response\"))\n",
    "        if global_rank == 0:\n",
    "            try:\n",
    "                wandb.log(stats | {\"ppo/epoch\": epoch, \n",
    "                                        \"rewards/consistency\": np.mean(reward.cpu().numpy()),})\n",
    "            except Exception as e:\n",
    "                print(\"Warning: failed to log to wandb!\")\n",
    "                pprint(e)\n",
    "\n",
    "        #### Save model\n",
    "        if epoch % 5 == 0 and global_rank == 0:\n",
    "            if not os.path.exists(os.path.join(this_results_dir, f\"epoch{epoch}\")):\n",
    "                os.makedirs(os.path.join(this_results_dir, f\"epoch{epoch}\"))\n",
    "            ppo_trainer.save_pretrained(os.path.join(this_results_dir, f\"epoch{epoch}\"))    \n",
    "\n",
    "print(f\"({global_rank}) Done training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Save model\n",
    "if global_rank == 0:\n",
    "    print(f\"({global_rank}) Saving model...\")\n",
    "    ppo_trainer.save_pretrained(this_results_dir)        \n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel",
   "language": "python",
   "name": "travel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
