{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir(\"/nfs/turbo/coe-chaijy/sstorks/simulation_informed_pcr4nlu/TRAVEl\")\n",
    "from travel import init_travel\n",
    "init_travel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a <mask> of a <mask> <mask> <mask>, where a <mask> is <mask> next to the <mask>. The <mask> is <mask> with various <mask>, <mask> a <mask>, a <mask>, a <mask>, and a <mask>. The <mask> is <mask> a <mask>, possibly <mask> to <mask> the <mask> or <mask> it. The <mask> also <mask> a <mask> and a <mask> of <mask>, which might be <mask>d for <mask> or <mask> <mask> in the <mask>.', \"This is a <mask> of a <mask> <mask> purple <mask> and <mask> a <mask> of <mask>. The <mask> is <mask> in a grassy <mask>, possibly in a <mask> or a <mask>. The <mask> are open, and the <mask> <mask> to be <mask> to <mask> something, possibly a <mask> or a <mask> of <mask>. The <mask> <mask> the <mask>'s <mask> on the <mask> at <mask>.\", 'This is a <mask> of a <mask> <mask> on a <mask>en <mask>, possibly in a <mask> or a <mask>. He is <mask> a <mask>en <mask> or a <mask>, possibly a <mask> of <mask> or a <mask>, and <mask> to be <mask> or <mask> on the <mask>. The <mask> is <mask> on the <mask>, and there is a <mask> nearby, possibly a <mask> or a <mask> <mask>. The <mask> <mask> that the <mask> is either <mask> or <mask> the <mask>en <mask>.', 'This is a <mask> of a <mask> <mask> where a <mask> is <mask> <mask>. There <mask> several <mask> in various <mask> of <mask>, with some of them being <mask> and <mask> being <mask> on a <mask>. The <mask> <mask> <mask> out on the <mask>, and the <mask> is <mask> on the <mask>-<mask> <mask>. The <mask> is <mask> with a <mask> and an <mask>, which <mask> essential <mask> for the <mask>-<mask> <mask>.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 14.09it/s]\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 14.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'consistency': 0.034,\n",
      " 'informativeness': 0.475,\n",
      " 'metrics_by_example': {0: {'consistency': [array([0.0001905], dtype=float16)],\n",
      "                            'informativeness': [array([0.03001], dtype=float16)],\n",
      "                            'relevance': [array([0.006348], dtype=float16)]},\n",
      "                        1: {'consistency': [array([0.0004964], dtype=float16)],\n",
      "                            'informativeness': [array([0.00574], dtype=float16)],\n",
      "                            'relevance': [array([0.0864], dtype=float16)]},\n",
      "                        2: {'consistency': [array([0.0405], dtype=float16)],\n",
      "                            'informativeness': [array([0.9116], dtype=float16)],\n",
      "                            'relevance': [array([0.04443], dtype=float16)]},\n",
      "                        3: {'consistency': [array([0.0962], dtype=float16)],\n",
      "                            'informativeness': [array([0.9517], dtype=float16)],\n",
      "                            'relevance': [array([0.1011], dtype=float16)]}},\n",
      " 'relevance': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from travel.model.metrics import consistency_metrics_caption\n",
    "from pprint import pprint\n",
    "\n",
    "procedure_descriptions = [\n",
    "    \"Put the spatula on the cutting board\",\n",
    "    \"Trim the fence branches with a lopper shears\",\n",
    "    \"Cut the carrot stem with the knife\",\n",
    "    \"Pick the half of the dough from the table with your hands\",\n",
    "]\n",
    "\n",
    "captions = [\n",
    "    [\"This is a photo of a bathroom sink area, where a person is standing next to the sink. The sink is filled with various items, including a cup, a bottle, a spoon, and a bowl. The person is holding a cup, possibly preparing to use the sink or cleaning it. The scene also includes a knife and a pair of scissors, which might be used for cutting or preparing items in the bathroom.\"],\n",
    "    [\"This is a photo of a person wearing purple gloves and holding a pair of scissors. The person is standing in a grassy area, possibly in a park or a garden. The scissors are open, and the person appears to be preparing to cut something, possibly a plant or a piece of fabric. The scene captures the person's focus on the task at hand.\"],\n",
    "    [\"This is a photo of a man working on a wooden floor, possibly in a kitchen or a room. He is holding a wooden stick or a plank, possibly a piece of wood or a tool, and appears to be measuring or working on the floor. The man is standing on the floor, and there is a tool nearby, possibly a drill or a measuring device. The scene suggests that the man is either installing or repairing the wooden floor.\"],\n",
    "    [\"This is a photo of a kitchen scene where a person is preparing doughnuts. There are several doughnuts in various stages of preparation, with some of them being shaped and others being placed on a tray. The doughnuts are spread out on the counter, and the person is working on the doughnut-making process. The kitchen is equipped with a sink and an oven, which are essential tools for the doughnut-making process.\"],\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    False,\n",
    "    False,\n",
    "    True,\n",
    "    True\n",
    "]\n",
    "\n",
    "metrics = consistency_metrics_caption(captions, procedure_descriptions, labels, list(range(len(procedure_descriptions))), [[0.0]] * len(procedure_descriptions))\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 14.91it/s]\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.92it/s]\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8])\n",
      "torch.Size([8])\n",
      "torch.Size([8])\n",
      "{'consistency': 0.5,\n",
      " 'informativeness': 0.771,\n",
      " 'metrics_by_output': {0: {'consistency': [0.893, 0.002],\n",
      "                           'informativeness': [0.984, 0.053],\n",
      "                           'relevance': [0.908, 0.046]},\n",
      "                       1: {'consistency': [0.291, 0.35],\n",
      "                           'informativeness': [0.611, 0.603],\n",
      "                           'relevance': [0.476, 0.581]},\n",
      "                       2: {'consistency': [0.302, 0.367],\n",
      "                           'informativeness': [0.983, 0.938],\n",
      "                           'relevance': [0.308, 0.392]},\n",
      "                       3: {'consistency': [0.942, 0.857],\n",
      "                           'informativeness': [0.998, 0.996],\n",
      "                           'relevance': [0.944, 0.861]}},\n",
      " 'relevance': 0.564}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from travel.data.vqg import VQGOutputs\n",
    "from travel.model.metrics import consistency_metrics_vqg\n",
    "from pprint import pprint\n",
    "\n",
    "procedure_descriptions = [\n",
    "    \"Put the spatula on the cutting board\",\n",
    "    \"Trim the fence branches with a lopper shears\",\n",
    "    \"Cut the carrot stem with the knife\",\n",
    "    \"Pick the half of the dough from the table with your hands\",\n",
    "]\n",
    "\n",
    "vqg_outputs = {\n",
    "    0: VQGOutputs(\n",
    "        procedure_id=0,\n",
    "        procedure_description=\"Put the spatula on the cutting board\",\n",
    "        questions=[\n",
    "            \"Is the spatula on the cutting board?\",\n",
    "            \"Is the cutting board empty?\",\n",
    "        ],\n",
    "        answers_str=[\"Yes\", \"No\"],\n",
    "    ),\n",
    "    1: VQGOutputs(\n",
    "        procedure_id=1,\n",
    "        procedure_description=\"Trim the fence branches with a lopper shears\",\n",
    "        questions=[\n",
    "            \"Are there branches?\",\n",
    "            \"Are the lopper shears rusty?\",\n",
    "        ],\n",
    "        answers_str=[\"Yes\", \"No\"],\n",
    "    ),\n",
    "    2: VQGOutputs(\n",
    "        procedure_id=2,\n",
    "        procedure_description=\"Cut the carrot stem with the knife\",\n",
    "        questions=[\n",
    "            \"Is the carrot in the bag?\",\n",
    "            \"Is the knife clean?\",\n",
    "        ],\n",
    "        answers_str=[\"Yes\", \"Yes\"],\n",
    "    ),\n",
    "    3: VQGOutputs(\n",
    "        procedure_id=3,\n",
    "        procedure_description=\"Pick the half of the dough from the table with your hands\",\n",
    "        questions=[\n",
    "            \"Is half of the dough in someone's hands?\",\n",
    "            \"Is half of the dough on the table?\",\n",
    "        ],\n",
    "        answers_str=[\"Yes\", \"Yes\"]\n",
    "    )\n",
    "}\n",
    "\n",
    "labels = [\n",
    "    False,\n",
    "    False,\n",
    "    True,\n",
    "    True\n",
    "]\n",
    "\n",
    "procedure_ids = [\n",
    "    0, 1, 2, 3\n",
    "]\n",
    "\n",
    "metrics = consistency_metrics_vqg(vqg_outputs, labels, procedure_ids)\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_old = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'consistency': 0.465,\n",
      " 'informativeness': 0.626,\n",
      " 'metrics_by_output': {0: {'consistency': [0.956, 0.003],\n",
      "                           'informativeness': [0.984, 0.053],\n",
      "                           'relevance': [0.972, 0.06]},\n",
      "                       1: {'consistency': [0.358, 0.285],\n",
      "                           'informativeness': [0.611, 0.603],\n",
      "                           'relevance': [0.585, 0.472]},\n",
      "                       2: {'consistency': [0.12, 0.248],\n",
      "                           'informativeness': [0.355, 0.53],\n",
      "                           'relevance': [0.339, 0.468]},\n",
      "                       3: {'consistency': [0.918, 0.832],\n",
      "                           'informativeness': [0.959, 0.914],\n",
      "                           'relevance': [0.957, 0.91]}},\n",
      " 'relevance': 0.595}\n"
     ]
    }
   ],
   "source": [
    "pprint(metrics_old)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 7/31/2024 - 8/1/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from travel.model.nli import NLI_HYPOTHESIS_TEMPLATE\n",
    "\n",
    "procedure = \"In a bowl, add the cut cherry tomatoes\"\n",
    "hypothesis = NLI_HYPOTHESIS_TEMPLATE.format(procedure=procedure)\n",
    "candidate_question_sets = [\n",
    "    ['Are there cherry tomatoes in the bowl?', # Yes\n",
    "     'Are there any cherry tomatoes that are not in the bowl?'], # No\n",
    "    ['Are the tomatoes in the bowl?', # Yes\n",
    "     'Are there any tomatoes that are not in the bowl?'], # No\n",
    "    ['Is there a bowl?', # Yes\n",
    "     'Is there a cut cherry tomato in the bowl?'], # Yes\n",
    "    ['Are the cherry tomatoes in a bowl?', # Yes\n",
    "     \"Is the bowl in someone's hand?\"] # Yes\n",
    "]\n",
    "\n",
    "questions_as_statements_expected = [\n",
    "    [\n",
    "        \"There are cherry tomatoes in the bowl.\",\n",
    "        \"There are not any cherry tomatoes that are not in the bowl.\",\n",
    "    ],\n",
    "    [\n",
    "        \"The tomatoes are in the bowl.\",\n",
    "        \"There are not any tomatoes that are not in the bowl.\",\n",
    "    ],\n",
    "    [\n",
    "        \"There is a bowl.\",\n",
    "        \"There is a cut cherry tomato in the bowl.\"\n",
    "    ],\n",
    "    [\n",
    "        \"There are cherry tomatoes in a bowl.\",\n",
    "        \"The bowl is in someone's hand.\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "questions_as_statements_unexpected = [\n",
    "    [\n",
    "        \"There are not cherry tomatoes in the bowl.\",\n",
    "        \"There are cherry tomatoes that are not in the bowl.\",\n",
    "    ],\n",
    "    [\n",
    "        \"The tomatoes are not in the bowl.\",\n",
    "        \"There are tomatoes that are not in the bowl.\",\n",
    "    ],\n",
    "    [\n",
    "        \"There is not a bowl.\",\n",
    "        \"There is not a cut cherry tomato in the bowl.\"\n",
    "    ],\n",
    "    [\n",
    "        \"There are not cherry tomatoes in a bowl.\",\n",
    "        \"The bowl is not in someone's hand.\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sstorks/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import spacy\n",
    "\n",
    "from travel.model.nli import NLI_MODEL_PATH\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(NLI_MODEL_PATH, quantization_config=bnb_config)\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(NLI_MODEL_PATH)\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9897]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2544]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from travel.model.nli import run_nli\n",
    "\n",
    "def get_entailment_probability(premise, hypothesis):\n",
    "    probs_expected = run_nli(nli_tokenizer, nli_model, list(zip(premise, hypothesis)))\n",
    "    return probs_expected[:, 1].numpy()\n",
    "\n",
    "print(get_entailment_probability([\"There is a puppy.\"], [\"There is a dog.\"]))\n",
    "print(get_entailment_probability([\"There is a puppy.\"], [\"There is a kitten.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['There are cherry tomatoes in the bowl.',\n",
      " 'There are not any cherry tomatoes that are not in the bowl.']\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.21it/s]\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance (negation):\n",
      "array([0.943 , 0.9185], dtype=float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 14.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance (mask):\n",
      "array([0.5347, 0.1262], dtype=float16)\n",
      "Informativeness (expected):\n",
      "array([0.977, 0.928], dtype=float16)\n",
      "Informativeness (unexpected):\n",
      "array([0.966, 0.99 ], dtype=float16)\n",
      "\n",
      "1\n",
      "['The tomatoes are in the bowl.',\n",
      " 'There are not any tomatoes that are not in the bowl.']\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.20it/s]\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance (negation):\n",
      "array([0.986, 0.829], dtype=float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance (mask):\n",
      "array([0.671 , 0.1438], dtype=float16)\n",
      "Informativeness (expected):\n",
      "array([0.9946, 0.8403], dtype=float16)\n",
      "Informativeness (unexpected):\n",
      "array([0.991, 0.989], dtype=float16)\n",
      "\n",
      "2\n",
      "['There is a bowl.', 'There is a cut cherry tomato in the bowl.']\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.86it/s]\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance (negation):\n",
      "array([0.5586, 0.95  ], dtype=float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance (mask):\n",
      "array([0.4731, 0.3987], dtype=float16)\n",
      "Informativeness (expected):\n",
      "array([0.5605, 0.968 ], dtype=float16)\n",
      "Informativeness (unexpected):\n",
      "array([0.998 , 0.9824], dtype=float16)\n",
      "\n",
      "3\n",
      "['There are cherry tomatoes in a bowl.', \"The bowl is in someone's hand.\"]\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.69it/s]\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance (negation):\n",
      "array([0.96  , 0.5444], dtype=float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance (mask):\n",
      "array([0.3853, 0.4224], dtype=float16)\n",
      "Informativeness (expected):\n",
      "array([0.9707, 0.628 ], dtype=float16)\n",
      "Informativeness (unexpected):\n",
      "array([0.9893, 0.9165], dtype=float16)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "from travel.model.metrics import mask_verbs_and_nouns\n",
    "\n",
    "for idx, (qe, qu) in enumerate(zip(questions_as_statements_expected, questions_as_statements_unexpected)):\n",
    "    print(idx)\n",
    "    pprint(qe)\n",
    "    print(\"==========\")\n",
    "    \n",
    "    probs_e = get_entailment_probability(qe, [hypothesis] * 2)\n",
    "    probs_u = get_entailment_probability(qu, [hypothesis] * 2)\n",
    "    \n",
    "    negation_based_relevance = np.abs(probs_e - probs_u)\n",
    "    print(\"Relevance (negation):\")\n",
    "    pprint(negation_based_relevance)\n",
    "    \n",
    "    mask_based_relevance = get_entailment_probability([mask_verbs_and_nouns(t, nlp, nli_tokenizer.mask_token) for t in qe], [hypothesis] * 2)\n",
    "    print(\"Relevance (mask):\")\n",
    "    pprint(mask_based_relevance)\n",
    "    \n",
    "    print(\"Informativeness (expected):\")\n",
    "    pprint(probs_e)\n",
    "    \n",
    "    print(\"Informativeness (unexpected):\")\n",
    "    pprint(1.0 - probs_u)\n",
    "    \n",
    "    print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from travel.model.nli import NLI_HYPOTHESIS_TEMPLATE\n",
    "\n",
    "# Look into how additional information might come together for \"sufficiency\" metric\n",
    "\n",
    "statement1 = \"There is a bowl.\"\n",
    "statement2 = \"There are cherry tomatoes in the bowl.\"\n",
    "# statement3 = \"There are not cherry tomatoes outside of the bowl.\"\n",
    "statement3 = \"The bowl is empty.\"\n",
    "\n",
    "\n",
    "statements = [\n",
    "    statement1,\n",
    "    statement1 + \" \" + statement2,\n",
    "    statement1 + \" \" + statement2 + \" \" + statement3,\n",
    "]\n",
    "\n",
    "procedure = \"In a bowl, add the cut cherry tomatoes\"\n",
    "hypothesis = NLI_HYPOTHESIS_TEMPLATE.format(procedure=procedure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([5.645e-01, 8.931e-01, 6.628e-04], dtype=float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "probs = get_entailment_probability(statements, [hypothesis]*3)\n",
    "pprint(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1->2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4946]\n",
      "\n",
      "12->3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.964]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"1->2\")\n",
    "probs = get_entailment_probability([statement1], [statement2])\n",
    "print(probs)\n",
    "print(\"\")\n",
    "\n",
    "print(\"12->3\")\n",
    "probs = get_entailment_probability([statement1 + \" \" + statement2], [statement3])\n",
    "print(probs)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 8/14/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "/home/sstorks/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import spacy\n",
    "\n",
    "from travel.model.nli import NLI_MODEL_PATH\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(NLI_MODEL_PATH, quantization_config=bnb_config)\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(NLI_MODEL_PATH)\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from travel.model.nli import run_nli\n",
    "\n",
    "def get_entailment_probability(premise, hypothesis):\n",
    "    probs_expected = run_nli(nli_tokenizer, nli_model, list(zip(premise, hypothesis)))\n",
    "    return probs_expected[:, 1].numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 16.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09393 0.7163 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "probs = get_entailment_probability(\n",
    "    [\"The person is not holding a plate in their hand.\", \"The person is holding a plate in their hand.\"], \n",
    "    ['The procedure \"Pick a plate\" has been successfully executed.', 'The procedure \"Pick a plate\" has been successfully executed.']\n",
    ")\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5505005020028562, 0.13939019498439031]\n",
      "0.6226\n",
      "0.5505005020028562\n",
      "0.34271881838556717\n"
     ]
    }
   ],
   "source": [
    "from travel.model.metrics import entropy\n",
    "import numpy as np\n",
    "\n",
    "ents = [1.0 - entropy(p) for p in probs]\n",
    "relevance = np.abs(probs[0] - probs[1])\n",
    "info = np.max(ents)\n",
    "\n",
    "print(ents)\n",
    "print(relevance)\n",
    "print(info)\n",
    "print(relevance * info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 8/15/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Messing with variations in NLI depending on batch size, padding, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "/home/sstorks/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import spacy\n",
    "\n",
    "from travel.model.nli import NLI_MODEL_PATH\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(NLI_MODEL_PATH, quantization_config=bnb_config)\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(NLI_MODEL_PATH)\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 15.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9609e-01, 3.7212e-03],\n",
      "        [4.2139e-01, 5.7861e-01],\n",
      "        [1.5833e-01, 8.4180e-01],\n",
      "        [1.0431e-04, 1.0000e+00],\n",
      "        [9.8975e-01, 1.0406e-02],\n",
      "        [5.0098e-01, 4.9902e-01],\n",
      "        [7.0215e-01, 2.9761e-01],\n",
      "        [9.9707e-01, 2.7103e-03]], dtype=torch.float16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from travel.model.nli import run_nli\n",
    "from pprint import pprint\n",
    "\n",
    "premise_hypothesis_pairs = [\n",
    "    (\"There is no paper.\", 'The procedure \"Take the paper\" has been successfully executed.'),\n",
    "    (\"There is paper.\", 'The procedure \"Take the paper\" has been successfully executed.'),    \n",
    "    (\"The refrigerator is open.\", 'The procedure \"Close the refrigerator\" has been successfully executed.'),\n",
    "    (\"The refrigerator is closed.\", 'The procedure \"Close the refrigerator\" has been successfully executed.'),\n",
    "    (\"There is no dish in the image.\", 'The procedure \"Pick a dish\" has been successfully executed.'),\n",
    "    (\"There is a dish in the image.\", 'The procedure \"Pick a dish\" has been successfully executed.'),\n",
    "    (\"There is a long pointy pencil in the desk drawer.\", 'The procedure \"Use the pencil to draw a stick figure\" has been successfully executed.'),\n",
    "    (\"There is not a long pointy pencil in the desk drawer.\", 'The procedure \"Use the pencil to draw a stick figure\" has been successfully executed.'),\n",
    "]\n",
    "results = run_nli(nli_tokenizer, nli_model, premise_hypothesis_pairs)\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9609e-01, 3.7212e-03],\n",
      "        [4.2139e-01, 5.7861e-01],\n",
      "        [1.5833e-01, 8.4180e-01],\n",
      "        [1.0431e-04, 1.0000e+00],\n",
      "        [9.8975e-01, 1.0406e-02],\n",
      "        [5.0098e-01, 4.9902e-01]], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "results1 = results\n",
    "pprint(results1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9609e-01, 3.7212e-03],\n",
      "        [4.2139e-01, 5.7861e-01],\n",
      "        [1.5833e-01, 8.4180e-01],\n",
      "        [1.0431e-04, 1.0000e+00],\n",
      "        [9.8975e-01, 1.0406e-02],\n",
      "        [5.0098e-01, 4.9902e-01],\n",
      "        [7.0215e-01, 2.9761e-01],\n",
      "        [9.9707e-01, 2.7103e-03]], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "results2 = results\n",
    "pprint(results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aaa4682ba2544b2ba58d0af767d1f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "\n",
    "VLM_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "vlm = AutoModelForVision2Seq.from_pretrained(VLM_NAME, \n",
    "                                            quantization_config=bnb_config)\n",
    "vlm_processor = AutoProcessor.from_pretrained(VLM_NAME)\n",
    "vlm_processor.tokenizer.padding_side = \"left\"\n",
    "vlm_processor.tokenizer.pad_token_id = vlm_processor.tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running generation (cuda:0): 100%|██████████| 1/1 [00:02<00:00,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' The fridge is opened.', ' The fridge is not opened.', ' There is a fridge in the image.', ' There is no fridge in the image.', ' There is a dog under the chair next to the desk behind the fridge in the kitchen.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from travel.model.metrics import rephrase_question_answer\n",
    "\n",
    "rep = rephrase_question_answer(\n",
    "    [\"Is the fridge opened?\", \n",
    "     \"Is the fridge opened?\", \n",
    "     \"Is there a fridge somewhere in the image?\", \n",
    "     \"Is there a fridge somewhere in the image?\",\n",
    "     \"Is there a dog under the chair next to the desk behind the fridge in the kitchen?\"],\n",
    "    [\"Yes\", \"No\", \"Yes\", \"No\", \"Yes\"],\n",
    "    vlm_processor.tokenizer,\n",
    "    vlm.language_model,\n",
    ")\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' The fridge is opened.']\n"
     ]
    }
   ],
   "source": [
    "rep1 = rep\n",
    "print(rep1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' The fridge is opened.', ' The fridge is not opened.', ' There is a fridge in the image.', ' There is no fridge in the image.']\n"
     ]
    }
   ],
   "source": [
    "rep2 = rep\n",
    "print(rep2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8/16/2024\n",
    "\n",
    "Checking why informativeness is sometimes not the same as informativeness_marginal when there are no previous questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "/home/sstorks/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import spacy\n",
    "\n",
    "from travel.model.nli import NLI_MODEL_PATH\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(NLI_MODEL_PATH, quantization_config=bnb_config)\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(NLI_MODEL_PATH)\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6687188f988241788c5efe79ea33d286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "\n",
    "VLM_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "vlm = AutoModelForVision2Seq.from_pretrained(VLM_NAME, \n",
    "                                            quantization_config=bnb_config)\n",
    "vlm_processor = AutoProcessor.from_pretrained(VLM_NAME)\n",
    "vlm_processor.tokenizer.padding_side = \"left\"\n",
    "vlm_processor.tokenizer.pad_token_id = vlm_processor.tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running generation (cuda:0): 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "running generation (cuda:0): 100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 20.76it/s]\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 20.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([0.02783], dtype=float16)\n",
      "tensor([[0.5977, 0.4021]], dtype=torch.float16)\n",
      "[\"The person's hands are visible in the photo.\"]\n",
      "[\"The person's hands are not visible in the photo.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running generation (cuda:0): 0it [00:00, ?it/s]\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 20.71it/s]\n",
      "running NLI (cuda:0): 100%|██████████| 1/1 [00:00<00:00, 20.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'informativeness': [0.027832],\n",
      " 'informativeness_marginal': [0.027832],\n",
      " 'informativeness_marginal_x_relevance_marginal': [0.010178],\n",
      " 'relevance': [0.365723],\n",
      " 'relevance_marginal': [0.365723],\n",
      " 'rephrased_questions_no': [\"The person's hands are not visible in the photo.\"],\n",
      " 'rephrased_questions_yes': [\"The person's hands are visible in the photo.\"]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from travel.model.metrics import question_coherence_metrics_nli\n",
    "from pprint import pprint\n",
    "\n",
    "metrics = question_coherence_metrics_nli(\n",
    "    nli_tokenizer,\n",
    "    nli_model,\n",
    "    vlm_processor.tokenizer,\n",
    "    vlm.language_model,\n",
    "    answers=[\"Yes\"],\n",
    "    procedures=[\"Cut a portion of the concrete from a heap of concrete with your hands\"],\n",
    "    questions=[\"Is the person's hands visible in the photo?\"],\n",
    "    previous_questions=[[]],\n",
    "    previous_answers=[[]],\n",
    ")\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel",
   "language": "python",
   "name": "travel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
