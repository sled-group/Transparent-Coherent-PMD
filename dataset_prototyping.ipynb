{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CACHE_DIR = \"/scratch/chaijy_root/chaijy0/sstorks/.cache/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CaptainCook4D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPIC KITCHENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# PARTITION = \"validation\"\n",
    "PARTITION = \"train\"\n",
    "EK_ANNOTATION_PATH = f\"/nfs/turbo/coe-chaijy/datasets/EPIC-KITCHENS/annotations/EPIC_100_{PARTITION}_full_sent.csv\"\n",
    "annotations = pd.read_csv(EK_ANNOTATION_PATH)\n",
    "EK_55_VIDEO_PATH = f\"/nfs/turbo/coe-chaijy/datasets/EPIC-KITCHENS/55\"\n",
    "EK_100_VIDEO_PATH = f\"/nfs/turbo/coe-chaijy/datasets/EPIC-KITCHENS/100\"\n",
    "\n",
    "\n",
    "# # Load visual annotations\n",
    "# NOUN_CLASSES_PATH = \"/nfs/turbo/coe-chaijy/datasets/EPIC-KITCHENS-VISOR/2v6cgv1x04ol22qp9rm9x2j6a7/EPIC_100_noun_classes_v2.csv\"\n",
    "# EK_VISUAL_ANNOTATION_PATH = f\"/nfs/turbo/coe-chaijy/datasets/EPIC-KITCHENS-VISOR/2v6cgv1x04ol22qp9rm9x2j6a7/GroundTruth-SparseAnnotations/annotations/{PARTITION}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 recipe, 2 video clips formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect lists of clips for all videos in EPIC KITCHENS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67217/67217 [00:05<00:00, 12288.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495 videos collected from EPIC KITCHENS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "structured_action_lists = defaultdict(dict)\n",
    "\n",
    "for row_idx, row in tqdm(annotations.iterrows(), total=len(annotations)):\n",
    "    \n",
    "    participant_index, video_index, clip_index = row['narration_id'].split('_')\n",
    "    video_index = participant_index + \"_\" + str(video_index)\n",
    "    clip_index = int(clip_index)\n",
    "\n",
    "    narration_text = row['full_sent_narration']\n",
    "    \n",
    "    start_timestamp = row['start_timestamp']\n",
    "    start_timestamp = datetime.strptime(start_timestamp, \"%H:%M:%S.%f\")\n",
    "    stop_timestamp = row['stop_timestamp']\n",
    "    stop_timestamp = datetime.strptime(stop_timestamp, \"%H:%M:%S.%f\")\n",
    "    action_seconds = stop_timestamp - start_timestamp\n",
    "    action_seconds = action_seconds.total_seconds()\n",
    "    \n",
    "    verb = row['verb']\n",
    "    noun = row['noun'] # TODO: \"all_nouns\" key gives access to other noun participants where applicable\n",
    "\n",
    "    structured_action_lists[video_index][clip_index] = {\n",
    "        \"video_id\": video_index,\n",
    "        \"clip_index\": clip_index,\n",
    "        \"verb\": verb,\n",
    "        \"noun\": noun,\n",
    "        \"all_nouns\": eval(row[\"all_nouns\"]),\n",
    "        \"narration_text\": narration_text,\n",
    "        \"action_seconds\": action_seconds,\n",
    "        \"start_timestamp\": row['start_timestamp'],\n",
    "        \"stop_timestamp\": row['stop_timestamp'],\n",
    "    }\n",
    "    \n",
    "print(f\"{len(structured_action_lists)} videos collected from EPIC KITCHENS\")\n",
    "\n",
    "# smooth out to a dict of lists\n",
    "for video_id in structured_action_lists:\n",
    "    min_clip_id = min(list(structured_action_lists[video_id].keys()))\n",
    "    max_clip_id = max(list(structured_action_lists[video_id].keys()))\n",
    "    \n",
    "    new_list = []\n",
    "    last_action = None\n",
    "    for clip_id in range(min_clip_id, max_clip_id+1):\n",
    "        \n",
    "        if clip_id not in structured_action_lists[video_id]:\n",
    "            continue\n",
    "        \n",
    "        current_action = structured_action_lists[video_id][clip_id]\n",
    "        new_list.append(current_action)\n",
    "        \n",
    "    structured_action_lists[video_id] = new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve clips for TRAVEl examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 clips retrieved from train partition of EK-100\n",
      "In videos: ['P06_106']\n",
      "{'start_timestamp': '00:01:10.89',\n",
      " 'stop_timestamp': '00:01:14.72',\n",
      " 'video_id': 'P06_106'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "query_verb = \"roll\"\n",
    "query_nouns = {\"tortilla\",}\n",
    "\n",
    "retrieved_clips = []\n",
    "for video in structured_action_lists:\n",
    "    for clip in structured_action_lists[video]:\n",
    "        if clip['verb'] == query_verb and set(clip['all_nouns']) == set(query_nouns):\n",
    "            retrieved_clips.append(clip)\n",
    "\n",
    "print(f\"{len(retrieved_clips)} clips retrieved from {PARTITION} partition of EK-100\")\n",
    "videos_to_look_at = list(set([clip['video_id'] for clip in retrieved_clips]))\n",
    "print(\"In videos:\", videos_to_look_at)\n",
    "\n",
    "for clip in retrieved_clips:\n",
    "    pprint(\n",
    "        {\n",
    "            \"video_id\": clip['video_id'],\n",
    "            # \"clip_index\": clip['clip_index'],\n",
    "            # \"verb\": clip['verb'],\n",
    "            # \"all_nouns\": clip['all_nouns'],\n",
    "            \"start_timestamp\": clip['start_timestamp'],\n",
    "            \"stop_timestamp\": clip['stop_timestamp'],            \n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve videos for clips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from video_blip.data.epic_kitchens import EpicKitchensDataset\n",
    "\n",
    "dataset = EpicKitchensDataset(EK_ANNOTATION_PATH,\n",
    "                              EK_55_VIDEO_PATH,\n",
    "                              EK_100_VIDEO_PATH,\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all_noun_classes': '[3]',\n",
      " 'all_nouns': \"['cupboard']\",\n",
      " 'aug_index': 0,\n",
      " 'clip_index': 0,\n",
      " 'full_sent_narration': 'The camera wearer opens the cupboard.',\n",
      " 'narration': 'open cupboard',\n",
      " 'narration_id': 'P25_106_0',\n",
      " 'narration_timestamp': '00:00:03.115',\n",
      " 'narration_timestamp_sec': 3.115,\n",
      " 'noun': 'cupboard',\n",
      " 'noun_class': '3',\n",
      " 'participant_id': 'P25',\n",
      " 'start_frame': '111',\n",
      " 'start_timestamp': '00:00:02.22',\n",
      " 'stop_frame': '177',\n",
      " 'stop_timestamp': '00:00:03.55',\n",
      " 'verb': 'open',\n",
      " 'verb_class': '3',\n",
      " 'video': tensor([[[[ 87,  87,  88,  ..., 128, 129, 129],\n",
      "          [ 87,  88,  88,  ..., 129, 129, 129],\n",
      "          [ 88,  88,  89,  ..., 129, 129, 129],\n",
      "          ...,\n",
      "          [ 41,  41,  41,  ..., 163, 163, 163],\n",
      "          [ 41,  41,  41,  ..., 162, 163, 163],\n",
      "          [ 41,  41,  41,  ..., 162, 163, 163]],\n",
      "\n",
      "         [[ 91,  92,  91,  ..., 128, 129, 129],\n",
      "          [ 92,  92,  92,  ..., 129, 129, 129],\n",
      "          [ 92,  92,  93,  ..., 129, 129, 129],\n",
      "          ...,\n",
      "          [ 41,  41,  41,  ..., 163, 163, 163],\n",
      "          [ 41,  41,  41,  ..., 162, 163, 163],\n",
      "          [ 41,  41,  41,  ..., 162, 163, 163]],\n",
      "\n",
      "         [[ 95,  95,  94,  ..., 128, 129, 129],\n",
      "          [ 95,  95,  95,  ..., 129, 129, 129],\n",
      "          [ 96,  96,  96,  ..., 129, 129, 129],\n",
      "          ...,\n",
      "          [ 41,  41,  41,  ..., 162, 163, 163],\n",
      "          [ 41,  41,  41,  ..., 162, 162, 162],\n",
      "          [ 41,  41,  41,  ..., 162, 162, 162]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[170, 169, 168,  ...,  76,  77,  79],\n",
      "          [169, 169, 169,  ...,  77,  77,  77],\n",
      "          [169, 169, 169,  ...,  78,  78,  76],\n",
      "          ...,\n",
      "          [ 28,  28,  28,  ..., 197, 197, 197],\n",
      "          [ 28,  28,  28,  ..., 197, 197, 197],\n",
      "          [ 28,  28,  28,  ..., 197, 197, 198]],\n",
      "\n",
      "         [[170, 169, 168,  ...,  69,  71,  72],\n",
      "          [169, 169, 168,  ...,  70,  71,  73],\n",
      "          [168, 168, 168,  ...,  71,  73,  75],\n",
      "          ...,\n",
      "          [ 28,  28,  28,  ..., 197, 197, 197],\n",
      "          [ 28,  28,  28,  ..., 197, 197, 197],\n",
      "          [ 28,  28,  28,  ..., 197, 197, 197]],\n",
      "\n",
      "         [[163, 164, 164,  ...,  67,  67,  68],\n",
      "          [163, 164, 164,  ...,  67,  67,  68],\n",
      "          [163, 164, 164,  ...,  67,  67,  68],\n",
      "          ...,\n",
      "          [ 26,  26,  26,  ..., 198, 198, 198],\n",
      "          [ 26,  26,  26,  ..., 198, 198, 198],\n",
      "          [ 26,  26,  26,  ..., 198, 198, 198]]],\n",
      "\n",
      "\n",
      "        [[[ 66,  66,  67,  ..., 118, 119, 119],\n",
      "          [ 66,  67,  67,  ..., 119, 119, 119],\n",
      "          [ 67,  67,  68,  ..., 119, 119, 119],\n",
      "          ...,\n",
      "          [ 30,  30,  30,  ..., 183, 183, 183],\n",
      "          [ 30,  30,  30,  ..., 184, 183, 183],\n",
      "          [ 30,  30,  30,  ..., 184, 183, 183]],\n",
      "\n",
      "         [[ 67,  68,  67,  ..., 118, 119, 119],\n",
      "          [ 68,  68,  68,  ..., 119, 119, 119],\n",
      "          [ 68,  68,  69,  ..., 119, 119, 119],\n",
      "          ...,\n",
      "          [ 30,  30,  30,  ..., 183, 183, 183],\n",
      "          [ 30,  30,  30,  ..., 184, 183, 183],\n",
      "          [ 30,  30,  30,  ..., 184, 183, 183]],\n",
      "\n",
      "         [[ 70,  70,  69,  ..., 118, 119, 119],\n",
      "          [ 70,  70,  70,  ..., 119, 119, 119],\n",
      "          [ 71,  71,  71,  ..., 119, 119, 119],\n",
      "          ...,\n",
      "          [ 30,  30,  30,  ..., 184, 183, 183],\n",
      "          [ 30,  30,  30,  ..., 184, 184, 184],\n",
      "          [ 30,  30,  30,  ..., 184, 184, 184]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[160, 159, 158,  ...,  68,  69,  71],\n",
      "          [159, 159, 159,  ...,  69,  69,  69],\n",
      "          [159, 159, 159,  ...,  68,  67,  65],\n",
      "          ...,\n",
      "          [ 11,  11,  11,  ..., 211, 211, 211],\n",
      "          [ 11,  11,  11,  ..., 211, 211, 211],\n",
      "          [ 11,  11,  11,  ..., 211, 211, 212]],\n",
      "\n",
      "         [[162, 161, 160,  ...,  59,  61,  62],\n",
      "          [161, 161, 160,  ...,  60,  61,  63],\n",
      "          [160, 160, 160,  ...,  61,  63,  65],\n",
      "          ...,\n",
      "          [ 11,  11,  11,  ..., 211, 211, 211],\n",
      "          [ 11,  11,  11,  ..., 211, 211, 211],\n",
      "          [ 11,  11,  11,  ..., 211, 211, 211]],\n",
      "\n",
      "         [[157, 158, 158,  ...,  57,  57,  58],\n",
      "          [157, 158, 158,  ...,  57,  57,  58],\n",
      "          [157, 158, 158,  ...,  57,  59,  60],\n",
      "          ...,\n",
      "          [ 11,  11,  11,  ..., 212, 212, 212],\n",
      "          [ 11,  11,  11,  ..., 212, 212, 212],\n",
      "          [ 11,  11,  11,  ..., 212, 212, 212]]],\n",
      "\n",
      "\n",
      "        [[[ 46,  46,  47,  ...,  97,  98,  98],\n",
      "          [ 46,  47,  47,  ...,  98,  98,  98],\n",
      "          [ 47,  47,  48,  ...,  98,  98,  98],\n",
      "          ...,\n",
      "          [ 18,  18,  18,  ..., 191, 191, 191],\n",
      "          [ 18,  18,  18,  ..., 189, 191, 191],\n",
      "          [ 18,  18,  18,  ..., 189, 191, 191]],\n",
      "\n",
      "         [[ 48,  49,  48,  ...,  97,  98,  98],\n",
      "          [ 49,  49,  49,  ...,  98,  98,  98],\n",
      "          [ 49,  49,  50,  ...,  98,  98,  98],\n",
      "          ...,\n",
      "          [ 18,  18,  18,  ..., 191, 191, 191],\n",
      "          [ 18,  18,  18,  ..., 189, 191, 191],\n",
      "          [ 18,  18,  18,  ..., 189, 191, 191]],\n",
      "\n",
      "         [[ 51,  51,  50,  ...,  97,  98,  98],\n",
      "          [ 51,  51,  51,  ...,  98,  98,  98],\n",
      "          [ 52,  52,  52,  ...,  98,  98,  98],\n",
      "          ...,\n",
      "          [ 18,  18,  18,  ..., 189, 191, 191],\n",
      "          [ 18,  18,  18,  ..., 189, 189, 189],\n",
      "          [ 18,  18,  18,  ..., 189, 189, 189]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[144, 143, 142,  ...,  51,  52,  54],\n",
      "          [143, 143, 143,  ...,  52,  52,  52],\n",
      "          [143, 143, 143,  ...,  54,  55,  53],\n",
      "          ...,\n",
      "          [  5,   5,   5,  ..., 205, 205, 205],\n",
      "          [  5,   5,   5,  ..., 205, 205, 205],\n",
      "          [  5,   5,   5,  ..., 205, 205, 206]],\n",
      "\n",
      "         [[147, 146, 145,  ...,  43,  45,  46],\n",
      "          [146, 146, 145,  ...,  44,  45,  47],\n",
      "          [145, 145, 145,  ...,  45,  47,  49],\n",
      "          ...,\n",
      "          [  5,   5,   5,  ..., 205, 205, 205],\n",
      "          [  5,   5,   5,  ..., 205, 205, 205],\n",
      "          [  5,   5,   5,  ..., 205, 205, 205]],\n",
      "\n",
      "         [[143, 144, 144,  ...,  41,  41,  42],\n",
      "          [143, 144, 144,  ...,  41,  41,  42],\n",
      "          [143, 144, 144,  ...,  43,  44,  45],\n",
      "          ...,\n",
      "          [  4,   4,   4,  ..., 206, 206, 206],\n",
      "          [  4,   4,   4,  ..., 206, 206, 206],\n",
      "          [  4,   4,   4,  ..., 206, 206, 206]]]], dtype=torch.uint8),\n",
      " 'video_id': 'P25_106',\n",
      " 'video_index': 360,\n",
      " 'video_name': 'P25_106.MP4'}\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    pprint(item)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel",
   "language": "python",
   "name": "travel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
