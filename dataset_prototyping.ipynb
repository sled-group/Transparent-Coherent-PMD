{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "CACHE_DIR = \"/scratch/chaijy_root/chaijy0/sstorks/.cache/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPIC KITCHENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"./VideoBLIP-internal\")\n",
    "\n",
    "from video_blip.data.epic_kitchens import EpicKitchensDataset\n",
    "\n",
    "PARTITION = \"validation\"\n",
    "# PARTITION = \"train\"\n",
    "EK_ANNOTATION_PATH = f\"/nfs/turbo/coe-chaijy/datasets/EPIC-KITCHENS/annotations/EPIC_100_{PARTITION}_full_sent.csv\"\n",
    "annotations = pd.read_csv(EK_ANNOTATION_PATH)\n",
    "\n",
    "# Load visual annotations\n",
    "NOUN_CLASSES_PATH = \"/nfs/turbo/coe-chaijy/datasets/EPIC-KITCHENS-VISOR/2v6cgv1x04ol22qp9rm9x2j6a7/EPIC_100_noun_classes_v2.csv\"\n",
    "EK_VISUAL_ANNOTATION_PATH = f\"/nfs/turbo/coe-chaijy/datasets/EPIC-KITCHENS-VISOR/2v6cgv1x04ol22qp9rm9x2j6a7/GroundTruth-SparseAnnotations/annotations/{PARTITION}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 recipe, 2 video clips formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "structured_action_lists = defaultdict(dict)\n",
    "\n",
    "for row_idx, row in tqdm(annotations.iterrows(), total=len(annotations)):\n",
    "    \n",
    "    participant_index, video_index, clip_index = row['narration_id'].split('_')\n",
    "    video_index = participant_index + \"_\" + str(video_index)\n",
    "    clip_index = int(clip_index)\n",
    "\n",
    "    narration_text = row['full_sent_narration']\n",
    "    \n",
    "    start_timestamp = row['start_timestamp']\n",
    "    start_timestamp = datetime.strptime(start_timestamp, \"%H:%M:%S.%f\")\n",
    "    stop_timestamp = row['stop_timestamp']\n",
    "    stop_timestamp = datetime.strptime(stop_timestamp, \"%H:%M:%S.%f\")\n",
    "    action_seconds = stop_timestamp - start_timestamp\n",
    "    action_seconds = action_seconds.total_seconds()\n",
    "    \n",
    "    verb = row['verb']\n",
    "    noun = row['noun'] # TODO: \"all_nouns\" key gives access to other noun participants where applicable\n",
    "\n",
    "    structured_action_lists[video_index][clip_index] = {\n",
    "        \"video_id\": video_index,\n",
    "        \"clip_index\": clip_index,\n",
    "        \"verb\": verb,\n",
    "        \"noun\": noun,\n",
    "        \"all_nouns\": eval(row[\"all_nouns\"]),\n",
    "        \"narration_text\": narration_text,\n",
    "        \"action_seconds\": action_seconds,\n",
    "        \"start_timestamp\": row['start_timestamp'],\n",
    "        \"stop_timestamp\": row['stop_timestamp'],\n",
    "    }\n",
    "    \n",
    "print(f\"{len(structured_action_lists)} videos collected from EPIC KITCHENS\")\n",
    "\n",
    "# smooth out to a dict of lists\n",
    "for video_id in structured_action_lists:\n",
    "    min_clip_id = min(list(structured_action_lists[video_id].keys()))\n",
    "    max_clip_id = max(list(structured_action_lists[video_id].keys()))\n",
    "    \n",
    "    new_list = []\n",
    "    last_action = None\n",
    "    for clip_id in range(min_clip_id, max_clip_id+1):\n",
    "        \n",
    "        if clip_id not in structured_action_lists[video_id]:\n",
    "            continue\n",
    "        \n",
    "        current_action = structured_action_lists[video_id][clip_id]\n",
    "        new_list.append(current_action)\n",
    "        \n",
    "    structured_action_lists[video_id] = new_list\n",
    "\n",
    "N_ACTIONS_PER_VIDEO = 5\n",
    "N_SECONDS_PER_CHUNK = 60\n",
    "travel_examples = []\n",
    "for video_id in structured_action_lists:\n",
    "        \n",
    "    time_so_far = 0.0\n",
    "    clips_so_far = []\n",
    "    for clip in structured_action_lists[video_id]:\n",
    "        \n",
    "        time_so_far += clip['action_seconds']\n",
    "        clips_so_far.append(clip)\n",
    "        actions_in_clip = set([(action['verb'], action['noun']) for action in clips_so_far])\n",
    "        \n",
    "        # Create a TRAVEl example for every 1min of video in EK\n",
    "        if time_so_far >= N_SECONDS_PER_CHUNK and len(actions_in_clip) >= N_ACTIONS_PER_VIDEO * 2:\n",
    "            \n",
    "            # Sample the 5 longest actions as the key actions for this video\n",
    "            key_actions = sorted(clips_so_far, key=lambda x: x['action_seconds'], reverse=True)\n",
    "\n",
    "            # Remove duplicates ordered by action length\n",
    "            key_actions.reverse()\n",
    "            remove_indices = []\n",
    "            for action_idx, action in enumerate(key_actions):\n",
    "                if (action['verb'], action['noun']) in [(comp_action['verb'], comp_action['noun']) for comp_action in key_actions[action_idx+1:]]:\n",
    "                    remove_indices.append(action_idx)\n",
    "                # Don't consider some actions as the key actions, e.g., look for, which is not so much a physical action affecting objects\n",
    "                elif action['verb'] in ['look-for', 'eat', 'drink']:\n",
    "                    remove_indices.append(action_idx)\n",
    "            remove_indices.reverse()\n",
    "            for action_idx in remove_indices:\n",
    "                del key_actions[action_idx]\n",
    "            key_actions.reverse()\n",
    "\n",
    "            key_actions = key_actions[:N_ACTIONS_PER_VIDEO]\n",
    "            key_actions = sorted(key_actions, key=lambda x: x['clip_index'], reverse=False)\n",
    "            \n",
    "            assert len(key_actions) == N_ACTIONS_PER_VIDEO, f\"Only collected {len(key_actions)} actions in this example!\"            \n",
    "            \n",
    "            min_clip_id = min([action['clip_index'] for action in clips_so_far])\n",
    "            max_clip_id = max([action['clip_index'] for action in clips_so_far])  \n",
    "            total_time = sum([clip['action_seconds'] for clip in clips_so_far]) / 60.0\n",
    "            example = {\n",
    "                \"video_id\": video_id,\n",
    "                \"key_clips\": key_actions,\n",
    "                \"start_clip_index\": min_clip_id,\n",
    "                \"end_clip_index\": max_clip_id,\n",
    "                \"all_clip_indices\": [clip['clip_index'] for clip in clips_so_far],\n",
    "                \"n_clips\": len(clips_so_far),\n",
    "                \"total_minutes\": total_time,\n",
    "                \"all_narrations\": [action['narration_text'].split('.')[0].strip() + \".\" for action in key_actions],\n",
    "            }\n",
    "            \n",
    "            # Compile annotated objects seen during this example\n",
    "            all_objects = []\n",
    "            for action in clips_so_far:\n",
    "                for noun in action['all_nouns']:\n",
    "                    all_objects.append(noun)\n",
    "            # for action in key_actions:\n",
    "            #     vis_ann_path = os.path.join(EK_VISUAL_ANNOTATION_PATH, f\"{video_id}.json\")\n",
    "            #     if os.path.exists(vis_ann_path):\n",
    "            #         vis_annotations = json.load(open(vis_ann_path, \"r\"))\n",
    "            #         for ann in vis_annotations['annotations']:\n",
    "            #             all_objects.append(ann[\"name\"])\n",
    "            #     else:\n",
    "            #         print(f\"Warning: Couldn't find visual annotations for video {video_id} clip {action['clip_index']}!\")\n",
    "            all_objects = list(set(all_objects))\n",
    "            example[\"scene_objects\"] = all_objects\n",
    "            \n",
    "            travel_examples.append(example)\n",
    "            \n",
    "            clips_so_far = []\n",
    "            time_so_far = 0.0\n",
    "\n",
    "average_clips = np.mean([example['n_clips'] for example in travel_examples])\n",
    "average_time = np.mean([example['total_minutes'] for example in travel_examples])\n",
    "print(f\"{len(travel_examples)} TRAVEl examples created\")\n",
    "print(f\"Average # action clip per examples: {average_clips}\")\n",
    "print(f\"Average time per example (minutes): {average_time}\")\n",
    "print(f\"Average time of an action (seconds): {average_time / average_clips * 60.0}\")\n",
    "pprint(travel_examples)\n",
    "\n",
    "# TODO: collect list of objects in clips from visual annotations? Seems VISOR data is incomplete - need to refer to Peter's code for epic kitchens\n",
    "# -- Should actually do this and maybe restrict data to only VISOR-annotated data - may want these masks later for our approach? Can also use them for grounding in Kosmos-2?\n",
    "# -- Maybe do this instead of over-sampling?\n",
    "# TODO: find a way to make examples include fewer clips? Or filter away less important actions?\n",
    "# -- We can cluster very quick actions together.\n",
    "# TODO: some actions next to each other are very similar or hypo/hypernyms of each other, e.g., \"wash glass\" -> \"rinse glass\". We should avoid this or cluster. \n",
    "# -- Maybe consecutive actions that exceed some threshold similarity should be clustered?\n",
    "# -- \"WayOf\" relations in ConceptNet can inform what actions should be clustered/ignored."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel",
   "language": "python",
   "name": "travel"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
