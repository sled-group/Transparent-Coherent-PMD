{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d89e332-8f41-4711-8649-f9e5ac3aa0ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Initial configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609598b0-4653-4b42-85ba-2a38a01e4300",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "177fd0ff-6cd5-4fcc-9e01-b3063bbe70b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0+cu117\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "print(torch.__version__)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "DATA_CACHE_DIR = \"cache_dir\"\n",
    "MODEL_CACHE_DIR = \"/scratch/chaijy_root/chaijy0/sstorks/.cache/huggingface\"\n",
    "os.environ['HF_HOME'] = MODEL_CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8388735e-8531-4f6d-a3fb-9426fd4871df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb  4 20:30:50 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A40                     On  | 00000000:23:00.0 Off |                    0 |\n",
      "|  0%   24C    P8              24W / 300W |      7MiB / 46068MiB |      0%   E. Process |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3979217a-1884-47da-b647-ea40a36ec49b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Utils\n",
    "\n",
    "Need to move these into external code files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d9a6ded-f32b-4a0f-a73b-433faf4a09dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VIDEO_DIR = \"/nfs/turbo/coe-chaijy-unreplicated/datasets/captaincook4d/data/captain_cook_4d/hololens/sync/pv\" # Directory containing CaptainCook4D mp4s\n",
    "ANNOTATIONS_DIR = \"/nfs/turbo/coe-chaijy-unreplicated/datasets/captaincook4d/annotations\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da263f28-4a27-4ced-a251-6bf10e3d71db",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Video Processing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cc4c178-6877-48b3-b63b-6dabba1694e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def get_video(video_path):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open video file\")\n",
    "    \n",
    "    return cap\n",
    "    # remember to call cap.release() later\n",
    "\n",
    "def extract_frames(cap, times):\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)  # Frames per second\n",
    "    frames = []\n",
    "\n",
    "    for t in times:\n",
    "        frame_number = int(t * fps)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if ret:\n",
    "            # Convert to RGB\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)            \n",
    "            frames.append(frame)\n",
    "        else:\n",
    "            print(f\"Warning: Frame at time {t} seconds could not be read.\")\n",
    "            frames.append(None)\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e0b055-155e-4677-8dd4-88a534d6a36e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Other Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c4d12f8-de7a-4660-8da5-10dc45d044ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_float_series(start, end, step):\n",
    "    # Ensure step is a positive float\n",
    "    step = abs(step)\n",
    "\n",
    "    # Initialize the series with the start value\n",
    "    series = [start]\n",
    "\n",
    "    # Generate numbers in the series\n",
    "    while start + step <= end:\n",
    "        start += step\n",
    "        series.append(start)\n",
    "\n",
    "    # Check if the end value is already in the series\n",
    "    if series[-1] != end:\n",
    "        series.append(end)\n",
    "\n",
    "    return series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b10717-ba26-4c79-8fd7-2e2ff9d77506",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data/Eval Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1f053a3-7d66-45c1-ada0-2c9cc3ff28dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from dataclasses_json import dataclass_json\n",
    "from typing import Optional\n",
    "from enum import Enum\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "ERROR_CATEGORIES = json.load(open(os.path.join(ANNOTATIONS_DIR, \"annotation_json/error_category_idx.json\"), \"r\"))\n",
    "\n",
    "# TODO: at some point make this more task-agnostic to reuse with other datasets - or just rename CaptainCook4DMistakeDetectionExample\n",
    "@dataclass\n",
    "class MistakeDetectionExample:\n",
    "     video_id: str\n",
    "     procedure_id: int\n",
    "     frames: list[Image]\n",
    "     frame_times: list[float]\n",
    "     procedure_description: str\n",
    "     mistake: bool\n",
    "     mistake_type: Optional[str] = None\n",
    "     mistake_description: Optional[str] = None\n",
    "\n",
    "class VQAResponse(Enum):\n",
    "    No = 0\n",
    "    Yes = 1\n",
    "\n",
    "@dataclass_json\n",
    "@dataclass\n",
    "class VQGOutputs:\n",
    "    \"\"\"Dataclass to hold all LM outputs from visual question generation (VQG).\"\"\"\n",
    "    procedure_id: int\n",
    "    procedure_description: str\n",
    "    target_object: str\n",
    "    # state_description: str\n",
    "    questions: list[str]\n",
    "    answers_str: list[str]\n",
    "    answers: list[VQAResponse] = field(default_factory=list)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validation steps to ensure every QA-pair is valid and every question has an answer.\"\"\"\n",
    "        for answer in self.answers_str:\n",
    "            try: \n",
    "                self.answers.append(VQAResponse[answer])\n",
    "            except:\n",
    "                raise ValueError(f\"Unrecognized VQA answer could not be accepted by VQAResponse class: {answer}\")\n",
    "            \n",
    "        assert len(self.questions) == len(self.answers), \"VQGOutputs received mismatched number of questions and answers.\"\n",
    "        \n",
    "        for question in self.questions:\n",
    "            if not question.strip().endswith(\"?\"):\n",
    "                print(f\"Warning: Question '{question}' doesn't appear to be a question.\")\n",
    "\n",
    "\n",
    "                \n",
    "# NOTE: below classes can't handle video-based pipeline yet                    \n",
    "\n",
    "@dataclass_json\n",
    "@dataclass\n",
    "class VQAOutputs:\n",
    "    \"\"\"Dataclass to hold all VLM outputs from visual question answering (VQA).\"\"\"\n",
    "    procedure_id: int\n",
    "    frame: Image\n",
    "    prompt: str\n",
    "    expected_answer: VQAResponse\n",
    "    response_token_ids: dict[VQAResponse, int]\n",
    "    logits: torch.FloatTensor # (# questions, vocab size) \n",
    "    answer_probs: dict[VQAResponse, float] = field(default_factory=list)\n",
    "    predicted_answer: VQAResponse = VQAResponse[\"No\"]\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Processes logits output from VLM into answer probabilities and final answer.\"\"\"\n",
    "        for response_type in VQAResponse:\n",
    "            assert response_type in self.response_token_ids, f\"VLM token ID for {response_type} not provided in VQAOutputs.answer_token_ids.\"\n",
    "\n",
    "        this_probs = torch.stack([self.logits[self.response_token_ids[response_type]] for response_type in VQAResponse], dim=0)\n",
    "        this_probs = torch.softmax(this_probs, dim=0)\n",
    "        \n",
    "        self.predicted_answer = VQAResponse(torch.argmax(this_probs, dim=0).numpy())\n",
    "        \n",
    "        this_probs = this_probs.numpy()\n",
    "        self.answer_probs = {response_type: this_probs[response_type.value] for response_type in VQAResponse}\n",
    "\n",
    "@dataclass_json\n",
    "@dataclass\n",
    "class MistakeDetectionEvaluator:\n",
    "    \"\"\"Superclass to implement different types of evaluators based on VQAOutputs.\"\"\"\n",
    "    examples: list[MistakeDetectionExample]\n",
    "    vqa_outputs: list[list[VQAOutputs]]\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        assert len(self.examples) == len(self.vqa_outputs), \"Should have same number of examples and VQAOutputs lists.\"\n",
    "        n_outputs_per_example = len(self.vqa_outputs[0])\n",
    "        for output in self.vqa_outputs:\n",
    "            assert len(output) == n_outputs_per_example, \"All examples should have same number of VQAOutputs.\"\n",
    "    \n",
    "    def check_mistake(self) -> list[bool]:\n",
    "        raise NotImplementedError(\"Subclass should define the strategy to check for mistakes.\")\n",
    "    \n",
    "    def get_mistake_detection_metrics(self) -> dict[str, float]:\n",
    "        \n",
    "        labels = [example.mistake for example in self.examples]\n",
    "        preds = self.check_mistakes()\n",
    "        \n",
    "        metrics = {}\n",
    "        metrics['accuracy'] = accuracy_score(labels, preds)\n",
    "        metrics['precision'] = precision_score(labels, preds)\n",
    "        metrics['recall'] = recall_score(labels, preds)\n",
    "        metrics['f1'] = f1_score(labels, preds)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def get_logits_errors(self) -> torch.FloatTensor:        \n",
    "        assert len(self.vqa_outputs[0][0].response_token_ids) == 2, \"get_logits_errors() depends on VQA responses being binary (e.g., yes or no). If there are more response classes, you cannot use this method.\"\n",
    "        target_logits = torch.stack([torch.stack([torch.nn.functional.one_hot(torch.LongTensor([output.expected_answer.value]), num_classes=len(output.response_token_ids)) for output in outputs], dim=0) for example, outputs in zip(examples, self.vqa_outputs)], dim=0) # (# examples, # questions, # possible responses)\n",
    "        # print(\"target_logits:\", target_logits)\n",
    "        \n",
    "        # Flip target logits for mistake examples\n",
    "        mistake_labels = torch.stack([torch.stack([torch.nn.functional.one_hot(torch.LongTensor([int(example.mistake)]), num_classes=2) for output in outputs], dim=0) for example, outputs in zip(examples, self.vqa_outputs)], dim=0)\n",
    "        # print(\"mistake_labels:\", mistake_labels)\n",
    "        target_logits = mistake_labels * (torch.ones_like(target_logits) - target_logits) + (torch.ones_like(target_logits) - mistake_labels) * target_logits\n",
    "        # print(\"target_logits:\", target_logits)\n",
    "        target_logits = target_logits.squeeze(2)\n",
    "        # print(\"target_logits:\", target_logits)\n",
    "        \n",
    "        predicted_logits = torch.stack([torch.stack([torch.stack([output.logits[output.response_token_ids[response_type]] for response_type in VQAResponse], dim=0) for output in outputs], dim=0) for outputs in self.vqa_outputs], dim=0) # (# examples, # questions, # possible responses)\n",
    "        # print(\"predicted_logits:\", predicted_logits)\n",
    "        predicted_logits = torch.nn.functional.sigmoid(predicted_logits)\n",
    "        # print(\"predicted_logits:\", predicted_logits)\n",
    "        \n",
    "        assert target_logits.shape == predicted_logits.shape, f\"Expected target_logits shape {target_logits.shape} to be the same as predicted_logits shape {predicted_logits.shape}.\"\n",
    "        return (predicted_logits - target_logits).float()\n",
    "        \n",
    "        # NOTE: possible problem: the target_logits imply that in mistake cases, all expected answers should be violated. This may incentivize models to create duplicate questions that ultimately are incomplete.\n",
    "\n",
    "class HeuristicMistakeDetectionEvaluator(MistakeDetectionEvaluator):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 example: MistakeDetectionExample,\n",
    "                 vqa_outputs: list[VQAOutputs],\n",
    "                 mistake_threshold = 0.1):\n",
    "        super().__init__(example, vqa_outputs)\n",
    "        self.mistake_threshold = mistake_threshold\n",
    "        \n",
    "    def check_mistakes(self) -> list[bool]:\n",
    "        # If VQA answers don't match expected answers, there's a mistake (we can decide to make this more lenient later)\n",
    "        mistake_predictions = []\n",
    "        for example, outputs in zip(self.examples, self.vqa_outputs):\n",
    "            this_mistake_predictions = []\n",
    "            for output in outputs:       \n",
    "                if output.predicted_answer != output.expected_answer:\n",
    "                    predicted_mistake = True\n",
    "                else:\n",
    "                    predicted_mistake = False\n",
    "                this_mistake_predictions.append(predicted_mistake)\n",
    "            mistake_predictions.append(this_mistake_predictions)\n",
    "            \n",
    "        # Heuristic: for last 10% of frames (based on step duration), take majority prediction of mistake/success\n",
    "        # In the future, can prompt LLaMA again for this information?\n",
    "        agg_preds = []\n",
    "        for mistake_pred, example in zip(mistake_predictions, self.examples):\n",
    "            if len(mistake_pred) > 0:\n",
    "                cutoff_time = max(example.frame_times) - ((max(example.frame_times) - min(example.frame_times)) * self.mistake_threshold)\n",
    "                mistake_pred_cut = [pred for pred, ft in zip(mistake_pred, example.frame_times) if ft >= cutoff_time]\n",
    "                if len(mistake_pred_cut) == 0:\n",
    "                    mistake_pred_cut = [mistake_pred[-1]]\n",
    "                \n",
    "                # last_n = max(int(len(mistake_pred) * self.mistake_threshold), 1) # Round up to a minimum of 1 frame\n",
    "                mistake_pred_cut = Counter(mistake_pred_cut)\n",
    "                mistake_pred_cut, _ = mistake_pred_cut.most_common()[0]\n",
    "            else:\n",
    "                mistake_pred_cut = False\n",
    "            agg_preds.append(mistake_pred_cut)            \n",
    "            \n",
    "        return agg_preds       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b9ae4c-bc5b-499b-a59c-7be6380582f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd8e4751-169b-4061-b028-b2a7e4b78a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from Kosmos-2 code\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import requests\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers.utils.constants import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD\n",
    "\n",
    "def is_overlapping(rect1, rect2):\n",
    "    x1, y1, x2, y2 = rect1\n",
    "    x3, y3, x4, y4 = rect2\n",
    "    return not (x2 < x3 or x1 > x4 or y2 < y3 or y1 > y4)\n",
    "\n",
    "def draw_entity_boxes_on_image(image, entities, show=False, save_path=None):\n",
    "    \"\"\"_summary_\n",
    "    Args:\n",
    "        image (_type_): image or image path\n",
    "        collect_entity_location (_type_): _description_\n",
    "    \"\"\"\n",
    "    if isinstance(image, Image.Image):\n",
    "        image_h = image.height\n",
    "        image_w = image.width\n",
    "        image = np.array(image)[:, :, [2, 1, 0]]\n",
    "    elif isinstance(image, str):\n",
    "        if os.path.exists(image):\n",
    "            pil_img = Image.open(image).convert(\"RGB\")\n",
    "            image = np.array(pil_img)[:, :, [2, 1, 0]]\n",
    "            image_h = pil_img.height\n",
    "            image_w = pil_img.width\n",
    "        else:\n",
    "            raise ValueError(f\"invaild image path, {image}\")\n",
    "    elif isinstance(image, torch.Tensor):\n",
    "        # pdb.set_trace()\n",
    "        image_tensor = image.cpu()\n",
    "        reverse_norm_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073])[:, None, None]\n",
    "        reverse_norm_std = torch.tensor([0.26862954, 0.26130258, 0.27577711])[:, None, None]\n",
    "        image_tensor = image_tensor * reverse_norm_std + reverse_norm_mean\n",
    "        pil_img = T.ToPILImage()(image_tensor)\n",
    "        image_h = pil_img.height\n",
    "        image_w = pil_img.width\n",
    "        image = np.array(pil_img)[:, :, [2, 1, 0]]\n",
    "    else:\n",
    "        raise ValueError(f\"invaild image format, {type(image)} for {image}\")\n",
    "\n",
    "    if len(entities) == 0:\n",
    "        return image\n",
    "\n",
    "    new_image = image.copy()\n",
    "    previous_bboxes = []\n",
    "    # size of text\n",
    "    text_size = 1\n",
    "    # thickness of text\n",
    "    text_line = 1  # int(max(1 * min(image_h, image_w) / 512, 1))\n",
    "    box_line = 3\n",
    "    (c_width, text_height), _ = cv2.getTextSize(\"F\", cv2.FONT_HERSHEY_COMPLEX, text_size, text_line)\n",
    "    base_height = int(text_height * 0.675)\n",
    "    text_offset_original = text_height - base_height\n",
    "    text_spaces = 3\n",
    "\n",
    "    for entity_name, _, bboxes in entities: # NOTE: \"_\" is returned by Kosmos-2 as a tuple (start, end); but it's never used\n",
    "        for box in bboxes:\n",
    "            if type(box) != BoundingBox:\n",
    "                box = BoundingBox(*box)\n",
    "            x1_norm, y1_norm, x2_norm, y2_norm = box.coords\n",
    "            orig_x1, orig_y1, orig_x2, orig_y2 = int(x1_norm * image_w), int(y1_norm * image_h), int(x2_norm * image_w), int(y2_norm * image_h)\n",
    "            # draw bbox\n",
    "            # random color\n",
    "            color = tuple(np.random.randint(0, 255, size=3).tolist())\n",
    "            new_image = cv2.rectangle(new_image, (orig_x1, orig_y1), (orig_x2, orig_y2), color, box_line)\n",
    "\n",
    "            l_o, r_o = box_line // 2 + box_line % 2, box_line // 2 + box_line % 2 + 1\n",
    "\n",
    "            x1 = orig_x1 - l_o\n",
    "            y1 = orig_y1 - l_o\n",
    "\n",
    "            if y1 < text_height + text_offset_original + 2 * text_spaces:\n",
    "                y1 = orig_y1 + r_o + text_height + text_offset_original + 2 * text_spaces\n",
    "                x1 = orig_x1 + r_o\n",
    "\n",
    "            # add text background\n",
    "            (text_width, text_height), _ = cv2.getTextSize(f\"  {entity_name}\", cv2.FONT_HERSHEY_COMPLEX, text_size, text_line)\n",
    "            text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2 = x1, y1 - (text_height + text_offset_original + 2 * text_spaces), x1 + text_width, y1\n",
    "\n",
    "            for prev_bbox in previous_bboxes:\n",
    "                while is_overlapping((text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2), prev_bbox):\n",
    "                    text_bg_y1 += (text_height + text_offset_original + 2 * text_spaces)\n",
    "                    text_bg_y2 += (text_height + text_offset_original + 2 * text_spaces)\n",
    "                    y1 += (text_height + text_offset_original + 2 * text_spaces)\n",
    "\n",
    "                    if text_bg_y2 >= image_h:\n",
    "                        text_bg_y1 = max(0, image_h - (text_height + text_offset_original + 2 * text_spaces))\n",
    "                        text_bg_y2 = image_h\n",
    "                        y1 = image_h\n",
    "                        break\n",
    "\n",
    "            alpha = 0.5\n",
    "            for i in range(text_bg_y1, text_bg_y2):\n",
    "                for j in range(text_bg_x1, text_bg_x2+120):\n",
    "                    if i < image_h and j < image_w:\n",
    "                        if j < text_bg_x1 + 1.35 * c_width:\n",
    "                            # original color\n",
    "                            bg_color = color\n",
    "                        else:\n",
    "                            # white\n",
    "                            bg_color = [255, 255, 255]\n",
    "                        new_image[i, j] = (alpha * new_image[i, j] + (1 - alpha) * np.array(bg_color)).astype(np.uint8)\n",
    "\n",
    "            cv2.putText(\n",
    "                new_image, f\"  {entity_name} ({round(box.score, 2)})\", (x1, y1 - text_offset_original - 1 * text_spaces), cv2.FONT_HERSHEY_COMPLEX, text_size, (0, 0, 0), text_line, cv2.LINE_AA\n",
    "            )\n",
    "            # previous_locations.append((x1, y1))\n",
    "            previous_bboxes.append((text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2))\n",
    "\n",
    "    pil_image = Image.fromarray(new_image[:, :, [2, 1, 0]])\n",
    "    if save_path:\n",
    "        pil_image.save(save_path)\n",
    "    if show:\n",
    "        plt.figure()\n",
    "        fig, ax = plt.subplots(figsize=(16, 12))\n",
    "        ax.imshow(pil_image)\n",
    "        plt.show()\n",
    "    return new_image\n",
    "\n",
    "def convert_bbox(center_x, center_y, width, height):\n",
    "    top_left_x = center_x - width / 2\n",
    "    top_left_y = center_y - height / 2\n",
    "    bottom_right_x = center_x + width / 2\n",
    "    bottom_right_y = center_y + height / 2\n",
    "    return (top_left_x, top_left_y, bottom_right_x, bottom_right_y)\n",
    "\n",
    "def get_preprocessed_image(pixel_values):\n",
    "    pixel_values = pixel_values.squeeze().numpy()\n",
    "    unnormalized_image = (pixel_values * np.array(OPENAI_CLIP_STD)[:, None, None]) + np.array(OPENAI_CLIP_MEAN)[:, None, None]\n",
    "    unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "    unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "    unnormalized_image = Image.fromarray(unnormalized_image)\n",
    "    return unnormalized_image\n",
    "\n",
    "# bounding box merging code from GPT4\n",
    "class BoundingBox:\n",
    "    \"\"\"\n",
    "    A class representing a bounding box with a score.\n",
    "    \n",
    "    Attributes:\n",
    "        x1 (float): The x-coordinate of the top-left corner.\n",
    "        y1 (float): The y-coordinate of the top-left corner.\n",
    "        x2 (float): The x-coordinate of the bottom-right corner.\n",
    "        y2 (float): The y-coordinate of the bottom-right corner.\n",
    "        score (float): The confidence score of the bounding box.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x1, y1, x2, y2, score=0.0):\n",
    "        \"\"\"\n",
    "        Initialize a BoundingBox instance.\n",
    "        \n",
    "        Args:\n",
    "            x1 (float): The x-coordinate of the top-left corner.\n",
    "            y1 (float): The y-coordinate of the top-left corner.\n",
    "            x2 (float): The x-coordinate of the bottom-right corner.\n",
    "            y2 (float): The y-coordinate of the bottom-right corner.\n",
    "            score (float, optional): The confidence score of the bounding box. Defaults to 0.0.\n",
    "        \"\"\"\n",
    "        self.coords = (x1, y1, x2, y2)\n",
    "        self.score = score\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Allows accessing the bounding box coordinates like a list.\n",
    "        \n",
    "        Args:\n",
    "            index (int): The index of the coordinate.\n",
    "            \n",
    "        Returns:\n",
    "            float: The coordinate at the specified index.\n",
    "        \"\"\"\n",
    "        return self.coords[index]\n",
    "\n",
    "    def __setitem__(self, index, value):\n",
    "        \"\"\"\n",
    "        Allows setting the bounding box coordinates like a list.\n",
    "        \n",
    "        Args:\n",
    "            index (int): The index of the coordinate to set.\n",
    "            value (float): The new value for the coordinate.\n",
    "        \"\"\"\n",
    "        self.coords[index] = value\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Returns a string representation of the BoundingBox instance.\n",
    "        \n",
    "        Returns:\n",
    "            str: The string representation of the BoundingBox.\n",
    "        \"\"\"\n",
    "        return f\"BoundingBox({self.coords[0]}, {self.coords[1]}, {self.coords[2]}, {self.coords[3]}, score={self.score})\"\n",
    "\n",
    "    def normalize(self, image_width, image_height):\n",
    "        \"\"\"\n",
    "        Normalizes the bounding box coordinates with respect to the image dimensions.\n",
    "        \n",
    "        Args:\n",
    "            image_width (float): The width of the image.\n",
    "            image_height (float): The height of the image.\n",
    "            \n",
    "        Returns:\n",
    "            BoundingBox: A new instance of BoundingBox with normalized coordinates.\n",
    "        \"\"\"\n",
    "        x1, y1, x2, y2 = self.coords\n",
    "        return BoundingBox(x1 / image_width, y1 / image_height, x2 / image_width, y2 / image_height, self.score)\n",
    "    \n",
    "class BoundingBoxCluster:\n",
    "    def __init__(self, boxes: list[BoundingBox]):\n",
    "        self.boxes = boxes\n",
    "        self.graph = {i: set() for i in range(len(boxes))}\n",
    "        self.build_graph()\n",
    "    \n",
    "    def is_overlapping(self, box1, box2):\n",
    "        # Check if two boxes (x1, y1, x2, y2) overlap\n",
    "        return not (box1[2] < box2[0] or box1[0] > box2[2] or box1[3] < box2[1] or box1[1] > box2[3])\n",
    "    \n",
    "    def build_graph(self):\n",
    "        # Build a graph based on overlaps\n",
    "        for i in range(len(self.boxes)):\n",
    "            for j in range(i + 1, len(self.boxes)):\n",
    "                if self.is_overlapping(self.boxes[i], self.boxes[j]):\n",
    "                    self.graph[i].add(j)\n",
    "                    self.graph[j].add(i)\n",
    "    \n",
    "    def find_connected_components(self):\n",
    "        # Find connected components (clusters) in the graph\n",
    "        visited = set()\n",
    "        components = []\n",
    "        \n",
    "        def dfs(node, component):\n",
    "            visited.add(node)\n",
    "            component.append(node)\n",
    "            for neighbor in self.graph[node]:\n",
    "                if neighbor not in visited:\n",
    "                    dfs(neighbor, component)\n",
    "        \n",
    "        for node in range(len(self.boxes)):\n",
    "            if node not in visited:\n",
    "                component = []\n",
    "                dfs(node, component)\n",
    "                components.append(component)\n",
    "        \n",
    "        return components\n",
    "    \n",
    "    def merge_boxes(self, component):\n",
    "        # Merge all boxes in a connected component\n",
    "        x1 = min(self.boxes[node][0] for node in component)\n",
    "        y1 = min(self.boxes[node][1] for node in component)\n",
    "        x2 = max(self.boxes[node][2] for node in component)\n",
    "        y2 = max(self.boxes[node][3] for node in component)\n",
    "        score = max(self.boxes[node].score for node in component)\n",
    "        return BoundingBox(x1, y1, x2, y2, score)\n",
    "    \n",
    "    def get_merged_boxes(self):\n",
    "        # Get merged bounding boxes for each connected component\n",
    "        components = self.find_connected_components()\n",
    "        return [self.merge_boxes(component) for component in components]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729587a3-3902-4544-a502-2d861f7c52ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RLHF/DPO Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b25362c-8082-4235-838a-c925b6e68bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "from PIL import Image\n",
    "from typing import Union\n",
    "    \n",
    "PROMPT_TEMPLATES = {\n",
    "    \"llava-hf/llava-1.5-7b-hf\": \"USER: <image>\\n{question} (yes/no) ASSISTANT: \"\n",
    "}\n",
    "\n",
    "class MistakeDetectionScorer:\n",
    "    def __call__(self, \n",
    "                 examples: list[MistakeDetectionExample],\n",
    "                 vqg_outputs: dict[int, VQGOutputs],\n",
    "                 return_outputs: bool=False) -> torch.FloatTensor:\n",
    "        \"\"\"Score visual questions when posed on visual inputs to some multimodal language model.\n",
    "        \n",
    "        :arg examples: List of MistakeDetectionExample objects to run through the model.\n",
    "        :arg vqg_outputs: Dictionary mapping unique annotated procedures (by int ID) to VQGOutputs, which include a consistent number of visual questions to verify success of the procedure.\n",
    "        :return: FloatTensor of scores of shape (len(examples), # questions per example).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Need to use a subclass of MistakeDetectionScorer.\")\n",
    "        \n",
    "\n",
    "class FrameVQAMistakeDetectionScorer(MistakeDetectionScorer):\n",
    "    \"\"\"Class that provides preference scores for visual questions to facilitate mistake detection on individual video frames.\"\"\"\n",
    "    def __init__(self, vlm_name):\n",
    "        super().__init__()\n",
    "        self.model_name = vlm_name\n",
    "        self.processor = AutoProcessor.from_pretrained(vlm_name)\n",
    "        self.vlm = AutoModelForVision2Seq.from_pretrained(vlm_name, \n",
    "                                                          cache_dir=DATA_CACHE_DIR, \n",
    "                                                          load_in_8bit=True)\n",
    "        self.vlm.language_model.generation_config.top_p = None\n",
    "        self.vlm.language_model.generation_config.temperature = None\n",
    "        self.vlm.language_model.generation_config.do_sample = False\n",
    "        self.processor.tokenizer.padding_side = \"left\"\n",
    "        \n",
    "    def __call__(self, \n",
    "                 examples: list[MistakeDetectionExample],\n",
    "                 vqg_outputs: dict[int, VQGOutputs],\n",
    "                 return_vqa_outputs: bool=False,\n",
    "                 batch_size: int=1) -> Union[torch.FloatTensor, list[VQAOutputs]]:\n",
    "        \"\"\"Score visual questions when posed on individual video frames to a VLM.\n",
    "        \n",
    "        :arg examples: List of MistakeDetectionExample objects to run through the VLM.\n",
    "        :arg vqg_outputs: Dictionary mapping unique annotated procedures (by int ID) to VQGOutputs, which include a consistent number of visual questions to verify success of the procedure.\n",
    "        :arg return_vqa_outputs: Whether to return VQAOutputs from VQA inference instead of scores per example.\n",
    "        :arg batch_size: Batch size for VQA inference. Note that LLaVA may return nan logits if greater than 1.\n",
    "        :return: FloatTensor of scores of shape (len(examples), # questions per example).\n",
    "        \"\"\"\n",
    "        # Verify every example has only a single frame, as expected for VQA mistake detection (frame selection occurs elsewhere)\n",
    "        n_examples = len(examples)\n",
    "        for example in examples:\n",
    "            assert len(example.frames) == 1, \"Before sending MistakeDetectionExample objects to FrameVQAMistakeDetectionScorer, please filter frames property to a single frame.\"\n",
    "        \n",
    "        # Verify all relevant VQGOutputs have the same number of questions\n",
    "        example_vqg_outputs = [vqg_outputs[example.procedure_id] for example in examples]\n",
    "        n_questions_per_frame = len(example_vqg_outputs[0].questions)\n",
    "        for output in example_vqg_outputs:\n",
    "            assert len(output.questions) == n_questions_per_frame, \"All VQGOutputs should have the same number of generated questions.\"\n",
    "               \n",
    "        # Extract parallel frames, questions, answers, and mistake labels\n",
    "        examples_questions_answers = [(example, question, answer) for example, output in zip(examples, example_vqg_outputs) for question, answer in zip(output.questions, output.answers)]\n",
    "        examples_parallel = [example for example, _, _ in examples_questions_answers]\n",
    "        questions = [question for _, question, _ in examples_questions_answers]\n",
    "        answers = [answer for _, _, answer in examples_questions_answers]\n",
    "        frames = [example.frames[0] for example in examples_parallel]\n",
    "        assert len(examples_parallel) == len(questions) == len(answers) == len(frames) == n_examples * n_questions_per_frame\n",
    "        mistake_labels = [example.mistake for example in examples_parallel]\n",
    "             \n",
    "        prompt_template = PROMPT_TEMPLATES[self.model_name]\n",
    "        prompts = [prompt_template.format(question=question) for question in questions]\n",
    "        \n",
    "        response_tokens = {}\n",
    "        for response_type in VQAResponse:\n",
    "            response_tokens[response_type] = self.processor.tokenizer(response_type.name, add_special_tokens=False)['input_ids'][0]\n",
    "            \n",
    "        # Run VQA in batches\n",
    "        logits = []\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(0, len(frames), batch_size), desc=\"running VQA\"):\n",
    "                # Prepare the batch\n",
    "                batch_frames = frames[i:i+batch_size]\n",
    "                batch_prompts = prompts[i:i+batch_size]            \n",
    "\n",
    "                inputs = self.processor(text=batch_prompts, images=batch_frames, padding=True, return_tensors=\"pt\")\n",
    "                inputs = inputs.to(self.vlm.device)\n",
    "                this_logits = self.vlm(**inputs).logits\n",
    "                this_logits = this_logits[:, -1].detach().cpu()\n",
    "                logits.append(this_logits)\n",
    "            logits = torch.cat(logits, dim=0)\n",
    "        \n",
    "        # Gather up VQAOutputs (# examples, # questions per example)\n",
    "        vqa_outputs = []\n",
    "        for i, example in enumerate(examples): \n",
    "            this_vqa_outputs = []\n",
    "            for j in range(n_questions_per_frame):\n",
    "                parallel_idx = i * n_questions_per_frame + j\n",
    "                this_vqa_outputs.append(\n",
    "                    VQAOutputs(\n",
    "                        example.procedure_id,\n",
    "                        frames[parallel_idx],\n",
    "                        prompts[parallel_idx],\n",
    "                        answers[parallel_idx],\n",
    "                        response_tokens,\n",
    "                        logits[parallel_idx]\n",
    "                    )\n",
    "                )\n",
    "            vqa_outputs.append(this_vqa_outputs)\n",
    "        \n",
    "        if return_vqa_outputs:\n",
    "            return vqa_outputs\n",
    "        else:\n",
    "            # In most cases, we just want scores for each generated question\n",
    "            evaluator = MistakeDetectionEvaluator(examples, vqa_outputs)\n",
    "            logits_errors = evaluator.get_logits_errors()\n",
    "            return logits_errors\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4f25a0-212e-4f97-9f67-adca4b2eb4b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CaptainCook4D Preliminary Empirical Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f74f1-7da4-48ed-8bc3-dcdc2fd8b97c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Gather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67479c4b-1d26-4899-9ccc-8b3a37799f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/335 [00:17<1:39:31, 17.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error examples: 1\n",
      "Success examples: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/335 [00:30<1:22:16, 14.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error examples: 3\n",
      "Success examples: 11\n",
      "Warning: Some error information discarded from only using the first annotated error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/335 [00:50<1:35:06, 17.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error examples: 8\n",
      "Success examples: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/335 [01:25<2:12:42, 24.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error examples: 8\n",
      "Success examples: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 5/335 [01:32<1:39:20, 18.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error examples: 11\n",
      "Success examples: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 6/335 [01:45<1:29:01, 16.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error examples: 12\n",
      "Success examples: 31\n",
      "Warning: Some error information discarded from only using the first annotated error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 7/335 [02:01<1:29:14, 16.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error examples: 17\n",
      "Success examples: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 8/335 [02:20<1:33:10, 17.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error examples: 17\n",
      "Success examples: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 8/335 [02:40<1:49:07, 20.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected at least 20 positive and negative examples!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# Pick a sample video from CaptainCook4D\n",
    "all_video_files = os.listdir(VIDEO_DIR)\n",
    "video_paths = [f for f in all_video_files if f.endswith('.mp4')]\n",
    "STEP_ANNOTATIONS = json.load(open(os.path.join(ANNOTATIONS_DIR, \"annotation_json/complete_step_annotations.json\"), \"r\"))\n",
    "ERROR_ANNOTATIONS = json.load(open(os.path.join(ANNOTATIONS_DIR, \"annotation_json/error_annotations.json\"), \"r\"))\n",
    "for error_annotation in ERROR_ANNOTATIONS:\n",
    "    video_id = error_annotation['recording_id']\n",
    "    STEP_ANNOTATIONS[video_id][\"steps_errors\"] = error_annotation[\"step_annotations\"]\n",
    "\n",
    "success_examples = []\n",
    "error_examples = []\n",
    "for sample_video_path in tqdm(video_paths):\n",
    "    sample_video_id = \"_\".join(sample_video_path.split('_')[:2])\n",
    "    sample_video_path = os.path.join(VIDEO_DIR, sample_video_path)\n",
    "    try:\n",
    "        sample_video = get_video(sample_video_path)\n",
    "    except:\n",
    "        print(f\"Warning: could not open video file: {sample_video_path}\")\n",
    "        continue\n",
    "\n",
    "    # Load step annotations for it and display precondition/effect frames\n",
    "    for step in STEP_ANNOTATIONS[sample_video_id][\"steps_errors\"]:\n",
    "        # Extract some keyframes for the action\n",
    "        step_duration = step['end_time'] - step['start_time']\n",
    "        step_id = int(step['step_id'])\n",
    "        \n",
    "        # Some steps are skipped\n",
    "        if step_duration < 0.1:\n",
    "            continue\n",
    "\n",
    "        adjusted_start = step['start_time'] + min(step_duration * 0.05, 0.5) # Adjust the start time to be later by a maximum of 0.5 seconds\n",
    "        adjusted_end = step['end_time'] - min(step_duration * 0.3, 3) # Adjust the end time to be earlier by a maximum of 3 seconds\n",
    "        SAMPLE_FREQUENCY = 4.0\n",
    "        times = generate_float_series(adjusted_start, adjusted_end, SAMPLE_FREQUENCY) # ultimately, we'll want to look at every image frame in some regular interval to determine if there's a mistake\n",
    "        frames = extract_frames(sample_video, times)\n",
    "        frames = [Image.fromarray(frame) for frame in frames]\n",
    "\n",
    "        verb, procedure_description = step['description'].split(\"-\")[0], \"-\".join(step['description'].split(\"-\")[1:])\n",
    "        \n",
    "        if \"errors\" in step and len(step[\"errors\"]) > 0:               \n",
    "            mistake_type = step['errors'][0][\"tag\"]\n",
    "            mistake_description = step['errors'][0]['description']\n",
    "            # altered_procedure_description = step['modified_description'] # NOTE: can use this later if needed\n",
    "            \n",
    "            # Start with only errors specific to a single step, not related to quantities\n",
    "            # Preparation error involves the wrong object(s)\n",
    "            # Technique error involves action being performed the wrong way\n",
    "            if mistake_type not in [\"Preparation Error\", \"Technique Error\"]:\n",
    "                continue\n",
    "            \n",
    "            if len(step['errors']) > 1:\n",
    "                print(\"Warning: Some error information discarded from only using the first annotated error.\")            \n",
    "            \n",
    "            error_examples.append(\n",
    "                MistakeDetectionExample(\n",
    "                    sample_video_id,\n",
    "                    step_id,\n",
    "                    frames,\n",
    "                    [time - min(times) for time in times],\n",
    "                    procedure_description,\n",
    "                    True,\n",
    "                    mistake_type,\n",
    "                    mistake_description\n",
    "                )\n",
    "            )\n",
    "            # pprint(error_examples[-1])\n",
    "        else:\n",
    "            success_examples.append(\n",
    "                MistakeDetectionExample(\n",
    "                    sample_video_id,\n",
    "                    step_id,\n",
    "                    frames,\n",
    "                    [time - min(times) for time in times],\n",
    "                    procedure_description,\n",
    "                    False\n",
    "                )\n",
    "            )        \n",
    "            # pprint(success_examples[-1])\n",
    "\n",
    "    if len(error_examples) >= 20 and len(success_examples) >= 20:\n",
    "        print(\"Collected at least 20 positive and negative examples!\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"Error examples:\", len(error_examples))\n",
    "        print(\"Success examples:\", len(success_examples))\n",
    "\n",
    "    sample_video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e5ba9b-72b1-4c65-b1c6-8f7a1f8fa89d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1: VQG with LLaMA for Recipe Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19146ad2-de89-4e12-b0f3-e12dc5f7cb14",
   "metadata": {},
   "source": [
    "Load model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e252bb11-0097-4112-ae3b-2ffa1893a71f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ec9d4fa60e4d10aa9306e8c45bc30c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "LM_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "lm = pipeline(\"text-generation\", \n",
    "                 model=LM_NAME, \n",
    "                 token=\"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\", \n",
    "                 model_kwargs={\"load_in_8bit\": True})\n",
    "# lm = lm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12ce1ae8-959a-4d9f-b7a5-f4518cfda7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.model.generation_config.top_p = None\n",
    "lm.model.generation_config.temperature = None\n",
    "lm.model.generation_config.do_sample = False\n",
    "lm.tokenizer.pad_token_id = lm.model.config.eos_token_id\n",
    "lm.tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e5d51a-4e59-4f84-87eb-5aeaf148de2d",
   "metadata": {},
   "source": [
    "Load recipe steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75f73e44-33ad-42aa-87c8-b6043c172f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'Pour 1 egg into the ramekin cup',\n",
      " 2: 'Place the egg from the cup over the lettuce',\n",
      " 3: 'Coat a 6-oz. ramekin cup with cooking spray',\n",
      " 4: 'Microwave the ramekin cup uncovered on high for 30 seconds',\n",
      " 5: 'sprinkle 1 tablespoon of cheese on cup',\n",
      " 6: 'Top cup with 1 tablespoon of salsa',\n",
      " 7: 'replace the top of the English muffin',\n",
      " 8: 'Continue to Microwave for 15-30 more seconds or until the egg is almost '\n",
      "    'set',\n",
      " 9: 'Line the bottom piece of the English muffin with lettuce',\n",
      " 10: 'Microwave just until cheese melts, about 10 seconds',\n",
      " 11: 'stir the ramekin cup',\n",
      " 12: 'Cut the English muffin into two pieces with a knife',\n",
      " 14: 'Peel 1 garlic clove',\n",
      " 15: 'Pour the sauces over the meatballs',\n",
      " 16: 'Cut 1/8 garlic clove',\n",
      " 17: 'Peel one medium onion',\n",
      " 18: 'Stir the contents in the microwave with a spoon',\n",
      " 19: 'Slice 1/8 medium onion',\n",
      " 20: 'Microwave the plate, covered, on high for 1.5 minutes',\n",
      " 21: 'Place 5 meatballs in a Microwave-safe plate',\n",
      " 22: 'Cut 1/4 medium carrot into short, thin strips',\n",
      " 23: 'Mix 1/4 cup sweet-and-sour sauce and 1/2 teaspoon soy sauce in a small '\n",
      "     'bowl',\n",
      " 24: 'Top the plate with the carrots, onion, garlic and 1/4 tsp pepper powder',\n",
      " 25: 'Microwave for 1 more minute',\n",
      " 26: 'Mince 1/8 garlic clove',\n",
      " 27: 'Cut onion into two pieces',\n",
      " 28: 'Measure 1/8 teaspoon of salt and add it to the mug',\n",
      " 29: 'Sprinkle dried Italian herbs inside the mug',\n",
      " 30: 'Take 1 tablespoon of marinara sauce',\n",
      " 31: 'spread marinara sauce around the surface of the batter',\n",
      " 32: 'Microwave for 1 minute 20 seconds, or until it rises and the toppings '\n",
      "     'are bubbling',\n",
      " 33: 'Measure 4 tablespoons of flour and add it to the mug',\n",
      " 34: '1 tablespoon of olive oil to the mug',\n",
      " 35: 'Measure 1/16 teaspoon of baking soda and add it to the mug',\n",
      " 36: 'Take a microwavable mug',\n",
      " 37: 'Mix the contents of the mug thoroughly. (There might be some lumps, but '\n",
      "     'that is ok.)',\n",
      " 38: 'Stir the contents in the mug well',\n",
      " 39: 'Sprinkle 1 generous tablespoon of mozzarella cheese on top of the sauce',\n",
      " 40: 'Measure 1/8 teaspoon of baking powder and add it to the mug',\n",
      " 41: 'Add in 3 tablespoons of milk to the mug',\n",
      " 42: 'Mix in the flavour packet to the bowl',\n",
      " 43: 'Stir noodles with a spoon or fork until the flavouring dissolves',\n",
      " 44: 'Remove the noodles from the package(Break Noodles / Keep them as a '\n",
      "     'block)',\n",
      " 45: 'Microwave the ramen for 4 minutes',\n",
      " 46: 'slice 1/4 medium onion into pieces',\n",
      " 47: 'Peel 1 medium onion',\n",
      " 48: 'Let the noodles sit for about 1 minute after the microwave stops',\n",
      " 49: 'Cover with a lid (or paper towel) to prevent splattering',\n",
      " 50: 'Add basil to the bowl',\n",
      " 51: 'Chop 1 garlic clove on a cutting board',\n",
      " 52: 'Add the noodles to the bowl',\n",
      " 53: 'Add chopped cilantro to the bowl',\n",
      " 54: 'Put all the Vegetables in a microwave-safe bowl',\n",
      " 55: 'cover the noodles with water',\n",
      " 56: 'Wait about 30 seconds for the coffee to bloom. (You will see small '\n",
      "     'bubbles or foam on the coffee grounds during this step.)',\n",
      " 57: 'Pour a small amount of water into the filter to wet the grounds',\n",
      " 58: 'Transfer the grounds to the filter cone',\n",
      " 59: 'spread open filter in dripper to create a cone',\n",
      " 60: 'Place the paper filter in the dripper',\n",
      " 61: 'Slowly pour the rest of the water over the grounds in a circular motion. '\n",
      "     'Do not overfill beyond the top of the paper filter',\n",
      " 62: 'Prepare the filter insert by folding the paper filter in half to create '\n",
      "     'a semi-circle, and in half again to create a quarter-circle',\n",
      " 63: 'Discard the paper filter and coffee grounds',\n",
      " 64: 'Boil the water. (While the water is boiling, assemble the filter cone)',\n",
      " 65: 'Weigh the coffee beans (0.8oz-0.12 oz)',\n",
      " 66: 'Let the coffee drain completely into the mug before removing the dripper',\n",
      " 67: 'Grind the coffee beans until the coffee grounds are the consistency of '\n",
      "     'coarse sand, about 20 seconds',\n",
      " 68: 'Measure 12 ounces of cold water',\n",
      " 69: 'Place the dripper on top of a coffee mug',\n",
      " 70: 'transfer water to a kettle',\n",
      " 71: 'Once the water has boiled, check the temperature of the water. (The '\n",
      "     'water should be between 195-205 degrees Fahrenheit or between 91-96 '\n",
      "     'degrees Celsius. If the water is too hot, let it cool briefly.)',\n",
      " 72: 'Whisk the egg',\n",
      " 73: 'Microwave for 3 minutes, stirring in between',\n",
      " 74: 'Add 1/2 tbsp sweet and sour sauce to the bowl',\n",
      " 75: 'Sprinkle 1 tbsp shredded cheddar cheese on top of the egg',\n",
      " 76: 'Mix the contents of the bowl well',\n",
      " 77: 'Extract and add contents of an egg to a microwave-safe bowl',\n",
      " 78: 'Roll the tortilla from one end to another into a log shape, about 1.5 '\n",
      "     'inches thick. Roll it tight enough to prevent gaps but not so tight that '\n",
      "     'the filling leaks',\n",
      " 79: 'Pour egg mixture on top of the tortilla',\n",
      " 80: 'Add 1 tbsp salsa to the bowl',\n",
      " 81: 'Place 8 inch tortilla on a cutting board',\n",
      " 82: 'Sprinkle oregano in the bowl',\n",
      " 83: 'Heat the contents of the mug for 1 minute and serve',\n",
      " 84: 'Add 1/5 teaspoon cinnamon to the mug',\n",
      " 85: 'Mix the contents of the mug',\n",
      " 87: 'Add 1 teaspoon of white sugar to the mug',\n",
      " 88: 'Fill a microwave-safe mug with skimmed milk',\n",
      " 89: 'Microwave the contents of the mug for 1 minute',\n",
      " 90: 'Add 2 pieces of chocolate to the mug',\n",
      " 91: 'Microwave on high for 90 seconds until the egg is cooked through',\n",
      " 92: 'add bread pieces to the egg mixture in the mug, pressing the bread down '\n",
      "     'into the egg',\n",
      " 93: 'stir the mug',\n",
      " 94: 'In a large mug, melt 1 tablespoon of softened butter in the microwave '\n",
      "     'for about 30 seconds',\n",
      " 95: 'Sprinkle 1/4 teaspoon cinnamon over the egg',\n",
      " 96: 'cut the contents on plate, and serve',\n",
      " 97: \"Put the mug's contents on a plate\",\n",
      " 98: 'Roll the butter around in the mug to coat it',\n",
      " 99: 'Cut or tear 1 slices of bread into bite-size pieces',\n",
      " 100: 'Add 1/4 teaspoon vanilla extract to the mug',\n",
      " 101: 'In the mug, whisk one egg with a fork until well blended',\n",
      " 102: 'Continue slicing with floss to create 1 more pinwheel',\n",
      " 103: 'Discard ends of the tortilla',\n",
      " 104: 'Clean the knife by wiping with a paper towel',\n",
      " 105: \"Cross the floss's two ends over the tortilla roll's top\",\n",
      " 106: 'Slide floss under the tortilla, perpendicular to the length of the roll',\n",
      " 107: 'Spread jelly over the nut butter',\n",
      " 108: 'pull the floss ends in opposite directions to slice',\n",
      " 109: 'Place the pinwheels on a plate',\n",
      " 110: 'Secure the rolled tortilla by inserting 5 toothpicks about 1 inch apart',\n",
      " 111: 'Use the knife to scoop jelly from the jar',\n",
      " 112: 'Spread nut butter onto the tortilla, leaving 1/2-inch uncovered at the '\n",
      "      'edges',\n",
      " 113: 'Use a butter knife to scoop nut butter from the jar',\n",
      " 114: 'Trim the ends of the tortilla roll with the butter knife, leaving 1/2 '\n",
      "      'inch margin between the last toothpick and the end of the roll',\n",
      " 115: 'Place 8-inch flour tortilla on cutting board',\n",
      " 116: 'Roll the tortilla from one end to the other into a log shape, about 1.5 '\n",
      "      'inches thick. Roll it tight enough to prevent gaps, but not so tight '\n",
      "      'that the filling leaks',\n",
      " 117: 'Place the floss halfway between toothpicks',\n",
      " 118: 'Clean the knife by wiping it with a paper towel',\n",
      " 119: 'Slice one tomato into about 1/2 inch thick slices',\n",
      " 120: 'Place the thick slices of tomatoes on a platter, ensuring they only '\n",
      "      'make a single layer',\n",
      " 121: 'Add a drizzle of extra-virgin olive oil, about 1 tablespoon, over the '\n",
      "      'entire platter',\n",
      " 122: 'gently dry it with a paper/tea towel',\n",
      " 123: 'Season the tomato slices with salt',\n",
      " 124: 'Sprinkle mozzarella cheese on top of the tomato throughout the platter',\n",
      " 125: 'Rinse a tomato',\n",
      " 126: 'Season platter with 1/4 teaspoon black pepper',\n",
      " 127: 'Garnish platter with italian seasoning',\n",
      " 128: 'add lime juice to the bowl',\n",
      " 129: 'Measure 2 cups of frozen corn',\n",
      " 130: '1 teaspoon of pepper powder to the bowl',\n",
      " 131: 'Extract lime juice from 1/3 lime',\n",
      " 132: 'Add the corn into a microwave-safe bowl',\n",
      " 133: 'Microwave the corn for 2 minutes',\n",
      " 134: 'Microwave the corn for 3 more minutes',\n",
      " 135: 'then stir the bowl',\n",
      " 136: 'Add 1 teaspoon salt to the bowl',\n",
      " 137: 'Thaw the frozen corn by putting it in a sieve and running it under cold '\n",
      "      'water',\n",
      " 138: 'Add 1 teaspoon of softened butter',\n",
      " 139: 'Add 1/4 tsp mustard to the pan',\n",
      " 140: 'mince the garlic',\n",
      " 141: '1/2 tsp cumin seeds to the pan',\n",
      " 142: 'Add tomato puree to the pan',\n",
      " 143: 'Take the pan off the heat',\n",
      " 144: 'Mix well tomato puree with contents in the pan',\n",
      " 145: 'Peel 4 large garlic cloves',\n",
      " 146: '1/2 tsp salt to the pan',\n",
      " 147: 'Add 2 tbsp red chili powder to the pan',\n",
      " 148: 'Saute the garlic for 2-3 minutes',\n",
      " 149: 'Take 1 tomato',\n",
      " 150: 'puree tomatoes without any water in a blender/mixer',\n",
      " 151: 'Lower the heat',\n",
      " 152: 'When mustard and cumin seeds begin to sizzle, add minced garlic',\n",
      " 153: 'Allow the mixture to simmer over low heat for 5 minutes or until the '\n",
      "      'mixture becomes thick',\n",
      " 154: 'Heat 3 tbsp oil in a pan over medium heat',\n",
      " 155: 'Chop tomato roughly (anysize chunks are fine)',\n",
      " 156: 'Transfer it to a serving bowl',\n",
      " 157: 'mix well contents of the pan',\n",
      " 158: 'Chop 1 tsp cilantro',\n",
      " 159: 'Whisk the egg mixture in the bowl',\n",
      " 160: 'Add garlic to the pan',\n",
      " 161: 'Chop 1 green chilli',\n",
      " 162: 'Add chilli to the pan',\n",
      " 163: 'Chop 1/4 medium onion',\n",
      " 164: 'add 1/3 tsp salt to the bowl',\n",
      " 165: 'Cook for 1 minute, mixing everything',\n",
      " 166: 'Saute the onions on medium heat until they are soft and translucent',\n",
      " 167: 'Crack one egg in the bowl',\n",
      " 168: 'Add 1/8 tsp of turmeric to the pan',\n",
      " 169: 'Chop 1/4 tomato',\n",
      " 170: 'Cook covered for 1 minute or until the tomatoes soften',\n",
      " 171: 'Keep mixing with a spatula for 3 minutes or until the eggs are almost '\n",
      "      'cooked',\n",
      " 172: 'Slowly pour the whisked eggs into the pan',\n",
      " 173: 'Heat 2 tbsp oil in a heavy-bottomed or nonstick pan on medium heat',\n",
      " 174: 'Garnish with 1 tbsp chopped cilantro and serve',\n",
      " 175: 'Add tomatoes to the pan',\n",
      " 176: 'Peel 2 garlic cloves',\n",
      " 177: 'add 1 tbsp milk to the bowl',\n",
      " 178: 'add 1/3 tsp salt to the pan',\n",
      " 179: 'add chopped onions to the pan',\n",
      " 180: 'Mince peeled garlic cloves',\n",
      " 181: '1/4 teaspoon of red chilli powder to the bowl',\n",
      " 182: 'peel the cucumber',\n",
      " 183: 'In a mixing bowl, whisk 1 cup of chilled curd until smooth. Use fresh '\n",
      "      'homemade or packaged curd',\n",
      " 184: '1/4 teaspoon salt to the bowl',\n",
      " 185: 'chop or grate the cucumber',\n",
      " 186: 'Combine all the ingredients in the bowl',\n",
      " 187: 'add 1 tablespoon of chopped cilantro leaves to the bowl',\n",
      " 188: 'Add 1 teaspoon of cumin powder to the bowl',\n",
      " 189: 'Rinse 1 medium sized cucumber',\n",
      " 190: 'Add the chopped or grated cucumber to the whisked curd',\n",
      " 191: '1/2 teaspoon of chaat masala powder to the bowl',\n",
      " 192: 'Cook for 2 minutes or until the zoodles are done',\n",
      " 193: '1/6 cup grated parmesan cheese',\n",
      " 194: 'pepper to taste',\n",
      " 195: 'Top with more parmesan if desired',\n",
      " 196: 'Heat a large pan on medium heat',\n",
      " 197: 'Remove from heat',\n",
      " 198: 'season with salt',\n",
      " 199: 'Add 1 large minced garlic cloves to the pan',\n",
      " 200: 'Peel 1 garlic cloves',\n",
      " 201: 'Add the zucchini noodles',\n",
      " 202: 'Cook garlic until fragrant (about 1 minutes). Be careful not to burn '\n",
      "      'garlic',\n",
      " 203: 'Spiralize 1 medium zucchini into thin noodles using a spiralizer',\n",
      " 204: 'Melt 1 tablespoons of softened butter',\n",
      " 205: 'pat rinsed mushrooms dry with a paper towel',\n",
      " 206: 'Rinse 3 mushrooms under cold water',\n",
      " 207: 'cook the pan, often stirring, for 1 minute',\n",
      " 208: 'Once the pan is hot, add the mushrooms',\n",
      " 209: 'Add chopped shallot to the pan',\n",
      " 210: 'Slice the mushrooms',\n",
      " 211: 'mince garlic cloves',\n",
      " 212: 'Chop 1 shallot',\n",
      " 213: 'cook for 3-5 minutes, stirring often, until mushrooms start to soften '\n",
      "      'and brown',\n",
      " 214: 'Pull out mushroom stems',\n",
      " 215: 'Heat 1 tbsp olive oil in a large skillet over medium-high heat',\n",
      " 216: 'Transfer the contents of the pan to a serving dish',\n",
      " 217: '1/4 tbsp balsamic vinegar to the pan',\n",
      " 218: 'Add 2 cloves of minced garlic to the pan',\n",
      " 219: 'pepper on pan to taste',\n",
      " 220: 'Season pan with salt',\n",
      " 221: '1/2 tsp baking powder to a blender',\n",
      " 222: 'Serve the pancakes with chopped strawberries',\n",
      " 223: 'Melt a small knob of butter in a non-stick frying pan over low-medium '\n",
      "      'heat',\n",
      " 224: 'splash maple syrup on plate',\n",
      " 225: 'Add 1 banana to a blender',\n",
      " 226: 'Cook for 1 min or until the tops start to bubble',\n",
      " 227: 'blitz the blender for 20 seconds',\n",
      " 228: 'Flip the pancakes with a fork or a fish slice spatula',\n",
      " 229: '1 egg to a blender',\n",
      " 230: 'cook for 20-30 seconds more',\n",
      " 231: 'Pour three little puddles straight from the blender into the frying pan',\n",
      " 232: '1 heaped tbsp flour to a blender',\n",
      " 233: 'Chop 1 strawberry',\n",
      " 234: 'Transfer to a plate',\n",
      " 235: 'Heat 1 tbsp oil in a non-stick frying pan',\n",
      " 236: 'put tomatoes on a serving plate',\n",
      " 237: 'Chop 2 tbsp cilantro',\n",
      " 238: 'stir gently with a wooden spoon so the egg that sets on the base of the '\n",
      "      'pan moves to enable the uncooked egg to flow into the space',\n",
      " 239: 'crack one egg in a bowl',\n",
      " 240: 'cook the tomatoes cut-side down until they start to soften and colour',\n",
      " 241: 'Pour the egg mixture into the pan',\n",
      " 242: 'add the chopped cilantro to the bowl',\n",
      " 243: 'Transfer omelette to the plate and serve with the tomatoes',\n",
      " 244: 'Scoop the tomatoes from the pan',\n",
      " 245: \"Stop stirring when it's nearly cooked to allow it to set into an \"\n",
      "      'omelette',\n",
      " 246: 'Cut tomato into two pieces',\n",
      " 247: 'Take a tomato',\n",
      " 248: 'Beat the contents of the bowl',\n",
      " 249: '1/2 tsp ground black pepper to the bowl',\n",
      " 250: 'Add 1/8 cup soy sauce to the bowl',\n",
      " 251: 'Set aside the sauce mixture',\n",
      " 252: 'Add 1 tablespoon honey to the bowl',\n",
      " 253: 'cook, stirring often, for 4 minutes. If the pan gets too hot on '\n",
      "      'medium-high, turn the heat down to medium',\n",
      " 254: 'Add 1 teaspoon cornstarch to the bowl',\n",
      " 255: 'Add 2 cloves minced garlic to the bowl',\n",
      " 256: 'Pour the sauce into the skillet',\n",
      " 257: 'Peel 2 cloves of garlic',\n",
      " 258: 'add sliced mushrooms to the skillet',\n",
      " 259: 'slice mushrooms',\n",
      " 260: 'Whisk the contents of bowl',\n",
      " 261: 'Add bell pepper to the skillet',\n",
      " 262: 'mince garlic',\n",
      " 263: 'slice 1/3 of the bell pepper',\n",
      " 264: 'cook, stirring, for 1 minute until the sauce thickens',\n",
      " 265: 'Add broccoli to the skillet',\n",
      " 266: 'Heat 2 tablespoons olive oil in a skillet over medium-high heat',\n",
      " 267: 'Add 1/2 tablespoon minced ginger to the bowl',\n",
      " 268: 'Add 1/8 teaspoon black pepper to the bowl',\n",
      " 269: 'Add 1/6 cup water the bowl',\n",
      " 270: 'Take 2 cremini mushrooms',\n",
      " 271: 'Take 1 bell pepper',\n",
      " 272: 'Whisk the sauce again to recombine the ingredients',\n",
      " 273: 'Take 5 in number broccoli florets',\n",
      " 274: 'continue cooking, stirring often, for 2-3 minutes, until vegetables are '\n",
      "      'crisp-tender',\n",
      " 275: 'Cut 1/4 block or 3 ounces of fresh tofu into large cubes (about 1 in x '\n",
      "      '1 in)',\n",
      " 276: 'Turn on the heat to medium',\n",
      " 277: 'drizzle 1 tablespoon soy sauce (watch for spitting) on the pan',\n",
      " 278: 'drizzle with the 1 tablespoons sesame oil on the pan',\n",
      " 279: 'add the tofu cubes to the pan',\n",
      " 280: 'Briefly remove the pan from the heat to reduce spitting',\n",
      " 281: 'flip tofu on the pan',\n",
      " 282: 'Briefly remove from the heat again',\n",
      " 283: 'Transfer to a serving dish',\n",
      " 284: 'Return to low heat',\n",
      " 285: 'Flip the tofu with tongs',\n",
      " 286: 'add 1/4 tsp salt to the pan',\n",
      " 287: 'Return the heat to medium',\n",
      " 288: 'cook pan for 2 minutes until the colour is darkened',\n",
      " 289: 'cook pan for 2 minutes',\n",
      " 290: 'cook until tofu turns brown',\n",
      " 291: 'Add 1 tablespoon of olive oil to a non-stick pan',\n",
      " 292: 'Cook 5 to 6 minutes until tofu cubes are lightly browned on the bottom',\n",
      " 293: 'pat tofu dry with a towel',\n",
      " 294: 'Whisk batter until no lumps remain',\n",
      " 295: 'then carefully remove the paper liner',\n",
      " 296: '2 tbsp water to the bowl',\n",
      " 297: '1.5 tbsp sugar to the mixing bowl',\n",
      " 298: 'While the cake is cooling, prepare to pipe the frosting. Scoop 4 '\n",
      "      'spoonfuls of chocolate frosting into a zip-top bag',\n",
      " 299: 'Measure and add 2 tbsp flour to the mixing bowl',\n",
      " 300: 'Measure and add 2 tsp vegetable oil to the bowl',\n",
      " 301: 'Set aside the lined mug',\n",
      " 302: 'Squeeze the frosting through the opening to apply small dollops of '\n",
      "      'frosting to the plate in a circle around the base of the cake',\n",
      " 303: 'Place the paper cupcake liner inside the mug',\n",
      " 304: '1/4 tsp baking powder to the bowl',\n",
      " 305: 'Allow to cool until it is no longer hot to the touch',\n",
      " 306: 'Use scissors to cut one corner from the bag to create a small opening '\n",
      "      '1/4 inch in diameter',\n",
      " 307: 'Pour batter into prepared mug',\n",
      " 308: 'Whisk to combine mixture of flour, sugar and baking powder in the bowl',\n",
      " 309: 'Invert the mug to release the cake onto a plate',\n",
      " 310: 'a pinch of salt to the mixing bowl',\n",
      " 311: 'seal zip top bag, removing as much air as possible',\n",
      " 312: 'Microwave the mug and batter on high power for 60 seconds. Check if the '\n",
      "      'cake is done by inserting and toothpick into the center of the cake and '\n",
      "      'then removing it. If wet batter clings to the toothpick, microwave for '\n",
      "      'an additional 5 seconds. If the toothpick comes out clean, continue',\n",
      " 313: '1/4 tsp vanilla extract to the bowl',\n",
      " 314: 'add 1/2 tbsp softened butter to the bowl',\n",
      " 315: 'Add 1/4 teaspoon salt to the bowl',\n",
      " 316: 'Add 1/3 cup cheddar cheese to a microwave-safe cup',\n",
      " 317: 'Mix the cheese and red bell pepper in the bowl',\n",
      " 318: 'Microwave the bowl, covered, for 2 minutes',\n",
      " 319: 'Add 1/4 teaspoon pepper to the bowl',\n",
      " 320: 'Add 1 tablespoons of water to the bowl',\n",
      " 321: 'Chop 1/4 red bell pepper into tiny bits',\n",
      " 322: 'Place the chopped pepper in the microwave-safe bowl',\n",
      " 323: 'Melt the cheese by microwaving cup for 30 sec. (Check after 30 seconds '\n",
      "      'and microwave for 10 seconds more if needed)',\n",
      " 324: 'Mix all the ingredients of the bowl well',\n",
      " 325: 'place avocado slices on each leaf',\n",
      " 326: 'season 1/4 tsp pepper on the bowl',\n",
      " 327: '1/4 cup mayonnaise to the bowl',\n",
      " 328: 'drain excess water from can',\n",
      " 329: 'Lay out 2 large lettuce leaves',\n",
      " 330: 'Roll up the lettuce wraps',\n",
      " 331: 'Mix the contents of the bowl',\n",
      " 332: 'top lettuce leaves with the tuna mixture',\n",
      " 333: 'add chopped scallion to the bowl',\n",
      " 334: 'Add 1 can drained tuna to the bowl',\n",
      " 335: 'Take 1 ripe avocado',\n",
      " 336: 'Open a can of tuna',\n",
      " 337: 'secure the wrap with a toothpick',\n",
      " 338: 'Season bowl with 1/4 tsp salt',\n",
      " 339: '1 tsp Sriracha sauce to the bowl',\n",
      " 340: 'Chop 1 scallion',\n",
      " 341: 'cut avocado into thin slices',\n",
      " 342: '1/8 cup shredded mozzarella to a bowl',\n",
      " 343: '1/4 tsp salt to a bowl',\n",
      " 344: 'Slice two 1/2 inch thick rounds from a baguette (slice slanted)',\n",
      " 345: 'Spoon the mixture from the bowl onto the bread',\n",
      " 346: '1/4 tsp pepper to a bowl',\n",
      " 347: '1/16 cup basil to a bowl',\n",
      " 348: 'Combine the contents of the bowl',\n",
      " 349: 'Brush 2 slices of baguette with olive oil on both sides',\n",
      " 350: 'Cut 1/4 cup of cherry tomatoes into halves',\n",
      " 351: 'In a bowl, add the cut cherry tomatoes',\n",
      " 352: 'Toast both sides of the slices on the pan for 2 to 3 minutes until '\n",
      "      'lightly charred and crispy and transfer the slices to a plate'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "RECIPE_STEPS = json.load(open(os.path.join(ANNOTATIONS_DIR, \"annotation_json/step_idx_description.json\"), \"r\"))\n",
    "RECIPE_STEPS = {int(k): \"-\".join(v.split(\"-\")[1:]).strip() for k, v in RECIPE_STEPS.items()}\n",
    "\n",
    "# pprint(RECIPE_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f06d0f5-7849-4664-b0e2-691de792ab6d",
   "metadata": {},
   "source": [
    "Generate success verification questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8678903-3bdf-47fe-845e-fad45fa8b597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/350 [03:10<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m vqg_outputs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     46\u001b[0m prompt_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m tqdm(lm(KeyDataset(prompts, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m), \n\u001b[1;32m     48\u001b[0m                  batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, \n\u001b[1;32m     49\u001b[0m                  max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, \n\u001b[1;32m     50\u001b[0m                  return_full_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m     51\u001b[0m                  truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_not_truncate\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     52\u001b[0m                total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(prompts)):\n\u001b[1;32m     53\u001b[0m     inp \u001b[38;5;241m=\u001b[39m prompts[prompt_idx]\n\u001b[1;32m     55\u001b[0m     step_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(inp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/transformers/pipelines/base.py:1068\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1067\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1068\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1069\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:295\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1525\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1518\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1519\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1520\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1521\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1522\u001b[0m     )\n\u001b[1;32m   1524\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1542\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1543\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1548\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1549\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2622\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2619\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2621\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2622\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2623\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2625\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2626\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2630\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1183\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1180\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1183\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1070\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1061\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1062\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         use_cache,\n\u001b[1;32m   1068\u001b[0m     )\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1070\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1079\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:811\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    810\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 811\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_attention_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    813\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/travel-nQET-zRt-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from dataclasses_json import dataclass_json\n",
    "from enum import Enum\n",
    "import pickle\n",
    "import json\n",
    "                     \n",
    "USE_VQG_CACHE = False\n",
    "\n",
    "VQG_FILENAME = os.path.join(DATA_CACHE_DIR, \"vqg_outputs.json\")\n",
    "if not USE_VQG_CACHE or not os.path.exists(VQG_FILENAME):\n",
    "\n",
    "    # NOTE: this process makes an assumption that a single recipe step is all we need to determine the step requirements;\n",
    "    # there are some exceptions to this assumption, e.g., \"cook the pan until the colour is darkened\" where the target\n",
    "    # object is unclear just from a single step\n",
    "    \n",
    "    # TODO: source recipe steps from elsewhere; find a comprehensive set that covers variety of state changes?\n",
    "    task_graph1 = os.path.join(ANNOTATIONS_DIR, \"task_graphs/capresebruschetta.json\")\n",
    "    procedure_id1 = 345\n",
    "    step_idx = 4\n",
    "    example1 = 'The recipe step is \"Spoon the mixture from the bowl onto the bread\". To visually verify that this step is complete, what are 2 questions we could ask about an image of a target object and their expected answers?\\n' \\\n",
    "               'Target object: bread\\n' \\\n",
    "               '1. Is there mixture on the bread? Yes\\n' \\\n",
    "               '2. Is there any bread without mixture on top of it? No' \\\n",
    "\n",
    "    example2 = 'The recipe step is \"Roll the tortilla into a thin, log shape about 1 inch thick. Make sure no filling leaks out.\". To visually verify that this step is complete, what are 2 questions we could ask about an image of a target object and their expected answers?\\n' \\\n",
    "               'Target object: tortilla\\n' \\\n",
    "               '1. Is the tortilla in a thin log shape? Yes\\n' \\\n",
    "               '2. Is there any filling leaking out of the tortilla? No'\n",
    "\n",
    "    example3 = 'The recipe step is \"Fold the coffee filter into quarters\". To visually verify that this step is complete, what are 2 questions we could ask about an image of a target object and their expected answers?\\n' \\\n",
    "               'Target object: coffee filter\\n' \\\n",
    "               '1. Is the coffee filter in a quarter circle? Yes\\n' \\\n",
    "               '2. Is the coffee filter folded? Yes' \\\n",
    "\n",
    "    prompts = []\n",
    "    with torch.no_grad():\n",
    "        for step_id, step in RECIPE_STEPS.items():\n",
    "            test = f'The recipe step is \"{step}\". To visually verify that this step is complete, what are 2 questions we could ask about an image of a target object and their expected answers?\\n'\n",
    "            prompt = \"\\n\\n\".join([example1, example2, example3, test])\n",
    "            prompts.append({\"step_id\": step_id, \"step\": step, \"prompt\": prompt})\n",
    "\n",
    "    vqg_outputs = {}\n",
    "    prompt_idx = 0\n",
    "    for out in tqdm(lm(KeyDataset(prompts, \"prompt\"), \n",
    "                     batch_size=32, \n",
    "                     max_new_tokens=64, \n",
    "                     return_full_text=False, \n",
    "                     truncation=\"do_not_truncate\"),\n",
    "                   total=len(prompts)):\n",
    "        inp = prompts[prompt_idx]\n",
    "\n",
    "        step_id = int(inp['step_id'])\n",
    "        step = inp['step']\n",
    "\n",
    "        text = out[0]['generated_text']\n",
    "        \n",
    "        # print(\"===========================================================================\")\n",
    "        # print(text)\n",
    "        # Hack: sometimes output from LLaMA 2 starts with Љ and whitespace characters, and sometimes this character replaces the first \"T\" in \"Target object:\"\n",
    "        text_fixed = text.replace(\"Љ\", \"\").strip() \n",
    "        if not text_fixed.startswith(\"Target object:\") and \":\" in text_fixed:\n",
    "            text_fixed = \"Target object: \" + \":\".join(text_fixed.split(\":\")[1:]).strip()\n",
    "        \n",
    "        # Parse reported target object and questions and answers\n",
    "        try:\n",
    "            target_object = text_fixed.split(\"\\n\")[0].split(\"Target object: \")[1].strip()\n",
    "            # print(target_object)\n",
    "            # state_description = text_fixed.split(\"\\n\")[1].replace(\"Expected state: After this step, \", \"\").strip()\n",
    "            questions_answers = [(q_a.split(\"?\")[0].strip() + \"?\", q_a.split(\"?\")[1].strip()) for q_a in text_fixed.split(\"\\n\")[1:3]] # NOTE: only extract k=2 questions and answers; can adjust this as needed later\n",
    "            questions = [q[2:].strip() for q, _ in questions_answers]          \n",
    "            answers = [a for _, a in questions_answers]\n",
    "            output = VQGOutputs(step_id,\n",
    "                                step,\n",
    "                                target_object,\n",
    "                                # state_description,\n",
    "                                questions,\n",
    "                                answers)\n",
    "        except:\n",
    "            print(text)\n",
    "            print('=====')\n",
    "            print(text_fixed)\n",
    "            raise\n",
    "\n",
    "        vqg_outputs[step_id] = output\n",
    "        prompt_idx += 1\n",
    "\n",
    "        # # Early stopping for debugging\n",
    "        # if prompt_idx >= 20:\n",
    "        #     break\n",
    "else:\n",
    "    vqg_outputs = pickle.load(open(VQG_FILENAME, \"rb\"))\n",
    "    \n",
    "#     vqg_outputs_json = json.load(open(os.path.join(DATA_CACHE_DIR, \"vqg_outputs.json\"), \"r\"))\n",
    "    \n",
    "#     with open(os.path.join(DATA_CACHE_DIR, \"vqg_outputs.json\"), \"r\") as f:\n",
    "#         vqg_outputs_json = json.load(f)\n",
    "#         vqg_outputs = {k: VQGOutputs.from_dict(v) for k, v in vqg_outputs_json.items()}\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae61049a-eb1d-4048-a1e4-b5a9e00f4e5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Print and save outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f3bd3a-c56e-4e51-b806-9d3f13ab9db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step_id, output in vqg_outputs.items():\n",
    "    print(RECIPE_STEPS[step_id])\n",
    "    pprint(output)\n",
    "    print('===================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48dcb8ec-1556-47f3-826c-919bd71c40ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import asdict\n",
    "import pickle\n",
    "\n",
    "# TODO: cache in a more interpretable way later\n",
    "pickle.dump(vqg_outputs, open(VQG_FILENAME, \"wb\"))\n",
    "\n",
    "# Sort the dict\n",
    "# vqg_keys = sorted(vqg_outputs.keys())\n",
    "# vqg_outputs_new = {}\n",
    "# for key in vqg_keys:\n",
    "#     vqg_outputs_new[key] = vqg_outputs[key]\n",
    "\n",
    "# with open(os.path.join(DATA_CACHE_DIR, \"vqg_outputs.json\"), \"w\") as f:\n",
    "#     for key in vqg_outputs_new:\n",
    "#         vqg_outputs_new[key] = VQGOutputs.to_dict(vqg_outputs_new[key])\n",
    "#     json.dump(vqg_outputs_new, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a975ce-5389-48c1-8d78-6a4847c99145",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Step 2: VQA with LLaVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae51a976-d86c-442a-b600-b99e26504098",
   "metadata": {},
   "source": [
    "Load models and dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1c1f4dd-9b79-4cf1-98dd-ded708bfa594",
   "metadata": {},
   "outputs": [],
   "source": [
    "del lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8f55dbf-5abc-4763-8c61-e51c28a7e5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ec0ec8b2f14f06a517e2169aeb4fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup code grabbed from docs: https://huggingface.co/docs/transformers/model_doc/llava#transformers.LlavaForConditionalGeneration\n",
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "MODEL_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
    "vlm_processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "vlm = LlavaForConditionalGeneration.from_pretrained(MODEL_NAME, cache_dir=DATA_CACHE_DIR, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef9244c2-7664-43c7-bde3-927e3fcca4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "\n",
    "DETECTOR_NAME = \"google/owlv2-base-patch16\"\n",
    "detector_processor = Owlv2Processor.from_pretrained(DETECTOR_NAME)\n",
    "detector = Owlv2ForObjectDetection.from_pretrained(DETECTOR_NAME, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cbcae49-fdfb-4db1-8739-0a35032a831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = success_examples[:20] + error_examples[:20]\n",
    "# examples = examples[:1] # Just for debug purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f424c3-742f-4647-8b2e-650da05b0b74",
   "metadata": {
    "tags": []
   },
   "source": [
    "### VQG->VQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8d7fc4-0bcf-459b-9e1f-3136bf78f61d",
   "metadata": {},
   "source": [
    "First, use OWL to filter out frames where target object isn't visible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96443f64-46ff-4d5d-bcdc-1a8d23452b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "detecting objects: 100%|██████████| 70/70 [02:36<00:00,  2.23s/it]\n",
      "filtering frames: 100%|██████████| 40/40 [00:00<00:00, 24139.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332 / 553 frames filtered out for not containing target object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import dataclasses\n",
    "from collections import defaultdict\n",
    "\n",
    "SHOW_OUTPUTS = False\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    all_frames = [frame for example in examples for frame in example.frames]\n",
    "    all_texts = [f\"a photo of {'an' if vqg_outputs[example.procedure_id].target_object[0] in ['a','e','i','o','u'] else 'a'} {vqg_outputs[example.procedure_id].target_object}\" for example in examples for frame in example.frames]\n",
    "    \n",
    "    batch_size = 8\n",
    "    all_results = []\n",
    "    all_padded_images = []\n",
    "    for i in tqdm(range(0, len(all_frames), batch_size), desc=\"detecting objects\"):\n",
    "        # Prepare the batch\n",
    "        batch_frames = all_frames[i:i+batch_size]\n",
    "        batch_texts = all_texts[i:i+batch_size]\n",
    "        \n",
    "        inputs = detector_processor(text=batch_texts, images=batch_frames, return_tensors=\"pt\").to(device)\n",
    "        outputs = detector(**inputs)\n",
    "        inputs = inputs.to(\"cpu\")  \n",
    "        \n",
    "        padded_images = [get_preprocessed_image(inputs.pixel_values[j].detach().to('cpu')) for j in range(len(batch_frames))]\n",
    "\n",
    "        # Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n",
    "        target_sizes = torch.Tensor([pi.size[::-1] for pi in padded_images])\n",
    "        # Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n",
    "        results = detector_processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.2)\n",
    "        all_results += results\n",
    "        all_padded_images += padded_images\n",
    "        \n",
    "    texts = all_texts\n",
    "    results = all_results\n",
    "    padded_images = all_padded_images\n",
    "\n",
    "    example_frame_idx = 0\n",
    "    filtered_examples = []  \n",
    "    filtered_out_frames = 0\n",
    "    for example in tqdm(examples, desc=\"filtering frames\"):        \n",
    "        step_id = example.procedure_id\n",
    "        vqg_output = vqg_outputs[step_id]\n",
    "        target_object = vqg_output.target_object\n",
    "\n",
    "        filtered_frames = []\n",
    "        filtered_frame_times = []\n",
    "        for i in range(example_frame_idx, example_frame_idx + len(example.frames)):\n",
    "            \n",
    "            text = texts[i]\n",
    "            boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
    "\n",
    "            if len(boxes) > 0:\n",
    "                filtered_frames.append(example.frames[i - example_frame_idx])\n",
    "                filtered_frame_times.append(example.frame_times[i - example_frame_idx])\n",
    "            else:\n",
    "                filtered_out_frames += 1\n",
    "            \n",
    "            if SHOW_OUTPUTS:\n",
    "                entities = defaultdict(list)\n",
    "                for box, score, label in zip(boxes, scores, labels):\n",
    "                    box = BoundingBox(*[round(j, 2) for j in box.tolist()], float(score.detach().cpu()))\n",
    "\n",
    "                    # normalize bounding box coordinates\n",
    "                    box_norm = box.normalize(padded_images[example_frame_idx + i].width, padded_images[example_frame_idx + i].height)\n",
    "                    # print(f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\")\n",
    "\n",
    "                    entities[text[label]].append(box_norm)\n",
    "\n",
    "                entities = [(entity, None, BoundingBoxCluster(boxes).get_merged_boxes()) for entity, boxes in entities.items()]\n",
    "                plt.figure()\n",
    "                draw_entity_boxes_on_image(padded_images[example_frame_idx + i].resize((padded_images[example_frame_idx + i].width*2, padded_images[example_frame_idx + i].height*2)), entities, show=True)\n",
    "        \n",
    "        new_example = dataclasses.replace(example)\n",
    "        new_example.frames = filtered_frames\n",
    "        new_example.frame_times = filtered_frame_times\n",
    "        filtered_examples.append(new_example)\n",
    "        example_frame_idx += len(example.frames)\n",
    "        \n",
    "print(f\"{filtered_out_frames} / {len(padded_images)} frames filtered out for not containing target object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5d2305-c055-4946-b72a-71e9b9cb7b33",
   "metadata": {},
   "source": [
    "Ask success verification questions per frame:\n",
    "\n",
    "(doesn't try to focus VLM on target object yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05c6b7b5-e552-4da4-85a2-a23350f2476e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [02:45<00:00,  4.14s/it]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pprint import pprint\n",
    "import torch\n",
    "\n",
    "prompt_template = \"USER: <image>\\n{question} (yes/no) ASSISTANT: \"\n",
    "NO_ID = vlm_processor.tokenizer(\"No\", add_special_tokens=False)['input_ids'][0]\n",
    "YES_ID = vlm_processor.tokenizer(\"Yes\", add_special_tokens=False)['input_ids'][0]\n",
    "RESPONSE_TOKEN_IDS = {\n",
    "    VQAResponse[\"No\"]: NO_ID, \n",
    "    VQAResponse[\"Yes\"]: YES_ID\n",
    "}\n",
    "\n",
    "vqa_outputs = []\n",
    "with torch.no_grad():\n",
    "    for example in tqdm(filtered_examples):\n",
    "        this_vqa_outputs = []\n",
    "        \n",
    "        step_id = example.procedure_id\n",
    "        \n",
    "        questions = vqg_outputs[step_id].questions\n",
    "        prompts = [prompt_template.format(question=question.strip()) for question in questions]\n",
    "        expected_answers = vqg_outputs[step_id].answers\n",
    "                           \n",
    "        for frame in example.frames:\n",
    "            \n",
    "            for prompt, expected_answer in zip(prompts, expected_answers):\n",
    "                inputs = vlm_processor(text=prompt, images=frame, return_tensors=\"pt\").to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                logits = vlm(**inputs).logits[0] # (seq. length, vocab size)\n",
    "                logits = logits[-1].detach().cpu() # just logits for last input (next generation)\n",
    "                \n",
    "                this_vqa_outputs.append(\n",
    "                    VQAOutputs(\n",
    "                        step_id,\n",
    "                        frame,\n",
    "                        prompt,\n",
    "                        expected_answer,\n",
    "                        RESPONSE_TOKEN_IDS,\n",
    "                        logits,        \n",
    "                    )\n",
    "                )\n",
    "\n",
    "        vqa_outputs.append(this_vqa_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfc9a94-78b8-467b-939b-aee7f8233877",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Baseline: Direct VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ee59eee-c7d3-49f3-b253-268fdc8e0022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [03:29<00:00,  5.24s/it]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pprint import pprint\n",
    "import torch\n",
    "\n",
    "prompt_template = 'USER: <image>\\nThe current goal is to \"{step}\". Did the person successfully do this? (yes/no) ASSISTANT: '\n",
    "NO_ID = vlm_processor.tokenizer(\"No\", add_special_tokens=False)['input_ids'][0]\n",
    "YES_ID = vlm_processor.tokenizer(\"Yes\", add_special_tokens=False)['input_ids'][0]\n",
    "RESPONSE_TOKEN_IDS = {\n",
    "    VQAResponse[\"No\"]: NO_ID, \n",
    "    VQAResponse[\"Yes\"]: YES_ID\n",
    "}\n",
    "\n",
    "vqa_outputs = []\n",
    "with torch.no_grad():\n",
    "    for example in tqdm(examples):\n",
    "        this_vqa_outputs = []\n",
    "        \n",
    "        step_id = example.procedure_id\n",
    "        step = example.procedure_description\n",
    "        \n",
    "        prompt = prompt_template.format(step=step)\n",
    "        expected_answer = VQAResponse[\"Yes\"]\n",
    "        \n",
    "        for frame in example.frames:\n",
    "            inputs = vlm_processor(text=prompt, images=frame, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = vlm(**inputs).logits[0] # (seq length, vocab size)\n",
    "            logits = logits[-1].detach().cpu()\n",
    "\n",
    "            this_vqa_outputs.append(\n",
    "                VQAOutputs(\n",
    "                    step_id,\n",
    "                    frame,\n",
    "                    prompt,\n",
    "                    expected_answer,\n",
    "                    RESPONSE_TOKEN_IDS,\n",
    "                    logits,        \n",
    "                )\n",
    "            )        \n",
    "            \n",
    "        vqa_outputs.append(this_vqa_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3bdde0-974c-403e-b448-9d1342841de4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Step 3: Evaluate VQA Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c910966f-cc39-4c12-9d8c-a2c3595a73cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics:\n",
      "{'accuracy': 0.5, 'f1': 0.4117647058823529, 'precision': 0.5, 'recall': 0.35}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "evaluator = HeuristicMistakeDetectionEvaluator(examples, vqa_outputs)\n",
    "metrics = evaluator.get_mistake_detection_metrics()\n",
    "    \n",
    "print(\"Metrics:\")\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c77696-1b58-42d3-ba89-c7880be2dfdc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# RLHF/DPO Preliminary Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be075b49-8468-4afc-88be-39ed78492d8d",
   "metadata": {},
   "source": [
    "Simple test of VQA scorer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a17c936-cb86-487a-9c01-26be578157ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m lm\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lm' is not defined"
     ]
    }
   ],
   "source": [
    "del lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c025cd5e-64ca-4768-a250-359c8a788b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "del scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "572fe301-23a0-457a-903f-0805c8ae2d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741dfd345b1a4285ae135b7391fa6ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running VQA: 100%|██████████| 80/80 [00:29<00:00,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistakeDetectionExample(video_id='29_7',\n",
      "                        procedure_id=342,\n",
      "                        frames=[<PIL.Image.Image image mode=RGB size=640x360 at 0x14F508543B50>],\n",
      "                        frame_times=[<PIL.Image.Image image mode=RGB size=640x360 at 0x14F508543B50>],\n",
      "                        procedure_description='1/8 cup shredded mozzarella to '\n",
      "                                              'a bowl',\n",
      "                        mistake=False,\n",
      "                        mistake_type=None,\n",
      "                        mistake_description=None)\n",
      "VQGOutputs(procedure_id=342,\n",
      "           procedure_description='1/8 cup shredded mozzarella to a bowl',\n",
      "           target_object='bowl',\n",
      "           questions=['Is there mozzarella in the bowl?',\n",
      "                      'Is there any cheese in the bowl?'],\n",
      "           answers_str=['Yes', 'Yes'],\n",
      "           answers=[<VQAResponse.Yes: 1>, <VQAResponse.Yes: 1>])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import dataclasses\n",
    "\n",
    "MODEL_NAME = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "# Select last frame from each CaptainCook4D video clip\n",
    "examples = success_examples[:20] + error_examples[:20]\n",
    "filtered_examples = []\n",
    "for example in examples:\n",
    "    filtered_frames = example.frames[-1]\n",
    "    new_example = dataclasses.replace(example)\n",
    "    new_example.frames = [example.frames[-1]]\n",
    "    new_example.frame_times = [example.frames[-1]]\n",
    "    filtered_examples.append(new_example)\n",
    "examples = filtered_examples\n",
    "\n",
    "scorer = FrameVQAMistakeDetectionScorer(MODEL_NAME)\n",
    "logits_errors = scorer(examples, vqg_outputs)\n",
    "# vqa_outputs = scorer(examples, vqg_outputs, return_vqa_outputs=True)\n",
    "\n",
    "pprint(examples[14])\n",
    "pprint(vqg_outputs[examples[14].procedure_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0994aa60-f7c3-4c75-9c73-8d6eaf2cc21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 2, 2])\n",
      "tensor([[[-4.1604e-05,  0.0000e+00],\n",
      "         [ 1.0000e+00,  1.0000e+00]],\n",
      "\n",
      "        [[-1.1921e-07, -2.3842e-07],\n",
      "         [ 1.0000e+00,  1.0000e+00]],\n",
      "\n",
      "        [[-7.2718e-06, -3.5763e-07],\n",
      "         [-1.4067e-05, -9.5367e-07]],\n",
      "\n",
      "        [[-2.3842e-07, -2.3842e-07],\n",
      "         [ 1.0000e+00,  1.0000e+00]],\n",
      "\n",
      "        [[-1.1921e-07, -3.5763e-07],\n",
      "         [-1.1921e-07, -3.5763e-07]],\n",
      "\n",
      "        [[-3.5763e-07, -1.1921e-07],\n",
      "         [-2.3842e-07, -1.1921e-07]],\n",
      "\n",
      "        [[-5.9605e-07, -2.3842e-07],\n",
      "         [-1.1921e-07, -1.1921e-07]],\n",
      "\n",
      "        [[-1.1921e-07, -5.9605e-07],\n",
      "         [ 1.0000e+00,  1.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00, -1.1921e-07],\n",
      "         [ 0.0000e+00, -1.1921e-07]],\n",
      "\n",
      "        [[ 0.0000e+00, -1.1921e-07],\n",
      "         [ 1.0000e+00,  1.0000e+00]],\n",
      "\n",
      "        [[-1.1921e-07, -1.1921e-07],\n",
      "         [-1.1921e-07, -2.3842e-07]],\n",
      "\n",
      "        [[-3.5763e-07, -1.1921e-07],\n",
      "         [-2.3842e-07, -1.1921e-07]],\n",
      "\n",
      "        [[-8.3447e-07, -2.3842e-07],\n",
      "         [-5.9605e-07, -2.3842e-07]],\n",
      "\n",
      "        [[-2.3961e-05, -1.1921e-07],\n",
      "         [-1.1921e-07, -4.7684e-07]],\n",
      "\n",
      "        [[-2.3842e-07, -5.9605e-07],\n",
      "         [-1.1921e-07, -5.9605e-07]],\n",
      "\n",
      "        [[-1.0729e-06, -2.3842e-07],\n",
      "         [-1.0729e-06, -2.3842e-07]],\n",
      "\n",
      "        [[-2.8610e-06, -2.3842e-07],\n",
      "         [ 1.0000e+00,  1.0000e+00]],\n",
      "\n",
      "        [[-1.1921e-07, -1.1921e-07],\n",
      "         [ 1.0000e+00,  1.0000e+00]],\n",
      "\n",
      "        [[-4.7684e-07, -3.5763e-07],\n",
      "         [-1.3113e-06, -5.9605e-07]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.0000e+00],\n",
      "         [ 0.0000e+00, -2.3842e-07]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.0000e+00],\n",
      "         [-2.2650e-06, -2.3842e-07]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.0000e+00],\n",
      "         [ 1.0000e+00,  1.0000e+00]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.0000e+00],\n",
      "         [ 1.0000e+00,  1.0000e+00]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.0000e+00],\n",
      "         [-8.3447e-07, -9.5367e-07]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.0000e+00],\n",
      "         [-8.3447e-07, -8.3447e-07]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.0000e+00],\n",
      "         [ 1.0000e+00,  1.0000e+00]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.0000e+00],\n",
      "         [-3.5763e-07, -3.5763e-07]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.0000e+00],\n",
      "         [-8.3447e-06, -1.1921e-07]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.0000e+00],\n",
      "         [ 1.0000e+00,  1.0000e+00]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.0000e+00],\n",
      "         [ 1.0000e+00,  1.0000e+00]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.0000e+00],\n",
      "         [-1.3113e-06,  0.0000e+00]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[-1.1921e-07, -2.3842e-07],\n",
      "         [ 1.0000e+00,  1.0000e+00]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.0000e+00],\n",
      "         [ 1.0000e+00,  1.0000e+00]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.0000e+00],\n",
      "         [ 1.0000e+00,  1.0000e+00]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.0000e+00],\n",
      "         [-1.3113e-06, -1.5497e-06]],\n",
      "\n",
      "        [[ 9.9999e-01,  1.0000e+00],\n",
      "         [-3.2187e-06, -5.9605e-07]],\n",
      "\n",
      "        [[ 9.9998e-01,  1.0000e+00],\n",
      "         [-1.3113e-06, -4.7684e-07]],\n",
      "\n",
      "        [[ 1.0000e+00,  1.0000e+00],\n",
      "         [-2.3842e-06, -1.1921e-07]],\n",
      "\n",
      "        [[-7.1526e-07, -1.1921e-07],\n",
      "         [-3.5763e-07, -1.1921e-07]]])\n"
     ]
    }
   ],
   "source": [
    "print(logits_errors.shape)\n",
    "pprint(logits_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e29e7-5152-4e03-8caf-e29712524bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel",
   "language": "python",
   "name": "travel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
