import os
from io import BytesIO
from PIL import Image
from tqdm import tqdm
from typing import Callable, Optional
import base64
import pickle
import numpy as np
from openai import AzureOpenAI, OpenAI

from travel.data.mistake_detection import MistakeDetectionDataset, MistakeDetectionExample
from travel.data.vqa import VQAOutputs, VQAResponse
from travel.data.vqg import VQGInputs, VQGOutputs, parse_vqg_outputs, save_vqg_outputs
from travel.model.mistake_detection import DETECTION_FRAMES_PROPORTION
from travel.constants import CACHE_FREQUENCY

class GPT:
    def __init__(self, api_key, endpoint) -> None:
        self.api_key = api_key
        self.endpoint = endpoint
    
    def _encode_image(self, frame: Image.Image) -> str:
        """
        Encodes a PIL.Image.Image object to a base64 string.

        :param image_path: A PIL.Image.Image object to encode.
        """
        buffered = BytesIO()
        image = frame.convert('RGB') # Discard alpha channel if needed
        image.save(buffered, format="JPEG")  # You can change the format if necessary
        encoded_string = base64.b64encode(buffered.getvalue()).decode("utf-8")
        return encoded_string
    
    def _example_in_outputs(self, example_id: str, vqa_outputs: list[VQAOutputs]) -> bool:
        """
        Helper function to check if an example_id appears in any of the vqa_outputs.

        :param example_id: The example_id to look for.
        :param vqa_outputs: List of VQAOutputs to check.

        """
        for output in vqa_outputs:
            if output[0][0].example_id == example_id:
                return True
        return False
    
    def _is_yes(self, token: str) -> bool:
        """
        Returns True if the given token is 'yes', 'Yes, ' yes', or ' Yes'. False otherwise.

        :param token: A token from the API response.
        """
        if (token == 'yes' or token == 'Yes'
            or token == ' yes' or token == ' Yes'):
            return True
        return False

    def _is_no(self, token: str) -> bool:
        """
        Returns True if the given token is 'no', 'No, ' np', or ' No'. False otherwise.

        :param token: A token from the API response.
        """
        if (token == 'no' or token == 'No'
            or token == ' no' or token == ' No'):
            return True
        return False
    
    def _get_probs(self, gpt_response) -> dict[VQAResponse, float]:
        """
        Returns the normalized porbabilities of 'yes' answer and 'no' answer using the given API response.
        
        :param  gpt_response: Response from the API containing top log probabilities of each token.
        """
        logprobs = gpt_response.choices[0].logprobs.content

        yes_prob = None
        no_prob = None

        # Find the first token that is a 'yes' or 'no' and treat it as the model's answer.
        # Grab the corresponding top log porbabilties of that token to get the porbabilities of 'yes' and 'no'.
        for entry in logprobs:
            token = entry.token
            top_probs = entry.top_logprobs
            if self._is_yes(token) or self._is_no(token):
                for prob in top_probs:
                    if self._is_yes(prob.token) and yes_prob is None:
                        yes_prob = prob.logprob
                    elif self._is_no(prob.token) and no_prob is None:
                        no_prob = prob.logprob
                break

        # If one doesn't appear set its porbability to 0 and the other to 1.
        # If both don't appear, set both to 0.5.
        # Normalize if both appear
        if yes_prob == None and no_prob == None:
            yes_prob = no_prob = 0.5
        elif yes_prob == None:
            yes_prob = 0
            no_prob = 1
        elif no_prob == None:
            no_prob = 0
            yes_prob = 1
        else:
            yes_prob = np.exp(yes_prob)
            no_prob = np.exp(no_prob)
            old_yes_prob = yes_prob
            yes_prob = old_yes_prob / (old_yes_prob + no_prob)
            no_prob = no_prob / (old_yes_prob + no_prob)
        
        return {VQAResponse.No: no_prob, VQAResponse.Yes: yes_prob}
        

    def prompt_gpt(self,
                    prompt_text: str,
                    image: Image=None,
                    temperature: float=1,
                    top_p: float=1,
                    max_tokens: int=128,
                    logprobs: bool=False,
                    top_logprobs: int=0):
        """
        Sends a request to the GPT API and returns the response object.

        :param prompt_text: Text prompt for GPT.
        :param Image: Optional image to include in the prompt.
        :param temperature: Sampling temperature used for the GPT model. 
        :param max_tokens: Maximum number of tokens to be generated by GPT.
        :param logprobs: Boolean indicating if to include the log probabilities of every output token.
        :param top_logprobs: The number of top probabilities to return for every output token, in [0,20]. logprobs has to be True for this option.
        """

        client = AzureOpenAI(
            api_key=self.api_key,  
            api_version="2024-03-01-preview",
            azure_endpoint=self.endpoint,
        )
        if image is not None:
            encoded_image = self._encode_image(image)
            if not logprobs:
                top_logprobs = None
            response = client.chat.completions.create(
                model="sstorks-gpt4",
                messages=[
                    {
                    "role": "user",
                    "content": [
                        {
                            "type": "text", "text": prompt_text
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{encoded_image}"
                            }
                        }
                    ]
                    }
                ],
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                logprobs=logprobs,
                top_logprobs=top_logprobs
            )
        else:
            if not logprobs:
                top_logprobs = None
            response = client.chat.completions.create(
                model="sstorks-gpt4",
                messages=[
                    {
                    "role": "user",
                    "content": [
                        {
                            "type": "text", "text": prompt_text
                        }
                    ],
                    }
                ],
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                logprobs=logprobs,
                top_logprobs=top_logprobs
            )
        return response
    

    def run_vqa(self,
                eval_dataset: MistakeDetectionDataset,
                generate_prompts: Callable[[MistakeDetectionExample], tuple[list[str], list[str], list[VQAResponse], list[Image.Image]]],
                cache_dir: str,
                cache_frequency: int=CACHE_FREQUENCY,
                temperature: float=0,
                ) -> list[list[list[VQAOutputs]]]:
        """
        Method to run VQA on a MistakeDetectionDataset using calls to the GPT-4 API.
        
        :param eval_dataset: MistakeDetectionDataset to run inference on.
        :param generate_prompts: A method that generates a list of prompts from a single MistakeDetectionExample.
        :param cache_dir: Directory to cache outputs in.
        :param cache_frequency: Determines how frequent VQAOutputs are cached.
        :param temprature: Sampling temperature used for the GPT model.
        """
        vqa_outputs = []

        # Check and load if there are cached outputs 
        cache_fname = os.path.join(cache_dir, "cached_outputs.pkl")
        if os.path.exists(cache_fname):
            with open(cache_fname, 'rb') as f:
                vqa_outputs = pickle.load(f)

        last_save = len(vqa_outputs)
        # Iterate through dataset, skip over loaded cached output
        for ex in tqdm(eval_dataset.get_batches(batch_size=1),
                            desc=f"VQA using GPT API", 
                            total=len(eval_dataset)):
            example = ex
            # Skip if the outputs for this example were already loaded from the cache
            if self._example_in_outputs(example.example_id, vqa_outputs):
                continue
            
            example.cutoff_to_last_frames(DETECTION_FRAMES_PROPORTION)
            # Prompts for this example
            questions, prompts, answers, frames = generate_prompts(example)
            assert len(questions) == len(prompts) == len(answers) == len(frames), "Passed `generate_prompts` method must return same number of questions, prompts, answers, and frames!"
            
            # Send API Requests for every prompt
            example_outputs = []
            for idx, prompt in enumerate(prompts):
                response = self.prompt_gpt(prompt_text=prompt,
                                        image=frames[idx],
                                        temperature=temperature,
                                        logprobs=True,
                                        top_logprobs=20)
                answer_probs = self._get_probs(response)
                example_outputs.append(
                    [VQAOutputs(
                        example.task_name,
                        example.example_id,
                        example.procedure_id,
                        frames[idx],
                        prompt, 
                        answers[idx],
                        {},
                        None,
                        questions[idx],
                        answer_probs=answer_probs
                    )]
                )
            vqa_outputs.append(example_outputs)
            # Cache based on frequency
            if len(vqa_outputs) - last_save >= cache_frequency:
                with open(cache_fname, 'wb') as f:
                    pickle.dump(vqa_outputs, f)
                last_save = len(vqa_outputs)
        return vqa_outputs 
    

    def run_vqg(self,
                inputs: list[VQGInputs],
                input_ids: list[str],
                temperature: float=1,
                top_p: float=1,
                save_path: Optional[str]=None,
                vqg_outputs: dict[str, VQGOutputs]={},
                omit_failed_instances: bool=True) -> dict[str, Optional[VQGOutputs]]:
        """
        Runs VQG with a given LM text generation pipeline and list of VQG inputs.

        :param inputs: List of VQG inputs, including procedures, prompts, etc.
        :param input_ids: Unique string identifiers for inputs. These may characterize a specific prompt or run of a prompt (e.g., at a different temperature).
        :param temprature: Sampling temperature used for the GPT model.
        :param save_path: Optional path to save VQG outputs during and after running. Must either be a json filename or path to a directory.
        :param vqg_outputs: Partly filled dictionary of VQG outputs to start from; only pass this if starting from a partially completed run of VQG, and make sure complete/incomplete prompts are managed appropriately outside of this method.
        :return: Completed dictionary of VQGOutputs.
        """
        assert len(inputs) == len(input_ids), "run_vqg expected the same number of inputs and input IDs!"
        prompt_idx = 0
        for input, input_id in tqdm(zip(inputs, input_ids),
                                    desc="running VQG (GPT)",
                                    total=len(inputs)):
            
            procedure_id = int(input.procedure_id)
            step = input.procedure_description
            response = self.prompt_gpt(prompt_text=input.prompt,
                                    temperature=temperature,
                                    top_p=top_p)
            text = response.choices[0].message.content
            # Parse reported target object and questions and answers
            try:
                output = parse_vqg_outputs(text, procedure_id, step)
            except:
                print("Warning: failed to parse a VQG output.")
                print(text)

                if not omit_failed_instances:
                    vqg_outputs[input_id] = None

                continue

            vqg_outputs[input_id] = output
            if prompt_idx % CACHE_FREQUENCY == 0 and save_path is not None:
                print("Saving progress...")
                save_vqg_outputs(vqg_outputs, save_path)

            prompt_idx += 1

        # Save progress one last time after completion
        save_vqg_outputs(vqg_outputs, save_path)
        return vqg_outputs
